{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "208Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5_lz-I9d88z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukLSIkbOeCds",
        "colab_type": "text"
      },
      "source": [
        "#Progress update - Chamain\n",
        "\n",
        "Hello, we can you this page to share codes and update the progress.\n",
        "\n",
        "- I found this [code](https://github.com/wikiabhi/Cifar-10/blob/master/cifar10_pca.ipynb). This code performs some tests we want (can be useful, not the most accurate.)\n",
        "- keras CIFAR-10 code - [code](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)\n",
        "- Based on these two I stitched the following code. (very basic- need to optimize the network)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htGdoVgRfoCo",
        "colab_type": "code",
        "outputId": "d5e14b1f-3971-471e-847c-b80ef22d801a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3358
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu May 16 12:55:19 2019\n",
        "\n",
        "@author: Lahiru D. Chamain\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "data_augmentation = True\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "#flattening for PCA\n",
        "X_train = x_train.copy()\n",
        "X_train = X_train.astype('float32')\n",
        "x_train = x_train.reshape(x_train.shape[0],-1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], -1).astype('float32')\n",
        "  \n",
        "pca = PCA()\n",
        "pca.fit_transform(x_train)\n",
        "\n",
        "print('Original no of components: ',pca.explained_variance_.shape)\n",
        "\n",
        "#Reduce the dimension to preserve 99% of the variance\n",
        "# Calculating optimal k to have 95% (say) variance \n",
        "\n",
        "k = 0\n",
        "total = sum(pca.explained_variance_)\n",
        "current_sum = 0\n",
        "\n",
        "while(current_sum / total < 0.99):\n",
        "    current_sum += pca.explained_variance_[k]\n",
        "    k += 1\n",
        "numComp = k\n",
        "print('reduced num of compoments: ',numComp)\n",
        "\n",
        "#select only those compoments\n",
        "## Applying PCA with k calcuated above\n",
        "\n",
        "pca = PCA(n_components=k, whiten=True)\n",
        "\n",
        "x_train_pca = pca.fit_transform(x_train)\n",
        "x_test = pca.transform(x_test)\n",
        "x_test = x_test/np.max(np.abs(x_test))\n",
        "\n",
        "print('done with PCA.')\n",
        "print('shape of test: ',x_test.shape)\n",
        "\n",
        "\"\"\"\n",
        "print('KNN training')\n",
        "#KNN\n",
        "## Training \n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(x_train_pca, y_train)\n",
        "\n",
        "##predict\n",
        "## Predicting\n",
        "y_pred_knn = knn.predict(x_test_pca)\n",
        "\n",
        "knn_score = accuracy_score(y_test, y_pred_knn)\n",
        "print('K test score:',knn_score)\n",
        "\"\"\"\n",
        "\n",
        "#DenseNN\n",
        "model = Sequential()\n",
        "# For first layer, input shape must be supplied\n",
        "layer1 = Dense(units = 50, activation = 'relu', input_dim = numComp)\n",
        "model.add(layer1)\n",
        "\n",
        "layer2 = Dense(units = 25, activation = 'relu')\n",
        "model.add(layer2)\n",
        "\n",
        "layer3 = Dense(units = num_classes, activation = 'sigmoid')\n",
        "model.add(layer3)\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 90:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 60:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 30:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "def creategen(X,Y,batch_size):\n",
        "    while True:\n",
        "        # suffled indices    \n",
        "        #idx = np.random.permutation( X.shape[0])\n",
        "        # create image generator\n",
        "        datagen = ImageDataGenerator(\n",
        "                \n",
        "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "                samplewise_center=False,  # set each sample mean to 0\n",
        "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "                samplewise_std_normalization=False,  # divide each input by its std\n",
        "                zca_whitening=False,  # apply ZCA whitening\n",
        "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "                horizontal_flip=True,  # randomly flip images\n",
        "                vertical_flip=False)\n",
        "\n",
        "        batches= datagen.flow( X, Y, batch_size=batch_size,shuffle=True)\n",
        "       \n",
        "        idx0 = 0\n",
        "        for batch in batches:\n",
        "            idx1 = idx0 + batch[0].shape[0]\n",
        "            temp = batch[0].reshape(batch[0].shape[0],-1).astype('float32')\n",
        "            temp = pca.transform(temp)\n",
        "            yield  temp/np.max(np.abs(temp)), batch[1]\n",
        "\n",
        "            idx0 = idx1\n",
        "            if idx1 >= X.shape[0]:\n",
        "                break\n",
        "\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_model.{epoch:03d}.h5'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "history_callback= model.fit_generator(creategen(X_train, y_train, batch_size=batch_size),\n",
        "                                      steps_per_epoch=int(np.ceil(X_train.shape[0]/batch_size)),\n",
        "                                      validation_data=(x_test, y_test),\n",
        "                                      epochs=epochs, verbose=1, workers=1,\n",
        "                                      callbacks=callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "#print('time per image :',(time.time()-start)*1000/10000,' ms')\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "model.save('cdf97n_3R0model.h5')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 31s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "Original no of components:  (3072,)\n",
            "reduced num of compoments:  658\n",
            "done with PCA.\n",
            "shape of test:  (10000, 658)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: 1.9946 - acc: 0.2768 - val_loss: 1.8546 - val_acc: 0.3486\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.34860, saving model to /content/saved_models/cifar10_model.001.h5\n",
            "Epoch 2/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.7528 - acc: 0.3806 - val_loss: 1.7727 - val_acc: 0.3851\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.34860 to 0.38510, saving model to /content/saved_models/cifar10_model.002.h5\n",
            "Epoch 3/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.6797 - acc: 0.4030 - val_loss: 1.7474 - val_acc: 0.3899\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.38510 to 0.38990, saving model to /content/saved_models/cifar10_model.003.h5\n",
            "Epoch 4/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.6267 - acc: 0.4213 - val_loss: 1.7012 - val_acc: 0.3936\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.38990 to 0.39360, saving model to /content/saved_models/cifar10_model.004.h5\n",
            "Epoch 5/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 49s 31ms/step - loss: 1.5888 - acc: 0.4352 - val_loss: 1.6591 - val_acc: 0.4122\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.39360 to 0.41220, saving model to /content/saved_models/cifar10_model.005.h5\n",
            "Epoch 6/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: 1.5646 - acc: 0.4429 - val_loss: 1.6470 - val_acc: 0.4078\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.41220\n",
            "Epoch 7/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: 1.5403 - acc: 0.4535 - val_loss: 1.6393 - val_acc: 0.4076\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.41220\n",
            "Epoch 8/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: nan - acc: 0.3327 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.41220\n",
            "Epoch 9/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 51s 33ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.41220\n",
            "Epoch 10/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 54s 34ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.41220\n",
            "Epoch 11/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 52s 33ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.41220\n",
            "Epoch 12/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.41220\n",
            "Epoch 13/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.41220\n",
            "Epoch 14/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.41220\n",
            "Epoch 15/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.41220\n",
            "Epoch 16/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.41220\n",
            "Epoch 17/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.41220\n",
            "Epoch 18/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 49s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.41220\n",
            "Epoch 19/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.41220\n",
            "Epoch 20/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.41220\n",
            "Epoch 21/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.41220\n",
            "Epoch 22/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.41220\n",
            "Epoch 23/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.41220\n",
            "Epoch 24/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.41220\n",
            "Epoch 25/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 51s 33ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.41220\n",
            "Epoch 26/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 51s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.41220\n",
            "Epoch 27/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 50s 32ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.41220\n",
            "Epoch 28/100\n",
            "Learning rate:  0.001\n",
            "1563/1563 [==============================] - 55s 35ms/step - loss: nan - acc: 0.1000 - val_loss: nan - val_acc: 0.1000\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.41220\n",
            "Epoch 29/100\n",
            "Learning rate:  0.001\n",
            " 786/1563 [==============>...............] - ETA: 26s - loss: nan - acc: 0.1017"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2d6facc7502c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m                                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                                       callbacks=callbacks)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m# Score trained model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0menqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0menqueuer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_enqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfinished_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLV8CHpniW20",
        "colab_type": "text"
      },
      "source": [
        "Results after 45 epochs\n",
        "1. Network - (50,25,10), tr_acc = 53.16% val_acc = 49.42%,  tr_time = 30 s/epoch, parameters = 0.034M, comment = underfitting, need more paprameters\n",
        "2. Network - (200,500,100,10), tr_acc = % val_acc = %,  tr_time = 30 s/epoch, parameters = M, comment = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ-IMzhNhL-1",
        "colab_type": "text"
      },
      "source": [
        "# RGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEjF2ohjWkgE",
        "colab_type": "code",
        "outputId": "5e374b20-5bf0-4a0c-ec56-1f926cf70030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "#CNN for CIFAR10\n",
        "#Zachary Cosenza\n",
        "\n",
        "#import\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.datasets import cifar10\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras.utils import to_categorical\n",
        "from heapq import heappush, heappop, heapify\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def CrappyCNN(train_images,train_labels,test_images,test_labels):\n",
        "    \n",
        "    train_images = train_images.astype('float32') / max_image\n",
        "\n",
        "    test_images = test_images.astype('float32') / max_image\n",
        "    \n",
        "    train_labels = to_categorical(train_labels)\n",
        "    test_labels = to_categorical(test_labels)\n",
        "    \n",
        "    input_shape = train_images.shape[1:]\n",
        "    \n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    model.fit(train_images, train_labels, epochs=epoch_size, batch_size=batch_size)\n",
        "    \n",
        "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "    \n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "#Code for NN\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "image_size = 32\n",
        "channel_num = 3\n",
        "max_image = 255\n",
        "\n",
        "epoch_size = 5\n",
        "batch_size = 300\n",
        "\n",
        "BWtest = HuffmanBW(test_images)\n",
        "BWtrain = HuffmanBW(train_images)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.bar('RGB',(BWtest+BWtrain)/2)\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Bandwidth')\n",
        "plt.title('BW for RGB')\n",
        "\n",
        "test_loss_CNN, test_acc_CNN = CrappyCNN(train_images,train_labels,test_images,test_labels)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.bar('RGB',test_acc_CNN)\n",
        "plt.xlabel('Method')\n",
        "plt.ylabel('Acc.')\n",
        "plt.title('Acc. for RGB')\n",
        "\n",
        "#fit model with PCA\n",
        "\n",
        "train_images_pca = train_images.reshape(train_images.shape[0],-1).astype('float32')\n",
        "test_images_pca = test_images.reshape(test_images.shape[0],-1).astype('float32')\n",
        "\n",
        "pca = PCA(0.99)\n",
        "pca.fit_transform(train_images_pca)\n",
        "\n",
        "train_images_pca_proj = pca.fit_transform(train_images_pca)\n",
        "train_images_recon = pca.inverse_transform(train_images_pca_proj)\n",
        "\n",
        "train_images_recon = train_images_recon.reshape(train_images.shape).astype('float32')\n",
        "\n",
        "test_images_pca_proj = pca.fit_transform(test_images_pca)\n",
        "test_images_recon = pca.inverse_transform(test_images_pca_proj)\n",
        "\n",
        "test_images_recon = test_images_recon.reshape(test_images.shape).astype('float32')\n",
        "\n",
        "test_loss_CNN_PCA, test_acc_CNN_PCA = CrappyCNN(train_images_recon,train_labels,test_images_recon,test_labels)\n",
        "\n",
        "BWtest_PCA = HuffmanBW(test_images_pca_proj)\n",
        "BWtrain_PCA = HuffmanBW(train_images_pca_proj)\n",
        "\n",
        "plt.figure(1)\n",
        "plt.bar('RGB PCA',(BWtest_PCA+BWtrain_PCA)/2)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.bar('RGB PCA',test_acc_CNN_PCA)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 28s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-052e2a17ef30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mBWtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuffmanBW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0mBWtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHuffmanBW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'HuffmanBW' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n1Sm0fEhON_",
        "colab_type": "text"
      },
      "source": [
        "# DCT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw9x8FPphQZl",
        "colab_type": "text"
      },
      "source": [
        "# DWT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4WVYpJkrBaH",
        "colab_type": "text"
      },
      "source": [
        "Let's load the data set and calculate PCA components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLSfXwpKrLSK",
        "colab_type": "code",
        "outputId": "2dbe8f74-0f71-4fc3-f71d-dc668a470083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#import libraries\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR2gyYJVf8cR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DWT\n",
        "from numpy.linalg import inv\n",
        "\n",
        "def batchRGB2YCRCB(x_batch):\n",
        "    alpha_R = 0.299\n",
        "    alpha_G = 0.587\n",
        "    alpha_B = 0.114\n",
        "    x_batchnew = np.zeros((x_batch.shape)).astype('float32')\n",
        "    for i in range(0,x_batch.shape[0]):\n",
        "        #Y\n",
        "        x_batchnew[i,:,:,0] = alpha_R*x_batch[i,:,:,0] + alpha_G*x_batch[i,:,:,1] + alpha_B*x_batch[i,:,:,2]\n",
        "        #Cb\n",
        "        x_batchnew[i,:,:,1] = (0.5/(1-alpha_B))*(x_batch[i,:,:,2]-x_batchnew[i,:,:,0])\n",
        "        #Cr\n",
        "        x_batchnew[i,:,:,2] = (0.5/(1-alpha_R))*(x_batch[i,:,:,0]-x_batchnew[i,:,:,0])\n",
        "    return x_batchnew\n",
        "\n",
        "#CDF 9/7\n",
        "def getTcdf97(height):\n",
        "    a1 = -1.586134342\n",
        "    a2 = -0.05298011854\n",
        "    a3 = 0.8829110762\n",
        "    a4 = 0.4435068522\n",
        "\n",
        "    # Scale coeff:\n",
        "    k1 = 0.8128662109 # 1/1.230174104914 // 0,2,4,6\n",
        "    k2 = 0.6149902344 # 1.230174104914/2 // 5038 1,3,5,7\n",
        "    X1 = np.identity(height)\n",
        "    X2 = np.identity(height)\n",
        "    X3 = np.identity(height)\n",
        "    X4 = np.identity(height)\n",
        "    X5 = np.zeros((height,height)).astype('float32')\n",
        "    for col in range(1,height-2,2):\n",
        "        X1[col-1,col]=X1[col+1,col]=a1\n",
        "    X1[height-2,height-1] = 2*a1\n",
        "    \n",
        "    #print(X1)\n",
        "    for col in range(2,height-1,2):\n",
        "        X2[col-1,col]=X2[col+1,col]=a2\n",
        "    X2[1,0] = 2*a2\n",
        "    #print(X2)\n",
        "    for col in range(1,height-2,2):\n",
        "        X3[col-1,col]=X3[col+1,col]=a3\n",
        "    X3[height-2,height-1] = 2*a3\n",
        "    \n",
        "    #print(X1)\n",
        "    for col in range(2,height-1,2):\n",
        "        X4[col-1,col]=X4[col+1,col]=a4\n",
        "    X4[1,0] = 2*a4\n",
        "    \n",
        "    for col in range(0,height,1):\n",
        "        if(col%2==0 ):\n",
        "            #print(col)\n",
        "            X5[col,int(col/2)]=k1\n",
        "        else:\n",
        "            X5[col,int(height/2 + (col-1)/2)]=k2\n",
        "    #print(X3)\n",
        "    X =np.matmul(np.matmul(np.matmul(np.matmul(X1,X2),X3),X4),X5)\n",
        "    return X,inv(X)\n",
        "  \n",
        "# DWT transform\n",
        "def batchwaveletcdf97mat(x_batch,X,dimhalf):\n",
        "    x_batchnew = np.zeros((x_batch.shape[0],dimhalf,dimhalf,12)).astype('float32')\n",
        "    for i in range(0,x_batch.shape[0]):\n",
        "        for j in range(0,x_batch.shape[3]):\n",
        "            coeff_array = np.matmul(np.matmul(X.transpose(),x_batch[i,:,:,j]),X)\n",
        "            x_batchnew[i,:,:,j*4+0]=coeff_array[0:dimhalf,0:dimhalf]\n",
        "            x_batchnew[i,:,:,j*4+1]=coeff_array[0:dimhalf,dimhalf:2*dimhalf]\n",
        "            x_batchnew[i,:,:,j*4+2]=coeff_array[dimhalf:2*dimhalf,0:dimhalf]\n",
        "            x_batchnew[i,:,:,j*4+3]=coeff_array[dimhalf:2*dimhalf,dimhalf:2*dimhalf]\n",
        "    return x_batchnew"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIQpNFyU3SZM",
        "colab_type": "text"
      },
      "source": [
        "# BW calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXdcdlP4raBN",
        "colab_type": "code",
        "outputId": "3919c5ab-aa1b-4c35-9e4b-bd9eff0af8db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "#load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "#convert to float32\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "#convert to DWT\n",
        "#level offset\n",
        "X_train = X_train - 128.0\n",
        "X_test = X_test - 128.0\n",
        "\n",
        "\n",
        "#RGB2YCbCr\n",
        "X_train = batchRGB2YCRCB(X_train)\n",
        "X_test = batchRGB2YCRCB(X_test)\n",
        "\n",
        "M,M_inv = getTcdf97(32)\n",
        "\n",
        "#DWT transform\n",
        "X_train = batchwaveletcdf97mat(X_train,M,16)\n",
        "X_test = batchwaveletcdf97mat(X_test,M,16)\n",
        "\n",
        "#flattening for PCA\n",
        "x_train_ori = X_train.copy()\n",
        "x_train_ori = x_train_ori.astype('float32')\n",
        "x_train = X_train.reshape(X_train.shape[0],-1).astype('float32')\n",
        "x_test = X_test.reshape(X_test.shape[0], -1).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKR0Q3bBr5p5",
        "colab_type": "text"
      },
      "source": [
        "Calculate PCA and choose the components to preserve 99% of the variance - 658 components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccMFSnbpshWd",
        "colab_type": "code",
        "outputId": "c138ed78-2a41-4d5d-e854-0faff4f6d4d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "pca = PCA(0.99)\n",
        "x_train_proj= pca.fit_transform(x_train)\n",
        "x_train_proj = np.floor(x_train_proj/10)*10\n",
        "\n",
        "x_test_proj= pca.transform(x_test)\n",
        "x_test_proj = np.floor(x_test_proj/10)*10\n",
        "\n",
        "\n",
        "print('original representation',x_train.shape)\n",
        "print('reduced representation: ',x_train_proj.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original representation (50000, 3072)\n",
            "reduced representation:  (50000, 687)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0bl0x_SxEoj",
        "colab_type": "code",
        "outputId": "f3a3d734-2c95-4cba-f15d-b9a1cd7c4c93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "print(pca.components_[0,:])\n",
        "print(x_train_proj)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.0042940e-01 -7.9095364e-05  2.6768446e-04 ...  1.3978239e-05\n",
            "  3.5628349e-05 -2.3968446e-06]\n",
            "[[-410.  270.   90. ...  -10.  -10.    0.]\n",
            " [  50.  -60.  420. ...   10.    0.    0.]\n",
            " [ 620. -860. -160. ...    0.  -10.  -10.]\n",
            " ...\n",
            " [  50. -870.   10. ...  -10.    0.  -10.]\n",
            " [ 800. -200.  400. ...    0.  -10.  -10.]\n",
            " [ 320. -120. -340. ...  -10.    0.  -20.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpkwQS0juFcL",
        "colab_type": "text"
      },
      "source": [
        "Display the first few princiapal components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UwCzv5-u9G4",
        "colab_type": "code",
        "outputId": "293c846b-5d60-4d20-aac0-b8e6279dec07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "#Setup a figure 8 inches by 8 inches\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "# plot the components, each image is 26 by 26 pixels\n",
        "for i in range(10):\n",
        "  ax = fig.add_subplot(5, 5, i+1, xticks=[], yticks=[])\n",
        "  ax.imshow(np.reshape(pca.components_[i,:]/np.max(pca.components_[i,:]), (32,32,3)), cmap=plt.cm.bone, interpolation='nearest')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAD4CAYAAADFGxOrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8vEER6nxod-",
        "colab_type": "text"
      },
      "source": [
        "Now recontruct the images after dim reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX0BofgTxnOy",
        "colab_type": "code",
        "outputId": "b4d8c53a-918a-4cc1-9028-e967cd2ce803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "x_train_recon = pca.inverse_transform(x_train_proj)\n",
        "x_train_recon = x_train_recon.reshape(X_train.shape)\n",
        "\n",
        "x_test_recon = pca.inverse_transform(x_test_proj)\n",
        "x_test_recon = x_test_recon.reshape(X_test.shape)\n",
        "\n",
        "#show original images\n",
        "#Setup a figure 8 inches by 8 inches\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "# plot the components, each image is 26 by 26 pixels\n",
        "for i in range(10):\n",
        "  ax = fig.add_subplot(5, 5, i+1, xticks=[], yticks=[])\n",
        "  ax.imshow(np.reshape(x_train_ori[i,:,:,0]/np.max(x_train_ori[i,:,:,0]), (16,16)), cmap=plt.cm.bone, interpolation='nearest')\n",
        "  \n",
        "#recon images\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "# plot the components, each image is 26 by 26 pixels\n",
        "for i in range(10):\n",
        "  ax = fig.add_subplot(5, 5, i+1, xticks=[], yticks=[])\n",
        "  ax.imshow(np.reshape(x_train_recon[i,:,:,0]/np.max(x_train_recon[i,:,:,0]), (16,16)), cmap=plt.cm.bone, interpolation='nearest')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAD4CAYAAADFGxOrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 576x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAD4CAYAAADFGxOrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUjepNQA0xcU",
        "colab_type": "text"
      },
      "source": [
        "Lets compare the BW of roginal and reduced train sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j1h4mqh06Wa",
        "colab_type": "code",
        "outputId": "fbe88ec3-05f8-4675-939b-c3f18527fba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "zeros,BW = HuffmanBW(np.floor(x_train_ori).astype('int32'))\n",
        "print('Original images: % of zeros = ',str(zeros*100),' and BW: ',str(BW),' Bits')\n",
        "zerosr,BWr = HuffmanBW(np.floor(x_train_recon).astype('int32'))\n",
        "print('reduced images: % of zeros = ',str(zerosr*100),' and BW: ',str(BWr),' Bits')\n",
        "#projections\n",
        "zerosp,BWp = HuffmanBW(np.floor(x_train_proj/10).astype('int32'))\n",
        "print('reduced images: % of zeros = ',str(zerosp*100),' and BW: ',str(BWp),' Bits')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original images: % of zeros =  27.667077473958336  and BW:  12417.0  Bits\n",
            "reduced images: % of zeros =  28.055281250000004  and BW:  12619.0  Bits\n",
            "reduced images: % of zeros =  31.283828238719067  and BW:  1915.0  Bits\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5JnwW553XTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from heapq import heappush, heappop, heapify\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "def encode(symb2freq):\n",
        "    \"\"\"Huffman encode the given dict mapping symbols to weights\"\"\"\n",
        "    heap = [[wt, [sym, \"\"]] for sym, wt in symb2freq.items()]\n",
        "    heapify(heap)\n",
        "    while len(heap) > 1:\n",
        "        lo = heappop(heap)\n",
        "        hi = heappop(heap)\n",
        "        for pair in lo[1:]:\n",
        "            pair[1] = '0' + pair[1]\n",
        "        for pair in hi[1:]:\n",
        "            pair[1] = '1' + pair[1]\n",
        "        heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
        "    return sorted(heappop(heap)[1:], key=lambda p: (len(p[-1]), p))\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "layer=0\n",
        "nbits=3\n",
        "\n",
        "\n",
        "####################################################Assume the dataset you want to calculate the average BW is x_test\n",
        "def HuffmanBW(x_test):\n",
        "  bitstream=0\n",
        "  for imnum in range(0,x_test.shape[0]):\n",
        "  #for imnum in tqdm(range(0,1)):\n",
        "      img = x_test[imnum,]\n",
        "      #print(img.shape)\n",
        "      bitcount=0\n",
        "\n",
        "\n",
        "      if(np.min(img) < 0):\n",
        "          img = img - np.min(img)\n",
        "      if(np.max(img)==0):\n",
        "          img[0,0]=1\n",
        "\n",
        "      txt = \"\".join(map(chr, img.flatten()))\n",
        "\n",
        "      symb2freq = defaultdict(int)\n",
        "      for ch in txt:\n",
        "          symb2freq[ch] += 1\n",
        "      # in Python 3.1+:\n",
        "      # symb2freq = collections.Counter(txt)\n",
        "      huff = encode(symb2freq)\n",
        "      #print(\"Symbol\\tWeight\\tHuffman Code\")\n",
        "      #for p in huff:\n",
        "      #    print(\"%s\\t%s\\t%s\" % (p[0], symb2freq[p[0]], p[1]))\n",
        "\n",
        "      ##### Calculating the number of bits needed\n",
        "\n",
        "      for p in huff:\n",
        "          bitcount = bitcount+  symb2freq[p[0]]*len(p[1])\n",
        "      bitstream = bitstream + bitcount\n",
        "      input_bits = len(img.flatten())*nbits\n",
        "      #compression = compression + (1-bitcount/input_bits)*100\t# compression rate\n",
        "\n",
        "  \n",
        "  BW = np.round(bitstream/x_test.shape[0])            ## BW in bits\n",
        "  \n",
        "\n",
        "  return np.mean(x_test==0.0),BW\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxu7e_Lm5gN",
        "colab_type": "text"
      },
      "source": [
        "# ResNet-20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrv9EFGt6EH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Resnet-20\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "import time\n",
        "\n",
        "n=3\n",
        "\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "\n",
        "depth = n * 6 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%d' % (depth)\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=64,\n",
        "                 kernel_size=3, ######################### try to change this and see\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 64 #################################################################### changed ori:16\n",
        "    num_res_blocks = int((depth - 2) / 6)#2############################################################### changed ori: int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters  = int(num_filters*1.5)\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=4)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdvzKxcgpLSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data augmentation\n",
        "def creategen(X,Y,batch_size):\n",
        "    while True:\n",
        "        # suffled indices    \n",
        "        #idx = np.random.permutation( X.shape[0])\n",
        "        # create image generator\n",
        "        datagen = ImageDataGenerator(\n",
        "                \n",
        "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "                samplewise_center=False,  # set each sample mean to 0\n",
        "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "                samplewise_std_normalization=False,  # divide each input by its std\n",
        "                zca_whitening=False,  # apply ZCA whitening\n",
        "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "                horizontal_flip=True,  # randomly flip images\n",
        "                vertical_flip=False)\n",
        "\n",
        "        batches= datagen.flow( X, Y, batch_size=batch_size,shuffle=True)\n",
        "       \n",
        "        idx0 = 0\n",
        "        for batch in batches:\n",
        "            idx1 = idx0 + batch[0].shape[0]\n",
        "            temp = batch[0].astype('float32')\n",
        "            #waveletmy2.batchwaveletcdf97mat(batch[0].astype('float32'),M,16)\n",
        "            #temp = waveletmy2.batchwaveletsArrange(temp)\n",
        "            \n",
        "            yield temp/np.max(np.abs(temp)) , batch[1]\n",
        "\n",
        "            idx0 = idx1\n",
        "            if idx1 >= X.shape[0]:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-nOm5IEnrcv",
        "colab_type": "code",
        "outputId": "33fe269f-9171-41a5-d5fe-6c77a799919d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3198
        }
      },
      "source": [
        "#Let's train\n",
        "\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "x_test=x_test_recon/np.max(np.abs(x_test_recon))\n",
        "print('test shape:',x_test.shape)\n",
        "print('train shape:',x_train_recon.shape)\n",
        "\n",
        "\n",
        "input_shape = x_test.shape[1:]\n",
        "\n",
        "#training a Resnet-20\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-2\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-1\n",
        "    elif epoch > 80:\n",
        "        lr *= 0.5\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "                \n",
        "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(creategen(x_train_recon, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=int(np.ceil(x_train_recon.shape[0]/32.0)),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=epochs, verbose=1, workers=1,\n",
        "                    callbacks=callbacks)\n",
        "start = time.time()\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('time per image :',(time.time()-start)*1000/10000,' ms')\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test shape: (10000, 16, 16, 12)\n",
            "train shape: (50000, 16, 16, 12)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Learning rate:  0.001\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 16, 16, 12)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 16, 16, 64)   6976        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 16, 16, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 16, 16, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 16, 16, 64)   36928       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 64)   36928       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 16, 16, 64)   0           activation_1[0][0]               \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 64)   0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 16, 16, 64)   36928       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16, 16, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16, 16, 64)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 64)   36928       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 64)   0           activation_3[0][0]               \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 16, 16, 64)   0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   36928       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 64)   36928       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 64)   256         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 64)   0           activation_5[0][0]               \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 16, 16, 64)   0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 8, 8, 96)     55392       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 8, 8, 96)     384         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 8, 8, 96)     0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 8, 8, 96)     83040       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 96)     6240        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 8, 8, 96)     384         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 8, 8, 96)     0           conv2d_10[0][0]                  \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 8, 8, 96)     0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 96)     83040       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 96)     384         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 8, 8, 96)     0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 96)     83040       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 96)     384         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 8, 8, 96)     0           activation_9[0][0]               \n",
            "                                                                 batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 8, 8, 96)     0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 8, 8, 96)     83040       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 96)     384         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 8, 8, 96)     0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 8, 8, 96)     83040       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 8, 8, 96)     384         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 96)     0           activation_11[0][0]              \n",
            "                                                                 batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 8, 8, 96)     0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 144)    124560      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 144)    576         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 4, 4, 144)    0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 144)    186768      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 4, 4, 144)    13968       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 144)    576         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 4, 4, 144)    0           conv2d_17[0][0]                  \n",
            "                                                                 batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 4, 4, 144)    0           add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 4, 4, 144)    186768      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 144)    576         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 4, 4, 144)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 4, 4, 144)    186768      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 4, 4, 144)    576         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 4, 4, 144)    0           activation_15[0][0]              \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 4, 4, 144)    0           add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 4, 4, 144)    186768      activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 4, 4, 144)    576         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 4, 4, 144)    0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 4, 4, 144)    186768      activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 4, 4, 144)    576         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 4, 4, 144)    0           activation_17[0][0]              \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 4, 4, 144)    0           add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 144)    0           activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 144)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           1450        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,786,746\n",
            "Trainable params: 1,782,970\n",
            "Non-trainable params: 3,776\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (50000, 16, 16, 12) (12 channels).\n",
            "  str(self.x.shape[channels_axis]) + ' channels).')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n",
            "1487/1563 [===========================>..] - ETA: 1:10 - loss: 1.9106 - acc: 0.4569"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}