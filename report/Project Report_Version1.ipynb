{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests in RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zacco\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13440/50000 [=======>......................] - ETA: 19:17 - loss: 2.3021 - acc: 0.06 - ETA: 7:07 - loss: 2.8216 - acc: 0.0625 - ETA: 4:36 - loss: 2.6219 - acc: 0.068 - ETA: 3:32 - loss: 2.5302 - acc: 0.080 - ETA: 2:58 - loss: 2.4747 - acc: 0.100 - ETA: 2:34 - loss: 2.4461 - acc: 0.105 - ETA: 2:19 - loss: 2.4264 - acc: 0.108 - ETA: 2:08 - loss: 2.4033 - acc: 0.114 - ETA: 1:59 - loss: 2.3866 - acc: 0.115 - ETA: 1:52 - loss: 2.3766 - acc: 0.121 - ETA: 1:46 - loss: 2.3643 - acc: 0.128 - ETA: 1:42 - loss: 2.3588 - acc: 0.139 - ETA: 1:38 - loss: 2.3489 - acc: 0.140 - ETA: 1:35 - loss: 2.3385 - acc: 0.142 - ETA: 1:32 - loss: 2.3331 - acc: 0.141 - ETA: 1:30 - loss: 2.3192 - acc: 0.152 - ETA: 1:27 - loss: 2.3130 - acc: 0.153 - ETA: 1:25 - loss: 2.3254 - acc: 0.148 - ETA: 1:23 - loss: 2.3155 - acc: 0.154 - ETA: 1:22 - loss: 2.3006 - acc: 0.169 - ETA: 1:20 - loss: 2.2924 - acc: 0.169 - ETA: 1:18 - loss: 2.2765 - acc: 0.175 - ETA: 1:17 - loss: 2.2766 - acc: 0.175 - ETA: 1:16 - loss: 2.2683 - acc: 0.178 - ETA: 1:15 - loss: 2.2603 - acc: 0.182 - ETA: 1:14 - loss: 2.2475 - acc: 0.187 - ETA: 1:13 - loss: 2.2451 - acc: 0.185 - ETA: 1:12 - loss: 2.2400 - acc: 0.187 - ETA: 1:11 - loss: 2.2308 - acc: 0.191 - ETA: 1:10 - loss: 2.2236 - acc: 0.195 - ETA: 1:09 - loss: 2.2198 - acc: 0.193 - ETA: 1:08 - loss: 2.2104 - acc: 0.199 - ETA: 1:08 - loss: 2.2018 - acc: 0.201 - ETA: 1:07 - loss: 2.1924 - acc: 0.204 - ETA: 1:07 - loss: 2.1901 - acc: 0.207 - ETA: 1:06 - loss: 2.1831 - acc: 0.211 - ETA: 1:06 - loss: 2.1745 - acc: 0.216 - ETA: 1:05 - loss: 2.1673 - acc: 0.217 - ETA: 1:05 - loss: 2.1603 - acc: 0.218 - ETA: 1:04 - loss: 2.1592 - acc: 0.218 - ETA: 1:04 - loss: 2.1513 - acc: 0.221 - ETA: 1:03 - loss: 2.1442 - acc: 0.225 - ETA: 1:03 - loss: 2.1367 - acc: 0.230 - ETA: 1:03 - loss: 2.1324 - acc: 0.231 - ETA: 1:02 - loss: 2.1270 - acc: 0.234 - ETA: 1:02 - loss: 2.1242 - acc: 0.233 - ETA: 1:02 - loss: 2.1220 - acc: 0.236 - ETA: 1:01 - loss: 2.1166 - acc: 0.238 - ETA: 1:01 - loss: 2.1136 - acc: 0.237 - ETA: 1:01 - loss: 2.1088 - acc: 0.238 - ETA: 1:00 - loss: 2.1031 - acc: 0.239 - ETA: 1:00 - loss: 2.0996 - acc: 0.240 - ETA: 1:00 - loss: 2.0971 - acc: 0.241 - ETA: 59s - loss: 2.0909 - acc: 0.245 - ETA: 59s - loss: 2.0800 - acc: 0.24 - ETA: 59s - loss: 2.0765 - acc: 0.25 - ETA: 58s - loss: 2.0740 - acc: 0.25 - ETA: 58s - loss: 2.0691 - acc: 0.25 - ETA: 58s - loss: 2.0644 - acc: 0.25 - ETA: 58s - loss: 2.0585 - acc: 0.25 - ETA: 58s - loss: 2.0559 - acc: 0.25 - ETA: 57s - loss: 2.0517 - acc: 0.26 - ETA: 57s - loss: 2.0484 - acc: 0.26 - ETA: 57s - loss: 2.0443 - acc: 0.26 - ETA: 57s - loss: 2.0389 - acc: 0.26 - ETA: 57s - loss: 2.0329 - acc: 0.26 - ETA: 57s - loss: 2.0291 - acc: 0.26 - ETA: 57s - loss: 2.0270 - acc: 0.26 - ETA: 57s - loss: 2.0253 - acc: 0.26 - ETA: 57s - loss: 2.0202 - acc: 0.27 - ETA: 57s - loss: 2.0167 - acc: 0.27 - ETA: 57s - loss: 2.0124 - acc: 0.27 - ETA: 56s - loss: 2.0085 - acc: 0.27 - ETA: 56s - loss: 2.0043 - acc: 0.27 - ETA: 56s - loss: 1.9993 - acc: 0.27 - ETA: 56s - loss: 1.9995 - acc: 0.27 - ETA: 56s - loss: 1.9977 - acc: 0.27 - ETA: 56s - loss: 1.9961 - acc: 0.27 - ETA: 56s - loss: 1.9946 - acc: 0.28 - ETA: 56s - loss: 1.9894 - acc: 0.28 - ETA: 56s - loss: 1.9914 - acc: 0.28 - ETA: 56s - loss: 1.9889 - acc: 0.28 - ETA: 56s - loss: 1.9889 - acc: 0.28 - ETA: 55s - loss: 1.9848 - acc: 0.28 - ETA: 55s - loss: 1.9831 - acc: 0.28 - ETA: 55s - loss: 1.9794 - acc: 0.28 - ETA: 55s - loss: 1.9757 - acc: 0.28 - ETA: 55s - loss: 1.9742 - acc: 0.28 - ETA: 55s - loss: 1.9743 - acc: 0.28 - ETA: 55s - loss: 1.9720 - acc: 0.28 - ETA: 55s - loss: 1.9695 - acc: 0.29 - ETA: 55s - loss: 1.9669 - acc: 0.29 - ETA: 55s - loss: 1.9631 - acc: 0.29 - ETA: 55s - loss: 1.9591 - acc: 0.29 - ETA: 54s - loss: 1.9552 - acc: 0.29 - ETA: 54s - loss: 1.9550 - acc: 0.29 - ETA: 54s - loss: 1.9525 - acc: 0.29 - ETA: 54s - loss: 1.9503 - acc: 0.29 - ETA: 54s - loss: 1.9469 - acc: 0.29 - ETA: 54s - loss: 1.9426 - acc: 0.30 - ETA: 54s - loss: 1.9396 - acc: 0.30 - ETA: 54s - loss: 1.9385 - acc: 0.30 - ETA: 54s - loss: 1.9377 - acc: 0.30 - ETA: 54s - loss: 1.9333 - acc: 0.30 - ETA: 53s - loss: 1.9310 - acc: 0.30 - ETA: 53s - loss: 1.9310 - acc: 0.30 - ETA: 53s - loss: 1.9270 - acc: 0.30 - ETA: 53s - loss: 1.9255 - acc: 0.30 - ETA: 53s - loss: 1.9247 - acc: 0.30 - ETA: 53s - loss: 1.9230 - acc: 0.30 - ETA: 53s - loss: 1.9211 - acc: 0.31 - ETA: 53s - loss: 1.9193 - acc: 0.31 - ETA: 53s - loss: 1.9169 - acc: 0.31 - ETA: 53s - loss: 1.9145 - acc: 0.31 - ETA: 53s - loss: 1.9127 - acc: 0.31 - ETA: 53s - loss: 1.9099 - acc: 0.31 - ETA: 52s - loss: 1.9094 - acc: 0.31 - ETA: 52s - loss: 1.9068 - acc: 0.31 - ETA: 52s - loss: 1.9043 - acc: 0.31 - ETA: 52s - loss: 1.9036 - acc: 0.31 - ETA: 52s - loss: 1.9016 - acc: 0.31 - ETA: 52s - loss: 1.9000 - acc: 0.31 - ETA: 52s - loss: 1.8973 - acc: 0.31 - ETA: 52s - loss: 1.8968 - acc: 0.31 - ETA: 52s - loss: 1.8961 - acc: 0.31 - ETA: 52s - loss: 1.8947 - acc: 0.31 - ETA: 52s - loss: 1.8949 - acc: 0.31 - ETA: 52s - loss: 1.8919 - acc: 0.32 - ETA: 51s - loss: 1.8901 - acc: 0.32 - ETA: 51s - loss: 1.8885 - acc: 0.32 - ETA: 51s - loss: 1.8866 - acc: 0.32 - ETA: 51s - loss: 1.8848 - acc: 0.32 - ETA: 51s - loss: 1.8847 - acc: 0.32 - ETA: 51s - loss: 1.8828 - acc: 0.32 - ETA: 51s - loss: 1.8810 - acc: 0.32 - ETA: 51s - loss: 1.8788 - acc: 0.32 - ETA: 51s - loss: 1.8759 - acc: 0.32 - ETA: 51s - loss: 1.8732 - acc: 0.32 - ETA: 51s - loss: 1.8730 - acc: 0.32 - ETA: 50s - loss: 1.8703 - acc: 0.32 - ETA: 50s - loss: 1.8673 - acc: 0.33 - ETA: 50s - loss: 1.8652 - acc: 0.33 - ETA: 50s - loss: 1.8625 - acc: 0.33 - ETA: 50s - loss: 1.8610 - acc: 0.33 - ETA: 50s - loss: 1.8577 - acc: 0.33 - ETA: 50s - loss: 1.8550 - acc: 0.33 - ETA: 50s - loss: 1.8525 - acc: 0.33 - ETA: 50s - loss: 1.8491 - acc: 0.33 - ETA: 50s - loss: 1.8470 - acc: 0.33 - ETA: 50s - loss: 1.8458 - acc: 0.33 - ETA: 50s - loss: 1.8450 - acc: 0.33 - ETA: 50s - loss: 1.8425 - acc: 0.33 - ETA: 50s - loss: 1.8404 - acc: 0.33 - ETA: 49s - loss: 1.8385 - acc: 0.34 - ETA: 49s - loss: 1.8361 - acc: 0.34 - ETA: 49s - loss: 1.8344 - acc: 0.34 - ETA: 49s - loss: 1.8300 - acc: 0.34 - ETA: 49s - loss: 1.8282 - acc: 0.34 - ETA: 49s - loss: 1.8277 - acc: 0.34 - ETA: 49s - loss: 1.8265 - acc: 0.34 - ETA: 49s - loss: 1.8250 - acc: 0.34 - ETA: 49s - loss: 1.8238 - acc: 0.34 - ETA: 49s - loss: 1.8217 - acc: 0.34 - ETA: 49s - loss: 1.8204 - acc: 0.34 - ETA: 49s - loss: 1.8206 - acc: 0.34 - ETA: 48s - loss: 1.8199 - acc: 0.34 - ETA: 48s - loss: 1.8184 - acc: 0.34 - ETA: 48s - loss: 1.8172 - acc: 0.34 - ETA: 48s - loss: 1.8153 - acc: 0.34 - ETA: 48s - loss: 1.8131 - acc: 0.34 - ETA: 48s - loss: 1.8122 - acc: 0.34 - ETA: 48s - loss: 1.8122 - acc: 0.34 - ETA: 48s - loss: 1.8109 - acc: 0.35 - ETA: 48s - loss: 1.8090 - acc: 0.35 - ETA: 48s - loss: 1.8062 - acc: 0.35 - ETA: 48s - loss: 1.8050 - acc: 0.35 - ETA: 48s - loss: 1.8046 - acc: 0.35 - ETA: 47s - loss: 1.8028 - acc: 0.35 - ETA: 47s - loss: 1.8007 - acc: 0.35 - ETA: 47s - loss: 1.7981 - acc: 0.35 - ETA: 47s - loss: 1.7985 - acc: 0.35 - ETA: 47s - loss: 1.7969 - acc: 0.35 - ETA: 47s - loss: 1.7959 - acc: 0.35 - ETA: 47s - loss: 1.7942 - acc: 0.35 - ETA: 47s - loss: 1.7926 - acc: 0.35 - ETA: 47s - loss: 1.7928 - acc: 0.35 - ETA: 47s - loss: 1.7916 - acc: 0.35 - ETA: 47s - loss: 1.7908 - acc: 0.35 - ETA: 47s - loss: 1.7898 - acc: 0.35 - ETA: 46s - loss: 1.7885 - acc: 0.35 - ETA: 46s - loss: 1.7877 - acc: 0.35 - ETA: 46s - loss: 1.7868 - acc: 0.35 - ETA: 46s - loss: 1.7855 - acc: 0.35 - ETA: 46s - loss: 1.7830 - acc: 0.36 - ETA: 46s - loss: 1.7824 - acc: 0.36 - ETA: 46s - loss: 1.7808 - acc: 0.36 - ETA: 46s - loss: 1.7803 - acc: 0.36 - ETA: 46s - loss: 1.7785 - acc: 0.36 - ETA: 46s - loss: 1.7780 - acc: 0.36 - ETA: 46s - loss: 1.7766 - acc: 0.36 - ETA: 46s - loss: 1.7743 - acc: 0.36 - ETA: 45s - loss: 1.7728 - acc: 0.36 - ETA: 45s - loss: 1.7719 - acc: 0.36 - ETA: 45s - loss: 1.7707 - acc: 0.36 - ETA: 45s - loss: 1.7690 - acc: 0.36 - ETA: 45s - loss: 1.7675 - acc: 0.36 - ETA: 45s - loss: 1.7663 - acc: 0.36 - ETA: 45s - loss: 1.7641 - acc: 0.36 - ETA: 45s - loss: 1.7646 - acc: 0.36 - ETA: 45s - loss: 1.7630 - acc: 0.36 - ETA: 45s - loss: 1.7617 - acc: 0.36 - ETA: 45s - loss: 1.7610 - acc: 0.3677"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27168/50000 [===============>..............] - ETA: 45s - loss: 1.7607 - acc: 0.36 - ETA: 45s - loss: 1.7597 - acc: 0.36 - ETA: 44s - loss: 1.7587 - acc: 0.36 - ETA: 44s - loss: 1.7587 - acc: 0.36 - ETA: 44s - loss: 1.7589 - acc: 0.36 - ETA: 44s - loss: 1.7574 - acc: 0.36 - ETA: 44s - loss: 1.7555 - acc: 0.36 - ETA: 44s - loss: 1.7551 - acc: 0.37 - ETA: 44s - loss: 1.7530 - acc: 0.37 - ETA: 44s - loss: 1.7508 - acc: 0.37 - ETA: 44s - loss: 1.7494 - acc: 0.37 - ETA: 44s - loss: 1.7480 - acc: 0.37 - ETA: 44s - loss: 1.7470 - acc: 0.37 - ETA: 43s - loss: 1.7459 - acc: 0.37 - ETA: 43s - loss: 1.7447 - acc: 0.37 - ETA: 43s - loss: 1.7425 - acc: 0.37 - ETA: 43s - loss: 1.7419 - acc: 0.37 - ETA: 43s - loss: 1.7399 - acc: 0.37 - ETA: 43s - loss: 1.7401 - acc: 0.37 - ETA: 43s - loss: 1.7404 - acc: 0.37 - ETA: 43s - loss: 1.7392 - acc: 0.37 - ETA: 43s - loss: 1.7385 - acc: 0.37 - ETA: 43s - loss: 1.7370 - acc: 0.37 - ETA: 43s - loss: 1.7354 - acc: 0.37 - ETA: 42s - loss: 1.7345 - acc: 0.37 - ETA: 42s - loss: 1.7346 - acc: 0.37 - ETA: 42s - loss: 1.7346 - acc: 0.37 - ETA: 42s - loss: 1.7330 - acc: 0.37 - ETA: 42s - loss: 1.7320 - acc: 0.37 - ETA: 42s - loss: 1.7319 - acc: 0.37 - ETA: 42s - loss: 1.7309 - acc: 0.37 - ETA: 42s - loss: 1.7299 - acc: 0.37 - ETA: 42s - loss: 1.7281 - acc: 0.38 - ETA: 42s - loss: 1.7270 - acc: 0.38 - ETA: 42s - loss: 1.7263 - acc: 0.38 - ETA: 41s - loss: 1.7249 - acc: 0.38 - ETA: 41s - loss: 1.7238 - acc: 0.38 - ETA: 41s - loss: 1.7216 - acc: 0.38 - ETA: 41s - loss: 1.7212 - acc: 0.38 - ETA: 41s - loss: 1.7203 - acc: 0.38 - ETA: 41s - loss: 1.7192 - acc: 0.38 - ETA: 41s - loss: 1.7178 - acc: 0.38 - ETA: 41s - loss: 1.7171 - acc: 0.38 - ETA: 41s - loss: 1.7160 - acc: 0.38 - ETA: 41s - loss: 1.7153 - acc: 0.38 - ETA: 41s - loss: 1.7135 - acc: 0.38 - ETA: 41s - loss: 1.7126 - acc: 0.38 - ETA: 40s - loss: 1.7117 - acc: 0.38 - ETA: 40s - loss: 1.7099 - acc: 0.38 - ETA: 40s - loss: 1.7087 - acc: 0.38 - ETA: 40s - loss: 1.7085 - acc: 0.38 - ETA: 40s - loss: 1.7081 - acc: 0.38 - ETA: 40s - loss: 1.7070 - acc: 0.38 - ETA: 40s - loss: 1.7070 - acc: 0.38 - ETA: 40s - loss: 1.7054 - acc: 0.38 - ETA: 40s - loss: 1.7042 - acc: 0.38 - ETA: 40s - loss: 1.7026 - acc: 0.38 - ETA: 40s - loss: 1.7019 - acc: 0.38 - ETA: 40s - loss: 1.7002 - acc: 0.38 - ETA: 39s - loss: 1.7003 - acc: 0.38 - ETA: 39s - loss: 1.6994 - acc: 0.38 - ETA: 39s - loss: 1.6976 - acc: 0.39 - ETA: 39s - loss: 1.6970 - acc: 0.39 - ETA: 39s - loss: 1.6964 - acc: 0.39 - ETA: 39s - loss: 1.6954 - acc: 0.39 - ETA: 39s - loss: 1.6941 - acc: 0.39 - ETA: 39s - loss: 1.6936 - acc: 0.39 - ETA: 39s - loss: 1.6926 - acc: 0.39 - ETA: 39s - loss: 1.6925 - acc: 0.39 - ETA: 39s - loss: 1.6919 - acc: 0.39 - ETA: 39s - loss: 1.6910 - acc: 0.39 - ETA: 38s - loss: 1.6899 - acc: 0.39 - ETA: 38s - loss: 1.6890 - acc: 0.39 - ETA: 38s - loss: 1.6883 - acc: 0.39 - ETA: 38s - loss: 1.6866 - acc: 0.39 - ETA: 38s - loss: 1.6858 - acc: 0.39 - ETA: 38s - loss: 1.6858 - acc: 0.39 - ETA: 38s - loss: 1.6850 - acc: 0.39 - ETA: 38s - loss: 1.6833 - acc: 0.39 - ETA: 38s - loss: 1.6829 - acc: 0.39 - ETA: 38s - loss: 1.6815 - acc: 0.39 - ETA: 38s - loss: 1.6799 - acc: 0.39 - ETA: 38s - loss: 1.6797 - acc: 0.39 - ETA: 37s - loss: 1.6789 - acc: 0.39 - ETA: 37s - loss: 1.6780 - acc: 0.39 - ETA: 37s - loss: 1.6770 - acc: 0.39 - ETA: 37s - loss: 1.6769 - acc: 0.39 - ETA: 37s - loss: 1.6758 - acc: 0.39 - ETA: 37s - loss: 1.6749 - acc: 0.39 - ETA: 37s - loss: 1.6745 - acc: 0.39 - ETA: 37s - loss: 1.6733 - acc: 0.39 - ETA: 37s - loss: 1.6723 - acc: 0.39 - ETA: 37s - loss: 1.6728 - acc: 0.39 - ETA: 37s - loss: 1.6720 - acc: 0.39 - ETA: 37s - loss: 1.6708 - acc: 0.39 - ETA: 36s - loss: 1.6698 - acc: 0.39 - ETA: 36s - loss: 1.6685 - acc: 0.39 - ETA: 36s - loss: 1.6685 - acc: 0.39 - ETA: 36s - loss: 1.6675 - acc: 0.40 - ETA: 36s - loss: 1.6668 - acc: 0.40 - ETA: 36s - loss: 1.6655 - acc: 0.40 - ETA: 36s - loss: 1.6653 - acc: 0.40 - ETA: 36s - loss: 1.6648 - acc: 0.40 - ETA: 36s - loss: 1.6637 - acc: 0.40 - ETA: 36s - loss: 1.6629 - acc: 0.40 - ETA: 36s - loss: 1.6617 - acc: 0.40 - ETA: 36s - loss: 1.6614 - acc: 0.40 - ETA: 35s - loss: 1.6600 - acc: 0.40 - ETA: 35s - loss: 1.6598 - acc: 0.40 - ETA: 35s - loss: 1.6584 - acc: 0.40 - ETA: 35s - loss: 1.6592 - acc: 0.40 - ETA: 35s - loss: 1.6590 - acc: 0.40 - ETA: 35s - loss: 1.6582 - acc: 0.40 - ETA: 35s - loss: 1.6574 - acc: 0.40 - ETA: 35s - loss: 1.6559 - acc: 0.40 - ETA: 35s - loss: 1.6551 - acc: 0.40 - ETA: 35s - loss: 1.6541 - acc: 0.40 - ETA: 35s - loss: 1.6532 - acc: 0.40 - ETA: 35s - loss: 1.6526 - acc: 0.40 - ETA: 34s - loss: 1.6510 - acc: 0.40 - ETA: 34s - loss: 1.6504 - acc: 0.40 - ETA: 34s - loss: 1.6502 - acc: 0.40 - ETA: 34s - loss: 1.6496 - acc: 0.40 - ETA: 34s - loss: 1.6494 - acc: 0.40 - ETA: 34s - loss: 1.6492 - acc: 0.40 - ETA: 34s - loss: 1.6487 - acc: 0.40 - ETA: 34s - loss: 1.6478 - acc: 0.40 - ETA: 34s - loss: 1.6469 - acc: 0.40 - ETA: 34s - loss: 1.6461 - acc: 0.40 - ETA: 34s - loss: 1.6456 - acc: 0.40 - ETA: 34s - loss: 1.6445 - acc: 0.40 - ETA: 34s - loss: 1.6434 - acc: 0.40 - ETA: 33s - loss: 1.6427 - acc: 0.40 - ETA: 33s - loss: 1.6424 - acc: 0.40 - ETA: 33s - loss: 1.6422 - acc: 0.41 - ETA: 33s - loss: 1.6413 - acc: 0.41 - ETA: 33s - loss: 1.6409 - acc: 0.41 - ETA: 33s - loss: 1.6399 - acc: 0.41 - ETA: 33s - loss: 1.6391 - acc: 0.41 - ETA: 33s - loss: 1.6380 - acc: 0.41 - ETA: 33s - loss: 1.6366 - acc: 0.41 - ETA: 33s - loss: 1.6350 - acc: 0.41 - ETA: 33s - loss: 1.6340 - acc: 0.41 - ETA: 33s - loss: 1.6338 - acc: 0.41 - ETA: 33s - loss: 1.6331 - acc: 0.41 - ETA: 32s - loss: 1.6317 - acc: 0.41 - ETA: 32s - loss: 1.6311 - acc: 0.41 - ETA: 32s - loss: 1.6299 - acc: 0.41 - ETA: 32s - loss: 1.6295 - acc: 0.41 - ETA: 32s - loss: 1.6286 - acc: 0.41 - ETA: 32s - loss: 1.6280 - acc: 0.41 - ETA: 32s - loss: 1.6274 - acc: 0.41 - ETA: 32s - loss: 1.6272 - acc: 0.41 - ETA: 32s - loss: 1.6265 - acc: 0.41 - ETA: 32s - loss: 1.6249 - acc: 0.41 - ETA: 32s - loss: 1.6249 - acc: 0.41 - ETA: 32s - loss: 1.6240 - acc: 0.41 - ETA: 32s - loss: 1.6236 - acc: 0.41 - ETA: 32s - loss: 1.6233 - acc: 0.41 - ETA: 31s - loss: 1.6219 - acc: 0.41 - ETA: 31s - loss: 1.6207 - acc: 0.41 - ETA: 31s - loss: 1.6205 - acc: 0.41 - ETA: 31s - loss: 1.6200 - acc: 0.41 - ETA: 31s - loss: 1.6195 - acc: 0.41 - ETA: 31s - loss: 1.6187 - acc: 0.41 - ETA: 31s - loss: 1.6181 - acc: 0.41 - ETA: 31s - loss: 1.6171 - acc: 0.42 - ETA: 31s - loss: 1.6167 - acc: 0.42 - ETA: 31s - loss: 1.6159 - acc: 0.42 - ETA: 31s - loss: 1.6154 - acc: 0.42 - ETA: 31s - loss: 1.6141 - acc: 0.42 - ETA: 31s - loss: 1.6133 - acc: 0.42 - ETA: 30s - loss: 1.6127 - acc: 0.42 - ETA: 30s - loss: 1.6116 - acc: 0.42 - ETA: 30s - loss: 1.6110 - acc: 0.42 - ETA: 30s - loss: 1.6104 - acc: 0.42 - ETA: 30s - loss: 1.6095 - acc: 0.42 - ETA: 30s - loss: 1.6088 - acc: 0.42 - ETA: 30s - loss: 1.6076 - acc: 0.42 - ETA: 30s - loss: 1.6067 - acc: 0.42 - ETA: 30s - loss: 1.6067 - acc: 0.42 - ETA: 30s - loss: 1.6065 - acc: 0.42 - ETA: 30s - loss: 1.6065 - acc: 0.42 - ETA: 30s - loss: 1.6054 - acc: 0.42 - ETA: 30s - loss: 1.6047 - acc: 0.42 - ETA: 29s - loss: 1.6037 - acc: 0.42 - ETA: 29s - loss: 1.6029 - acc: 0.42 - ETA: 29s - loss: 1.6017 - acc: 0.42 - ETA: 29s - loss: 1.6013 - acc: 0.42 - ETA: 29s - loss: 1.6009 - acc: 0.42 - ETA: 29s - loss: 1.6007 - acc: 0.42 - ETA: 29s - loss: 1.5997 - acc: 0.42 - ETA: 29s - loss: 1.5992 - acc: 0.42 - ETA: 29s - loss: 1.5990 - acc: 0.42 - ETA: 29s - loss: 1.5979 - acc: 0.42 - ETA: 29s - loss: 1.5973 - acc: 0.42 - ETA: 29s - loss: 1.5965 - acc: 0.42 - ETA: 29s - loss: 1.5962 - acc: 0.42 - ETA: 28s - loss: 1.5953 - acc: 0.42 - ETA: 28s - loss: 1.5943 - acc: 0.42 - ETA: 28s - loss: 1.5933 - acc: 0.42 - ETA: 28s - loss: 1.5926 - acc: 0.42 - ETA: 28s - loss: 1.5923 - acc: 0.42 - ETA: 28s - loss: 1.5914 - acc: 0.42 - ETA: 28s - loss: 1.5917 - acc: 0.42 - ETA: 28s - loss: 1.5910 - acc: 0.43 - ETA: 28s - loss: 1.5906 - acc: 0.43 - ETA: 28s - loss: 1.5901 - acc: 0.43 - ETA: 28s - loss: 1.5894 - acc: 0.43 - ETA: 28s - loss: 1.5896 - acc: 0.43 - ETA: 28s - loss: 1.5889 - acc: 0.43 - ETA: 28s - loss: 1.5882 - acc: 0.43 - ETA: 27s - loss: 1.5876 - acc: 0.43 - ETA: 27s - loss: 1.5866 - acc: 0.43 - ETA: 27s - loss: 1.5864 - acc: 0.4320"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40896/50000 [=======================>......] - ETA: 27s - loss: 1.5863 - acc: 0.43 - ETA: 27s - loss: 1.5860 - acc: 0.43 - ETA: 27s - loss: 1.5857 - acc: 0.43 - ETA: 27s - loss: 1.5855 - acc: 0.43 - ETA: 27s - loss: 1.5850 - acc: 0.43 - ETA: 27s - loss: 1.5839 - acc: 0.43 - ETA: 27s - loss: 1.5832 - acc: 0.43 - ETA: 27s - loss: 1.5833 - acc: 0.43 - ETA: 27s - loss: 1.5824 - acc: 0.43 - ETA: 27s - loss: 1.5813 - acc: 0.43 - ETA: 26s - loss: 1.5808 - acc: 0.43 - ETA: 26s - loss: 1.5802 - acc: 0.43 - ETA: 26s - loss: 1.5798 - acc: 0.43 - ETA: 26s - loss: 1.5797 - acc: 0.43 - ETA: 26s - loss: 1.5790 - acc: 0.43 - ETA: 26s - loss: 1.5777 - acc: 0.43 - ETA: 26s - loss: 1.5767 - acc: 0.43 - ETA: 26s - loss: 1.5762 - acc: 0.43 - ETA: 26s - loss: 1.5762 - acc: 0.43 - ETA: 26s - loss: 1.5759 - acc: 0.43 - ETA: 26s - loss: 1.5757 - acc: 0.43 - ETA: 26s - loss: 1.5750 - acc: 0.43 - ETA: 25s - loss: 1.5744 - acc: 0.43 - ETA: 25s - loss: 1.5740 - acc: 0.43 - ETA: 25s - loss: 1.5734 - acc: 0.43 - ETA: 25s - loss: 1.5731 - acc: 0.43 - ETA: 25s - loss: 1.5727 - acc: 0.43 - ETA: 25s - loss: 1.5718 - acc: 0.43 - ETA: 25s - loss: 1.5719 - acc: 0.43 - ETA: 25s - loss: 1.5715 - acc: 0.43 - ETA: 25s - loss: 1.5715 - acc: 0.43 - ETA: 25s - loss: 1.5713 - acc: 0.43 - ETA: 25s - loss: 1.5708 - acc: 0.43 - ETA: 25s - loss: 1.5703 - acc: 0.43 - ETA: 24s - loss: 1.5692 - acc: 0.43 - ETA: 24s - loss: 1.5684 - acc: 0.43 - ETA: 24s - loss: 1.5679 - acc: 0.43 - ETA: 24s - loss: 1.5668 - acc: 0.43 - ETA: 24s - loss: 1.5663 - acc: 0.44 - ETA: 24s - loss: 1.5654 - acc: 0.44 - ETA: 24s - loss: 1.5650 - acc: 0.44 - ETA: 24s - loss: 1.5649 - acc: 0.44 - ETA: 24s - loss: 1.5640 - acc: 0.44 - ETA: 24s - loss: 1.5636 - acc: 0.44 - ETA: 24s - loss: 1.5627 - acc: 0.44 - ETA: 24s - loss: 1.5630 - acc: 0.44 - ETA: 24s - loss: 1.5625 - acc: 0.44 - ETA: 23s - loss: 1.5617 - acc: 0.44 - ETA: 23s - loss: 1.5608 - acc: 0.44 - ETA: 23s - loss: 1.5599 - acc: 0.44 - ETA: 23s - loss: 1.5592 - acc: 0.44 - ETA: 23s - loss: 1.5582 - acc: 0.44 - ETA: 23s - loss: 1.5572 - acc: 0.44 - ETA: 23s - loss: 1.5570 - acc: 0.44 - ETA: 23s - loss: 1.5566 - acc: 0.44 - ETA: 23s - loss: 1.5558 - acc: 0.44 - ETA: 23s - loss: 1.5555 - acc: 0.44 - ETA: 23s - loss: 1.5549 - acc: 0.44 - ETA: 23s - loss: 1.5544 - acc: 0.44 - ETA: 22s - loss: 1.5535 - acc: 0.44 - ETA: 22s - loss: 1.5530 - acc: 0.44 - ETA: 22s - loss: 1.5530 - acc: 0.44 - ETA: 22s - loss: 1.5529 - acc: 0.44 - ETA: 22s - loss: 1.5520 - acc: 0.44 - ETA: 22s - loss: 1.5514 - acc: 0.44 - ETA: 22s - loss: 1.5507 - acc: 0.44 - ETA: 22s - loss: 1.5504 - acc: 0.44 - ETA: 22s - loss: 1.5504 - acc: 0.44 - ETA: 22s - loss: 1.5499 - acc: 0.44 - ETA: 22s - loss: 1.5495 - acc: 0.44 - ETA: 22s - loss: 1.5491 - acc: 0.44 - ETA: 22s - loss: 1.5489 - acc: 0.44 - ETA: 21s - loss: 1.5485 - acc: 0.44 - ETA: 21s - loss: 1.5475 - acc: 0.44 - ETA: 21s - loss: 1.5471 - acc: 0.44 - ETA: 21s - loss: 1.5463 - acc: 0.44 - ETA: 21s - loss: 1.5457 - acc: 0.44 - ETA: 21s - loss: 1.5457 - acc: 0.44 - ETA: 21s - loss: 1.5452 - acc: 0.44 - ETA: 21s - loss: 1.5445 - acc: 0.44 - ETA: 21s - loss: 1.5435 - acc: 0.44 - ETA: 21s - loss: 1.5433 - acc: 0.44 - ETA: 21s - loss: 1.5429 - acc: 0.44 - ETA: 21s - loss: 1.5427 - acc: 0.44 - ETA: 20s - loss: 1.5424 - acc: 0.44 - ETA: 20s - loss: 1.5418 - acc: 0.44 - ETA: 20s - loss: 1.5411 - acc: 0.44 - ETA: 20s - loss: 1.5403 - acc: 0.44 - ETA: 20s - loss: 1.5397 - acc: 0.45 - ETA: 20s - loss: 1.5392 - acc: 0.45 - ETA: 20s - loss: 1.5387 - acc: 0.45 - ETA: 20s - loss: 1.5380 - acc: 0.45 - ETA: 20s - loss: 1.5372 - acc: 0.45 - ETA: 20s - loss: 1.5369 - acc: 0.45 - ETA: 20s - loss: 1.5365 - acc: 0.45 - ETA: 20s - loss: 1.5361 - acc: 0.45 - ETA: 20s - loss: 1.5359 - acc: 0.45 - ETA: 19s - loss: 1.5356 - acc: 0.45 - ETA: 19s - loss: 1.5349 - acc: 0.45 - ETA: 19s - loss: 1.5349 - acc: 0.45 - ETA: 19s - loss: 1.5345 - acc: 0.45 - ETA: 19s - loss: 1.5341 - acc: 0.45 - ETA: 19s - loss: 1.5339 - acc: 0.45 - ETA: 19s - loss: 1.5331 - acc: 0.45 - ETA: 19s - loss: 1.5325 - acc: 0.45 - ETA: 19s - loss: 1.5323 - acc: 0.45 - ETA: 19s - loss: 1.5315 - acc: 0.45 - ETA: 19s - loss: 1.5307 - acc: 0.45 - ETA: 19s - loss: 1.5301 - acc: 0.45 - ETA: 19s - loss: 1.5299 - acc: 0.45 - ETA: 18s - loss: 1.5291 - acc: 0.45 - ETA: 18s - loss: 1.5290 - acc: 0.45 - ETA: 18s - loss: 1.5288 - acc: 0.45 - ETA: 18s - loss: 1.5285 - acc: 0.45 - ETA: 18s - loss: 1.5280 - acc: 0.45 - ETA: 18s - loss: 1.5273 - acc: 0.45 - ETA: 18s - loss: 1.5264 - acc: 0.45 - ETA: 18s - loss: 1.5259 - acc: 0.45 - ETA: 18s - loss: 1.5258 - acc: 0.45 - ETA: 18s - loss: 1.5251 - acc: 0.45 - ETA: 18s - loss: 1.5249 - acc: 0.45 - ETA: 18s - loss: 1.5246 - acc: 0.45 - ETA: 18s - loss: 1.5244 - acc: 0.45 - ETA: 17s - loss: 1.5235 - acc: 0.45 - ETA: 17s - loss: 1.5232 - acc: 0.45 - ETA: 17s - loss: 1.5227 - acc: 0.45 - ETA: 17s - loss: 1.5221 - acc: 0.45 - ETA: 17s - loss: 1.5216 - acc: 0.45 - ETA: 17s - loss: 1.5213 - acc: 0.45 - ETA: 17s - loss: 1.5206 - acc: 0.45 - ETA: 17s - loss: 1.5204 - acc: 0.45 - ETA: 17s - loss: 1.5198 - acc: 0.45 - ETA: 17s - loss: 1.5194 - acc: 0.45 - ETA: 17s - loss: 1.5190 - acc: 0.45 - ETA: 17s - loss: 1.5186 - acc: 0.45 - ETA: 17s - loss: 1.5181 - acc: 0.45 - ETA: 16s - loss: 1.5179 - acc: 0.45 - ETA: 16s - loss: 1.5171 - acc: 0.45 - ETA: 16s - loss: 1.5166 - acc: 0.45 - ETA: 16s - loss: 1.5164 - acc: 0.45 - ETA: 16s - loss: 1.5157 - acc: 0.45 - ETA: 16s - loss: 1.5147 - acc: 0.45 - ETA: 16s - loss: 1.5143 - acc: 0.45 - ETA: 16s - loss: 1.5136 - acc: 0.46 - ETA: 16s - loss: 1.5136 - acc: 0.46 - ETA: 16s - loss: 1.5125 - acc: 0.46 - ETA: 16s - loss: 1.5121 - acc: 0.46 - ETA: 16s - loss: 1.5117 - acc: 0.46 - ETA: 15s - loss: 1.5109 - acc: 0.46 - ETA: 15s - loss: 1.5100 - acc: 0.46 - ETA: 15s - loss: 1.5096 - acc: 0.46 - ETA: 15s - loss: 1.5091 - acc: 0.46 - ETA: 15s - loss: 1.5084 - acc: 0.46 - ETA: 15s - loss: 1.5084 - acc: 0.46 - ETA: 15s - loss: 1.5078 - acc: 0.46 - ETA: 15s - loss: 1.5070 - acc: 0.46 - ETA: 15s - loss: 1.5066 - acc: 0.46 - ETA: 15s - loss: 1.5062 - acc: 0.46 - ETA: 15s - loss: 1.5056 - acc: 0.46 - ETA: 15s - loss: 1.5055 - acc: 0.46 - ETA: 15s - loss: 1.5049 - acc: 0.46 - ETA: 14s - loss: 1.5043 - acc: 0.46 - ETA: 14s - loss: 1.5037 - acc: 0.46 - ETA: 14s - loss: 1.5030 - acc: 0.46 - ETA: 14s - loss: 1.5027 - acc: 0.46 - ETA: 14s - loss: 1.5023 - acc: 0.46 - ETA: 14s - loss: 1.5016 - acc: 0.46 - ETA: 14s - loss: 1.5010 - acc: 0.46 - ETA: 14s - loss: 1.5008 - acc: 0.46 - ETA: 14s - loss: 1.5002 - acc: 0.46 - ETA: 14s - loss: 1.5000 - acc: 0.46 - ETA: 14s - loss: 1.5000 - acc: 0.46 - ETA: 14s - loss: 1.4999 - acc: 0.46 - ETA: 14s - loss: 1.4995 - acc: 0.46 - ETA: 13s - loss: 1.4989 - acc: 0.46 - ETA: 13s - loss: 1.4985 - acc: 0.46 - ETA: 13s - loss: 1.4981 - acc: 0.46 - ETA: 13s - loss: 1.4976 - acc: 0.46 - ETA: 13s - loss: 1.4971 - acc: 0.46 - ETA: 13s - loss: 1.4965 - acc: 0.46 - ETA: 13s - loss: 1.4959 - acc: 0.46 - ETA: 13s - loss: 1.4958 - acc: 0.46 - ETA: 13s - loss: 1.4953 - acc: 0.46 - ETA: 13s - loss: 1.4950 - acc: 0.46 - ETA: 13s - loss: 1.4951 - acc: 0.46 - ETA: 13s - loss: 1.4946 - acc: 0.46 - ETA: 13s - loss: 1.4941 - acc: 0.46 - ETA: 12s - loss: 1.4936 - acc: 0.46 - ETA: 12s - loss: 1.4932 - acc: 0.46 - ETA: 12s - loss: 1.4928 - acc: 0.46 - ETA: 12s - loss: 1.4923 - acc: 0.46 - ETA: 12s - loss: 1.4915 - acc: 0.46 - ETA: 12s - loss: 1.4914 - acc: 0.46 - ETA: 12s - loss: 1.4913 - acc: 0.46 - ETA: 12s - loss: 1.4908 - acc: 0.46 - ETA: 12s - loss: 1.4904 - acc: 0.46 - ETA: 12s - loss: 1.4896 - acc: 0.46 - ETA: 12s - loss: 1.4894 - acc: 0.46 - ETA: 12s - loss: 1.4888 - acc: 0.46 - ETA: 12s - loss: 1.4884 - acc: 0.46 - ETA: 11s - loss: 1.4880 - acc: 0.46 - ETA: 11s - loss: 1.4878 - acc: 0.46 - ETA: 11s - loss: 1.4874 - acc: 0.46 - ETA: 11s - loss: 1.4870 - acc: 0.46 - ETA: 11s - loss: 1.4865 - acc: 0.46 - ETA: 11s - loss: 1.4860 - acc: 0.47 - ETA: 11s - loss: 1.4856 - acc: 0.47 - ETA: 11s - loss: 1.4850 - acc: 0.47 - ETA: 11s - loss: 1.4843 - acc: 0.47 - ETA: 11s - loss: 1.4843 - acc: 0.47 - ETA: 11s - loss: 1.4841 - acc: 0.47 - ETA: 11s - loss: 1.4836 - acc: 0.47 - ETA: 11s - loss: 1.4829 - acc: 0.47 - ETA: 10s - loss: 1.4825 - acc: 0.47 - ETA: 10s - loss: 1.4822 - acc: 0.4716"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - ETA: 10s - loss: 1.4819 - acc: 0.47 - ETA: 10s - loss: 1.4811 - acc: 0.47 - ETA: 10s - loss: 1.4810 - acc: 0.47 - ETA: 10s - loss: 1.4805 - acc: 0.47 - ETA: 10s - loss: 1.4801 - acc: 0.47 - ETA: 10s - loss: 1.4795 - acc: 0.47 - ETA: 10s - loss: 1.4789 - acc: 0.47 - ETA: 10s - loss: 1.4784 - acc: 0.47 - ETA: 10s - loss: 1.4779 - acc: 0.47 - ETA: 10s - loss: 1.4777 - acc: 0.47 - ETA: 10s - loss: 1.4773 - acc: 0.47 - ETA: 9s - loss: 1.4767 - acc: 0.4736 - ETA: 9s - loss: 1.4761 - acc: 0.473 - ETA: 9s - loss: 1.4760 - acc: 0.473 - ETA: 9s - loss: 1.4750 - acc: 0.474 - ETA: 9s - loss: 1.4744 - acc: 0.474 - ETA: 9s - loss: 1.4739 - acc: 0.474 - ETA: 9s - loss: 1.4735 - acc: 0.474 - ETA: 9s - loss: 1.4727 - acc: 0.475 - ETA: 9s - loss: 1.4726 - acc: 0.475 - ETA: 9s - loss: 1.4720 - acc: 0.475 - ETA: 9s - loss: 1.4719 - acc: 0.475 - ETA: 9s - loss: 1.4717 - acc: 0.475 - ETA: 9s - loss: 1.4716 - acc: 0.475 - ETA: 8s - loss: 1.4713 - acc: 0.475 - ETA: 8s - loss: 1.4708 - acc: 0.476 - ETA: 8s - loss: 1.4707 - acc: 0.476 - ETA: 8s - loss: 1.4701 - acc: 0.476 - ETA: 8s - loss: 1.4698 - acc: 0.476 - ETA: 8s - loss: 1.4693 - acc: 0.476 - ETA: 8s - loss: 1.4688 - acc: 0.476 - ETA: 8s - loss: 1.4684 - acc: 0.476 - ETA: 8s - loss: 1.4679 - acc: 0.477 - ETA: 8s - loss: 1.4679 - acc: 0.477 - ETA: 8s - loss: 1.4674 - acc: 0.477 - ETA: 8s - loss: 1.4669 - acc: 0.477 - ETA: 8s - loss: 1.4667 - acc: 0.477 - ETA: 7s - loss: 1.4663 - acc: 0.477 - ETA: 7s - loss: 1.4659 - acc: 0.477 - ETA: 7s - loss: 1.4651 - acc: 0.478 - ETA: 7s - loss: 1.4642 - acc: 0.478 - ETA: 7s - loss: 1.4640 - acc: 0.478 - ETA: 7s - loss: 1.4637 - acc: 0.478 - ETA: 7s - loss: 1.4633 - acc: 0.478 - ETA: 7s - loss: 1.4625 - acc: 0.479 - ETA: 7s - loss: 1.4625 - acc: 0.479 - ETA: 7s - loss: 1.4618 - acc: 0.479 - ETA: 7s - loss: 1.4614 - acc: 0.479 - ETA: 7s - loss: 1.4612 - acc: 0.479 - ETA: 7s - loss: 1.4610 - acc: 0.479 - ETA: 6s - loss: 1.4606 - acc: 0.480 - ETA: 6s - loss: 1.4603 - acc: 0.480 - ETA: 6s - loss: 1.4601 - acc: 0.480 - ETA: 6s - loss: 1.4596 - acc: 0.480 - ETA: 6s - loss: 1.4593 - acc: 0.480 - ETA: 6s - loss: 1.4587 - acc: 0.480 - ETA: 6s - loss: 1.4581 - acc: 0.481 - ETA: 6s - loss: 1.4576 - acc: 0.481 - ETA: 6s - loss: 1.4574 - acc: 0.481 - ETA: 6s - loss: 1.4571 - acc: 0.481 - ETA: 6s - loss: 1.4568 - acc: 0.481 - ETA: 6s - loss: 1.4565 - acc: 0.481 - ETA: 6s - loss: 1.4560 - acc: 0.481 - ETA: 5s - loss: 1.4557 - acc: 0.481 - ETA: 5s - loss: 1.4554 - acc: 0.481 - ETA: 5s - loss: 1.4552 - acc: 0.482 - ETA: 5s - loss: 1.4547 - acc: 0.482 - ETA: 5s - loss: 1.4545 - acc: 0.482 - ETA: 5s - loss: 1.4542 - acc: 0.482 - ETA: 5s - loss: 1.4536 - acc: 0.482 - ETA: 5s - loss: 1.4532 - acc: 0.482 - ETA: 5s - loss: 1.4528 - acc: 0.482 - ETA: 5s - loss: 1.4526 - acc: 0.482 - ETA: 5s - loss: 1.4525 - acc: 0.483 - ETA: 5s - loss: 1.4519 - acc: 0.483 - ETA: 5s - loss: 1.4513 - acc: 0.483 - ETA: 4s - loss: 1.4506 - acc: 0.483 - ETA: 4s - loss: 1.4501 - acc: 0.483 - ETA: 4s - loss: 1.4501 - acc: 0.484 - ETA: 4s - loss: 1.4499 - acc: 0.483 - ETA: 4s - loss: 1.4501 - acc: 0.484 - ETA: 4s - loss: 1.4497 - acc: 0.484 - ETA: 4s - loss: 1.4491 - acc: 0.484 - ETA: 4s - loss: 1.4488 - acc: 0.484 - ETA: 4s - loss: 1.4485 - acc: 0.484 - ETA: 4s - loss: 1.4477 - acc: 0.484 - ETA: 4s - loss: 1.4476 - acc: 0.484 - ETA: 4s - loss: 1.4471 - acc: 0.485 - ETA: 4s - loss: 1.4466 - acc: 0.485 - ETA: 4s - loss: 1.4465 - acc: 0.485 - ETA: 3s - loss: 1.4458 - acc: 0.485 - ETA: 3s - loss: 1.4454 - acc: 0.485 - ETA: 3s - loss: 1.4456 - acc: 0.485 - ETA: 3s - loss: 1.4454 - acc: 0.485 - ETA: 3s - loss: 1.4451 - acc: 0.486 - ETA: 3s - loss: 1.4448 - acc: 0.486 - ETA: 3s - loss: 1.4444 - acc: 0.486 - ETA: 3s - loss: 1.4437 - acc: 0.486 - ETA: 3s - loss: 1.4434 - acc: 0.486 - ETA: 3s - loss: 1.4430 - acc: 0.486 - ETA: 3s - loss: 1.4426 - acc: 0.487 - ETA: 3s - loss: 1.4422 - acc: 0.487 - ETA: 3s - loss: 1.4416 - acc: 0.487 - ETA: 2s - loss: 1.4411 - acc: 0.487 - ETA: 2s - loss: 1.4407 - acc: 0.487 - ETA: 2s - loss: 1.4402 - acc: 0.487 - ETA: 2s - loss: 1.4400 - acc: 0.487 - ETA: 2s - loss: 1.4397 - acc: 0.487 - ETA: 2s - loss: 1.4393 - acc: 0.488 - ETA: 2s - loss: 1.4390 - acc: 0.488 - ETA: 2s - loss: 1.4385 - acc: 0.488 - ETA: 2s - loss: 1.4380 - acc: 0.488 - ETA: 2s - loss: 1.4375 - acc: 0.488 - ETA: 2s - loss: 1.4374 - acc: 0.488 - ETA: 2s - loss: 1.4374 - acc: 0.488 - ETA: 2s - loss: 1.4368 - acc: 0.489 - ETA: 1s - loss: 1.4363 - acc: 0.489 - ETA: 1s - loss: 1.4360 - acc: 0.489 - ETA: 1s - loss: 1.4359 - acc: 0.489 - ETA: 1s - loss: 1.4356 - acc: 0.489 - ETA: 1s - loss: 1.4355 - acc: 0.489 - ETA: 1s - loss: 1.4351 - acc: 0.490 - ETA: 1s - loss: 1.4347 - acc: 0.490 - ETA: 1s - loss: 1.4341 - acc: 0.490 - ETA: 1s - loss: 1.4341 - acc: 0.490 - ETA: 1s - loss: 1.4336 - acc: 0.490 - ETA: 1s - loss: 1.4330 - acc: 0.490 - ETA: 1s - loss: 1.4324 - acc: 0.491 - ETA: 1s - loss: 1.4322 - acc: 0.491 - ETA: 0s - loss: 1.4323 - acc: 0.491 - ETA: 0s - loss: 1.4318 - acc: 0.491 - ETA: 0s - loss: 1.4316 - acc: 0.491 - ETA: 0s - loss: 1.4312 - acc: 0.491 - ETA: 0s - loss: 1.4309 - acc: 0.491 - ETA: 0s - loss: 1.4306 - acc: 0.491 - ETA: 0s - loss: 1.4301 - acc: 0.491 - ETA: 0s - loss: 1.4297 - acc: 0.492 - ETA: 0s - loss: 1.4294 - acc: 0.492 - ETA: 0s - loss: 1.4288 - acc: 0.492 - ETA: 0s - loss: 1.4286 - acc: 0.492 - ETA: 0s - loss: 1.4285 - acc: 0.492 - ETA: 0s - loss: 1.4281 - acc: 0.492 - 59s 1ms/step - loss: 1.4279 - acc: 0.4922\n",
      "10000/10000 [==============================] - ETA: 50 - ETA: 13 - ETA: 8 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 4s 390us/step\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13600/50000 [=======>......................] - ETA: 12:05 - loss: 2.3158 - acc: 0.06 - ETA: 4:35 - loss: 2.7433 - acc: 0.1458 - ETA: 3:03 - loss: 2.5714 - acc: 0.125 - ETA: 2:24 - loss: 2.4807 - acc: 0.116 - ETA: 2:02 - loss: 2.4590 - acc: 0.104 - ETA: 1:49 - loss: 2.4229 - acc: 0.105 - ETA: 1:39 - loss: 2.3902 - acc: 0.110 - ETA: 1:32 - loss: 2.3737 - acc: 0.108 - ETA: 1:27 - loss: 2.3565 - acc: 0.110 - ETA: 1:22 - loss: 2.3588 - acc: 0.108 - ETA: 1:19 - loss: 2.3437 - acc: 0.123 - ETA: 1:16 - loss: 2.3315 - acc: 0.134 - ETA: 1:14 - loss: 2.3170 - acc: 0.135 - ETA: 1:12 - loss: 2.3247 - acc: 0.133 - ETA: 1:10 - loss: 2.3164 - acc: 0.140 - ETA: 1:09 - loss: 2.3015 - acc: 0.144 - ETA: 1:07 - loss: 2.2900 - acc: 0.144 - ETA: 1:06 - loss: 2.2926 - acc: 0.145 - ETA: 1:05 - loss: 2.2804 - acc: 0.150 - ETA: 1:04 - loss: 2.2626 - acc: 0.158 - ETA: 1:03 - loss: 2.2569 - acc: 0.160 - ETA: 1:02 - loss: 2.2413 - acc: 0.170 - ETA: 1:01 - loss: 2.2425 - acc: 0.169 - ETA: 1:01 - loss: 2.2344 - acc: 0.171 - ETA: 1:00 - loss: 2.2259 - acc: 0.172 - ETA: 1:00 - loss: 2.2215 - acc: 0.175 - ETA: 59s - loss: 2.2335 - acc: 0.174 - ETA: 59s - loss: 2.2247 - acc: 0.17 - ETA: 59s - loss: 2.2169 - acc: 0.17 - ETA: 58s - loss: 2.2105 - acc: 0.18 - ETA: 58s - loss: 2.2019 - acc: 0.18 - ETA: 58s - loss: 2.1925 - acc: 0.19 - ETA: 58s - loss: 2.1921 - acc: 0.19 - ETA: 57s - loss: 2.1855 - acc: 0.19 - ETA: 57s - loss: 2.1806 - acc: 0.19 - ETA: 57s - loss: 2.1725 - acc: 0.20 - ETA: 57s - loss: 2.1706 - acc: 0.20 - ETA: 57s - loss: 2.1588 - acc: 0.20 - ETA: 57s - loss: 2.1546 - acc: 0.21 - ETA: 56s - loss: 2.1492 - acc: 0.21 - ETA: 56s - loss: 2.1413 - acc: 0.21 - ETA: 56s - loss: 2.1424 - acc: 0.22 - ETA: 56s - loss: 2.1389 - acc: 0.22 - ETA: 56s - loss: 2.1342 - acc: 0.22 - ETA: 56s - loss: 2.1258 - acc: 0.22 - ETA: 56s - loss: 2.1229 - acc: 0.22 - ETA: 55s - loss: 2.1220 - acc: 0.23 - ETA: 55s - loss: 2.1185 - acc: 0.23 - ETA: 55s - loss: 2.1128 - acc: 0.23 - ETA: 55s - loss: 2.1064 - acc: 0.23 - ETA: 55s - loss: 2.0984 - acc: 0.24 - ETA: 55s - loss: 2.0956 - acc: 0.24 - ETA: 55s - loss: 2.0961 - acc: 0.24 - ETA: 54s - loss: 2.0936 - acc: 0.24 - ETA: 54s - loss: 2.0874 - acc: 0.24 - ETA: 54s - loss: 2.0853 - acc: 0.24 - ETA: 54s - loss: 2.0796 - acc: 0.24 - ETA: 54s - loss: 2.0742 - acc: 0.24 - ETA: 54s - loss: 2.0685 - acc: 0.25 - ETA: 54s - loss: 2.0632 - acc: 0.25 - ETA: 54s - loss: 2.0596 - acc: 0.25 - ETA: 54s - loss: 2.0556 - acc: 0.25 - ETA: 54s - loss: 2.0480 - acc: 0.26 - ETA: 53s - loss: 2.0442 - acc: 0.26 - ETA: 53s - loss: 2.0387 - acc: 0.26 - ETA: 53s - loss: 2.0336 - acc: 0.26 - ETA: 53s - loss: 2.0273 - acc: 0.26 - ETA: 53s - loss: 2.0243 - acc: 0.26 - ETA: 53s - loss: 2.0228 - acc: 0.26 - ETA: 53s - loss: 2.0168 - acc: 0.27 - ETA: 53s - loss: 2.0153 - acc: 0.27 - ETA: 53s - loss: 2.0136 - acc: 0.27 - ETA: 53s - loss: 2.0085 - acc: 0.27 - ETA: 52s - loss: 2.0041 - acc: 0.27 - ETA: 52s - loss: 2.0004 - acc: 0.27 - ETA: 52s - loss: 1.9970 - acc: 0.27 - ETA: 52s - loss: 1.9944 - acc: 0.28 - ETA: 52s - loss: 1.9903 - acc: 0.28 - ETA: 52s - loss: 1.9917 - acc: 0.28 - ETA: 52s - loss: 1.9876 - acc: 0.28 - ETA: 52s - loss: 1.9840 - acc: 0.28 - ETA: 52s - loss: 1.9815 - acc: 0.28 - ETA: 52s - loss: 1.9784 - acc: 0.28 - ETA: 51s - loss: 1.9753 - acc: 0.28 - ETA: 51s - loss: 1.9717 - acc: 0.28 - ETA: 51s - loss: 1.9698 - acc: 0.29 - ETA: 51s - loss: 1.9676 - acc: 0.29 - ETA: 51s - loss: 1.9634 - acc: 0.29 - ETA: 51s - loss: 1.9612 - acc: 0.29 - ETA: 51s - loss: 1.9603 - acc: 0.29 - ETA: 51s - loss: 1.9564 - acc: 0.29 - ETA: 51s - loss: 1.9542 - acc: 0.29 - ETA: 51s - loss: 1.9528 - acc: 0.29 - ETA: 51s - loss: 1.9493 - acc: 0.29 - ETA: 50s - loss: 1.9483 - acc: 0.30 - ETA: 50s - loss: 1.9439 - acc: 0.30 - ETA: 50s - loss: 1.9400 - acc: 0.30 - ETA: 50s - loss: 1.9361 - acc: 0.30 - ETA: 50s - loss: 1.9312 - acc: 0.30 - ETA: 50s - loss: 1.9323 - acc: 0.30 - ETA: 50s - loss: 1.9315 - acc: 0.30 - ETA: 50s - loss: 1.9296 - acc: 0.30 - ETA: 50s - loss: 1.9283 - acc: 0.30 - ETA: 50s - loss: 1.9266 - acc: 0.30 - ETA: 50s - loss: 1.9252 - acc: 0.30 - ETA: 50s - loss: 1.9228 - acc: 0.30 - ETA: 50s - loss: 1.9212 - acc: 0.30 - ETA: 49s - loss: 1.9183 - acc: 0.30 - ETA: 49s - loss: 1.9167 - acc: 0.30 - ETA: 49s - loss: 1.9144 - acc: 0.30 - ETA: 49s - loss: 1.9128 - acc: 0.31 - ETA: 49s - loss: 1.9120 - acc: 0.31 - ETA: 49s - loss: 1.9106 - acc: 0.31 - ETA: 49s - loss: 1.9075 - acc: 0.31 - ETA: 49s - loss: 1.9051 - acc: 0.31 - ETA: 49s - loss: 1.9026 - acc: 0.31 - ETA: 49s - loss: 1.9002 - acc: 0.31 - ETA: 49s - loss: 1.8970 - acc: 0.31 - ETA: 49s - loss: 1.8952 - acc: 0.31 - ETA: 48s - loss: 1.8928 - acc: 0.31 - ETA: 48s - loss: 1.8931 - acc: 0.31 - ETA: 48s - loss: 1.8917 - acc: 0.31 - ETA: 48s - loss: 1.8898 - acc: 0.31 - ETA: 48s - loss: 1.8889 - acc: 0.31 - ETA: 48s - loss: 1.8853 - acc: 0.32 - ETA: 48s - loss: 1.8817 - acc: 0.32 - ETA: 48s - loss: 1.8807 - acc: 0.32 - ETA: 48s - loss: 1.8791 - acc: 0.32 - ETA: 48s - loss: 1.8774 - acc: 0.32 - ETA: 48s - loss: 1.8744 - acc: 0.32 - ETA: 48s - loss: 1.8728 - acc: 0.32 - ETA: 47s - loss: 1.8708 - acc: 0.32 - ETA: 47s - loss: 1.8715 - acc: 0.32 - ETA: 47s - loss: 1.8696 - acc: 0.32 - ETA: 47s - loss: 1.8671 - acc: 0.32 - ETA: 47s - loss: 1.8661 - acc: 0.32 - ETA: 47s - loss: 1.8635 - acc: 0.32 - ETA: 47s - loss: 1.8634 - acc: 0.32 - ETA: 47s - loss: 1.8608 - acc: 0.32 - ETA: 47s - loss: 1.8595 - acc: 0.33 - ETA: 47s - loss: 1.8601 - acc: 0.33 - ETA: 47s - loss: 1.8586 - acc: 0.33 - ETA: 47s - loss: 1.8571 - acc: 0.33 - ETA: 47s - loss: 1.8565 - acc: 0.33 - ETA: 46s - loss: 1.8539 - acc: 0.33 - ETA: 46s - loss: 1.8527 - acc: 0.33 - ETA: 46s - loss: 1.8515 - acc: 0.33 - ETA: 46s - loss: 1.8508 - acc: 0.33 - ETA: 46s - loss: 1.8501 - acc: 0.33 - ETA: 46s - loss: 1.8493 - acc: 0.33 - ETA: 46s - loss: 1.8482 - acc: 0.33 - ETA: 46s - loss: 1.8471 - acc: 0.33 - ETA: 46s - loss: 1.8463 - acc: 0.33 - ETA: 46s - loss: 1.8430 - acc: 0.33 - ETA: 46s - loss: 1.8406 - acc: 0.33 - ETA: 46s - loss: 1.8379 - acc: 0.33 - ETA: 46s - loss: 1.8377 - acc: 0.33 - ETA: 45s - loss: 1.8364 - acc: 0.33 - ETA: 45s - loss: 1.8350 - acc: 0.34 - ETA: 45s - loss: 1.8337 - acc: 0.34 - ETA: 45s - loss: 1.8310 - acc: 0.34 - ETA: 45s - loss: 1.8289 - acc: 0.34 - ETA: 45s - loss: 1.8275 - acc: 0.34 - ETA: 45s - loss: 1.8255 - acc: 0.34 - ETA: 45s - loss: 1.8235 - acc: 0.34 - ETA: 45s - loss: 1.8230 - acc: 0.34 - ETA: 45s - loss: 1.8215 - acc: 0.34 - ETA: 45s - loss: 1.8199 - acc: 0.34 - ETA: 45s - loss: 1.8189 - acc: 0.34 - ETA: 44s - loss: 1.8180 - acc: 0.34 - ETA: 44s - loss: 1.8158 - acc: 0.34 - ETA: 44s - loss: 1.8150 - acc: 0.34 - ETA: 44s - loss: 1.8132 - acc: 0.34 - ETA: 44s - loss: 1.8112 - acc: 0.35 - ETA: 44s - loss: 1.8115 - acc: 0.35 - ETA: 44s - loss: 1.8101 - acc: 0.35 - ETA: 44s - loss: 1.8078 - acc: 0.35 - ETA: 44s - loss: 1.8070 - acc: 0.35 - ETA: 44s - loss: 1.8050 - acc: 0.35 - ETA: 44s - loss: 1.8038 - acc: 0.35 - ETA: 44s - loss: 1.8033 - acc: 0.35 - ETA: 44s - loss: 1.8028 - acc: 0.35 - ETA: 43s - loss: 1.8014 - acc: 0.35 - ETA: 43s - loss: 1.8012 - acc: 0.35 - ETA: 43s - loss: 1.7993 - acc: 0.35 - ETA: 43s - loss: 1.7967 - acc: 0.35 - ETA: 43s - loss: 1.7955 - acc: 0.35 - ETA: 43s - loss: 1.7944 - acc: 0.35 - ETA: 43s - loss: 1.7921 - acc: 0.35 - ETA: 43s - loss: 1.7907 - acc: 0.35 - ETA: 43s - loss: 1.7889 - acc: 0.35 - ETA: 43s - loss: 1.7879 - acc: 0.35 - ETA: 43s - loss: 1.7859 - acc: 0.36 - ETA: 43s - loss: 1.7836 - acc: 0.36 - ETA: 43s - loss: 1.7818 - acc: 0.36 - ETA: 42s - loss: 1.7804 - acc: 0.36 - ETA: 42s - loss: 1.7798 - acc: 0.36 - ETA: 42s - loss: 1.7781 - acc: 0.36 - ETA: 42s - loss: 1.7762 - acc: 0.36 - ETA: 42s - loss: 1.7746 - acc: 0.36 - ETA: 42s - loss: 1.7734 - acc: 0.36 - ETA: 42s - loss: 1.7721 - acc: 0.36 - ETA: 42s - loss: 1.7713 - acc: 0.36 - ETA: 42s - loss: 1.7696 - acc: 0.36 - ETA: 42s - loss: 1.7692 - acc: 0.36 - ETA: 42s - loss: 1.7679 - acc: 0.36 - ETA: 42s - loss: 1.7673 - acc: 0.36 - ETA: 42s - loss: 1.7657 - acc: 0.36 - ETA: 41s - loss: 1.7640 - acc: 0.36 - ETA: 41s - loss: 1.7623 - acc: 0.36 - ETA: 41s - loss: 1.7617 - acc: 0.36 - ETA: 41s - loss: 1.7601 - acc: 0.36 - ETA: 41s - loss: 1.7588 - acc: 0.3683"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27360/50000 [===============>..............] - ETA: 41s - loss: 1.7565 - acc: 0.36 - ETA: 41s - loss: 1.7562 - acc: 0.36 - ETA: 41s - loss: 1.7550 - acc: 0.36 - ETA: 41s - loss: 1.7536 - acc: 0.36 - ETA: 41s - loss: 1.7521 - acc: 0.37 - ETA: 41s - loss: 1.7517 - acc: 0.37 - ETA: 41s - loss: 1.7499 - acc: 0.37 - ETA: 41s - loss: 1.7486 - acc: 0.37 - ETA: 40s - loss: 1.7483 - acc: 0.37 - ETA: 40s - loss: 1.7472 - acc: 0.37 - ETA: 40s - loss: 1.7448 - acc: 0.37 - ETA: 40s - loss: 1.7434 - acc: 0.37 - ETA: 40s - loss: 1.7416 - acc: 0.37 - ETA: 40s - loss: 1.7417 - acc: 0.37 - ETA: 40s - loss: 1.7406 - acc: 0.37 - ETA: 40s - loss: 1.7389 - acc: 0.37 - ETA: 40s - loss: 1.7374 - acc: 0.37 - ETA: 40s - loss: 1.7372 - acc: 0.37 - ETA: 40s - loss: 1.7355 - acc: 0.37 - ETA: 40s - loss: 1.7350 - acc: 0.37 - ETA: 40s - loss: 1.7335 - acc: 0.37 - ETA: 40s - loss: 1.7323 - acc: 0.37 - ETA: 39s - loss: 1.7320 - acc: 0.37 - ETA: 39s - loss: 1.7299 - acc: 0.37 - ETA: 39s - loss: 1.7277 - acc: 0.37 - ETA: 39s - loss: 1.7267 - acc: 0.37 - ETA: 39s - loss: 1.7259 - acc: 0.38 - ETA: 39s - loss: 1.7251 - acc: 0.38 - ETA: 39s - loss: 1.7230 - acc: 0.38 - ETA: 39s - loss: 1.7215 - acc: 0.38 - ETA: 39s - loss: 1.7210 - acc: 0.38 - ETA: 39s - loss: 1.7202 - acc: 0.38 - ETA: 39s - loss: 1.7201 - acc: 0.38 - ETA: 39s - loss: 1.7193 - acc: 0.38 - ETA: 39s - loss: 1.7178 - acc: 0.38 - ETA: 38s - loss: 1.7164 - acc: 0.38 - ETA: 38s - loss: 1.7155 - acc: 0.38 - ETA: 38s - loss: 1.7136 - acc: 0.38 - ETA: 38s - loss: 1.7135 - acc: 0.38 - ETA: 38s - loss: 1.7127 - acc: 0.38 - ETA: 38s - loss: 1.7118 - acc: 0.38 - ETA: 38s - loss: 1.7102 - acc: 0.38 - ETA: 38s - loss: 1.7096 - acc: 0.38 - ETA: 38s - loss: 1.7083 - acc: 0.38 - ETA: 38s - loss: 1.7077 - acc: 0.38 - ETA: 38s - loss: 1.7067 - acc: 0.38 - ETA: 38s - loss: 1.7056 - acc: 0.38 - ETA: 38s - loss: 1.7046 - acc: 0.38 - ETA: 38s - loss: 1.7031 - acc: 0.38 - ETA: 37s - loss: 1.7022 - acc: 0.39 - ETA: 37s - loss: 1.7018 - acc: 0.39 - ETA: 37s - loss: 1.7025 - acc: 0.39 - ETA: 37s - loss: 1.7015 - acc: 0.39 - ETA: 37s - loss: 1.7004 - acc: 0.39 - ETA: 37s - loss: 1.6989 - acc: 0.39 - ETA: 37s - loss: 1.6982 - acc: 0.39 - ETA: 37s - loss: 1.6975 - acc: 0.39 - ETA: 37s - loss: 1.6963 - acc: 0.39 - ETA: 37s - loss: 1.6982 - acc: 0.39 - ETA: 37s - loss: 1.6972 - acc: 0.39 - ETA: 37s - loss: 1.6967 - acc: 0.39 - ETA: 37s - loss: 1.6956 - acc: 0.39 - ETA: 36s - loss: 1.6949 - acc: 0.39 - ETA: 36s - loss: 1.6934 - acc: 0.39 - ETA: 36s - loss: 1.6935 - acc: 0.39 - ETA: 36s - loss: 1.6931 - acc: 0.39 - ETA: 36s - loss: 1.6921 - acc: 0.39 - ETA: 36s - loss: 1.6905 - acc: 0.39 - ETA: 36s - loss: 1.6885 - acc: 0.39 - ETA: 36s - loss: 1.6882 - acc: 0.39 - ETA: 36s - loss: 1.6884 - acc: 0.39 - ETA: 36s - loss: 1.6879 - acc: 0.39 - ETA: 36s - loss: 1.6864 - acc: 0.39 - ETA: 36s - loss: 1.6856 - acc: 0.39 - ETA: 36s - loss: 1.6851 - acc: 0.39 - ETA: 35s - loss: 1.6838 - acc: 0.39 - ETA: 35s - loss: 1.6825 - acc: 0.39 - ETA: 35s - loss: 1.6808 - acc: 0.39 - ETA: 35s - loss: 1.6800 - acc: 0.39 - ETA: 35s - loss: 1.6791 - acc: 0.39 - ETA: 35s - loss: 1.6781 - acc: 0.39 - ETA: 35s - loss: 1.6781 - acc: 0.39 - ETA: 35s - loss: 1.6764 - acc: 0.40 - ETA: 35s - loss: 1.6763 - acc: 0.40 - ETA: 35s - loss: 1.6766 - acc: 0.40 - ETA: 35s - loss: 1.6757 - acc: 0.40 - ETA: 35s - loss: 1.6748 - acc: 0.40 - ETA: 35s - loss: 1.6745 - acc: 0.40 - ETA: 35s - loss: 1.6746 - acc: 0.40 - ETA: 34s - loss: 1.6731 - acc: 0.40 - ETA: 34s - loss: 1.6731 - acc: 0.40 - ETA: 34s - loss: 1.6719 - acc: 0.40 - ETA: 34s - loss: 1.6706 - acc: 0.40 - ETA: 34s - loss: 1.6697 - acc: 0.40 - ETA: 34s - loss: 1.6689 - acc: 0.40 - ETA: 34s - loss: 1.6678 - acc: 0.40 - ETA: 34s - loss: 1.6667 - acc: 0.40 - ETA: 34s - loss: 1.6654 - acc: 0.40 - ETA: 34s - loss: 1.6652 - acc: 0.40 - ETA: 34s - loss: 1.6647 - acc: 0.40 - ETA: 34s - loss: 1.6641 - acc: 0.40 - ETA: 34s - loss: 1.6634 - acc: 0.40 - ETA: 34s - loss: 1.6620 - acc: 0.40 - ETA: 33s - loss: 1.6616 - acc: 0.40 - ETA: 33s - loss: 1.6603 - acc: 0.40 - ETA: 33s - loss: 1.6602 - acc: 0.40 - ETA: 33s - loss: 1.6592 - acc: 0.40 - ETA: 33s - loss: 1.6590 - acc: 0.40 - ETA: 33s - loss: 1.6576 - acc: 0.40 - ETA: 33s - loss: 1.6569 - acc: 0.40 - ETA: 33s - loss: 1.6559 - acc: 0.40 - ETA: 33s - loss: 1.6552 - acc: 0.40 - ETA: 33s - loss: 1.6551 - acc: 0.40 - ETA: 33s - loss: 1.6537 - acc: 0.40 - ETA: 33s - loss: 1.6537 - acc: 0.40 - ETA: 33s - loss: 1.6526 - acc: 0.41 - ETA: 32s - loss: 1.6519 - acc: 0.41 - ETA: 32s - loss: 1.6508 - acc: 0.41 - ETA: 32s - loss: 1.6501 - acc: 0.41 - ETA: 32s - loss: 1.6491 - acc: 0.41 - ETA: 32s - loss: 1.6483 - acc: 0.41 - ETA: 32s - loss: 1.6475 - acc: 0.41 - ETA: 32s - loss: 1.6467 - acc: 0.41 - ETA: 32s - loss: 1.6465 - acc: 0.41 - ETA: 32s - loss: 1.6462 - acc: 0.41 - ETA: 32s - loss: 1.6450 - acc: 0.41 - ETA: 32s - loss: 1.6441 - acc: 0.41 - ETA: 32s - loss: 1.6440 - acc: 0.41 - ETA: 32s - loss: 1.6439 - acc: 0.41 - ETA: 32s - loss: 1.6429 - acc: 0.41 - ETA: 31s - loss: 1.6416 - acc: 0.41 - ETA: 31s - loss: 1.6407 - acc: 0.41 - ETA: 31s - loss: 1.6406 - acc: 0.41 - ETA: 31s - loss: 1.6402 - acc: 0.41 - ETA: 31s - loss: 1.6396 - acc: 0.41 - ETA: 31s - loss: 1.6389 - acc: 0.41 - ETA: 31s - loss: 1.6382 - acc: 0.41 - ETA: 31s - loss: 1.6376 - acc: 0.41 - ETA: 31s - loss: 1.6371 - acc: 0.41 - ETA: 31s - loss: 1.6361 - acc: 0.41 - ETA: 31s - loss: 1.6357 - acc: 0.41 - ETA: 31s - loss: 1.6359 - acc: 0.41 - ETA: 31s - loss: 1.6359 - acc: 0.41 - ETA: 31s - loss: 1.6354 - acc: 0.41 - ETA: 30s - loss: 1.6346 - acc: 0.41 - ETA: 30s - loss: 1.6335 - acc: 0.41 - ETA: 30s - loss: 1.6328 - acc: 0.41 - ETA: 30s - loss: 1.6316 - acc: 0.41 - ETA: 30s - loss: 1.6311 - acc: 0.41 - ETA: 30s - loss: 1.6298 - acc: 0.41 - ETA: 30s - loss: 1.6299 - acc: 0.42 - ETA: 30s - loss: 1.6288 - acc: 0.42 - ETA: 30s - loss: 1.6278 - acc: 0.42 - ETA: 30s - loss: 1.6270 - acc: 0.42 - ETA: 30s - loss: 1.6265 - acc: 0.42 - ETA: 30s - loss: 1.6262 - acc: 0.42 - ETA: 30s - loss: 1.6255 - acc: 0.42 - ETA: 29s - loss: 1.6245 - acc: 0.42 - ETA: 29s - loss: 1.6233 - acc: 0.42 - ETA: 29s - loss: 1.6229 - acc: 0.42 - ETA: 29s - loss: 1.6217 - acc: 0.42 - ETA: 29s - loss: 1.6211 - acc: 0.42 - ETA: 29s - loss: 1.6204 - acc: 0.42 - ETA: 29s - loss: 1.6195 - acc: 0.42 - ETA: 29s - loss: 1.6185 - acc: 0.42 - ETA: 29s - loss: 1.6179 - acc: 0.42 - ETA: 29s - loss: 1.6178 - acc: 0.42 - ETA: 29s - loss: 1.6169 - acc: 0.42 - ETA: 29s - loss: 1.6161 - acc: 0.42 - ETA: 29s - loss: 1.6148 - acc: 0.42 - ETA: 29s - loss: 1.6146 - acc: 0.42 - ETA: 28s - loss: 1.6140 - acc: 0.42 - ETA: 28s - loss: 1.6136 - acc: 0.42 - ETA: 28s - loss: 1.6128 - acc: 0.42 - ETA: 28s - loss: 1.6121 - acc: 0.42 - ETA: 28s - loss: 1.6112 - acc: 0.42 - ETA: 28s - loss: 1.6101 - acc: 0.42 - ETA: 28s - loss: 1.6091 - acc: 0.42 - ETA: 28s - loss: 1.6080 - acc: 0.42 - ETA: 28s - loss: 1.6070 - acc: 0.42 - ETA: 28s - loss: 1.6059 - acc: 0.42 - ETA: 28s - loss: 1.6059 - acc: 0.42 - ETA: 28s - loss: 1.6052 - acc: 0.42 - ETA: 28s - loss: 1.6042 - acc: 0.42 - ETA: 27s - loss: 1.6032 - acc: 0.43 - ETA: 27s - loss: 1.6022 - acc: 0.43 - ETA: 27s - loss: 1.6033 - acc: 0.43 - ETA: 27s - loss: 1.6029 - acc: 0.43 - ETA: 27s - loss: 1.6021 - acc: 0.43 - ETA: 27s - loss: 1.6017 - acc: 0.43 - ETA: 27s - loss: 1.6010 - acc: 0.43 - ETA: 27s - loss: 1.6007 - acc: 0.43 - ETA: 27s - loss: 1.6000 - acc: 0.43 - ETA: 27s - loss: 1.5994 - acc: 0.43 - ETA: 27s - loss: 1.5984 - acc: 0.43 - ETA: 27s - loss: 1.5983 - acc: 0.43 - ETA: 27s - loss: 1.5972 - acc: 0.43 - ETA: 27s - loss: 1.5965 - acc: 0.43 - ETA: 26s - loss: 1.5954 - acc: 0.43 - ETA: 26s - loss: 1.5946 - acc: 0.43 - ETA: 26s - loss: 1.5939 - acc: 0.43 - ETA: 26s - loss: 1.5930 - acc: 0.43 - ETA: 26s - loss: 1.5927 - acc: 0.43 - ETA: 26s - loss: 1.5921 - acc: 0.43 - ETA: 26s - loss: 1.5916 - acc: 0.43 - ETA: 26s - loss: 1.5909 - acc: 0.43 - ETA: 26s - loss: 1.5906 - acc: 0.43 - ETA: 26s - loss: 1.5899 - acc: 0.43 - ETA: 26s - loss: 1.5894 - acc: 0.43 - ETA: 26s - loss: 1.5886 - acc: 0.43 - ETA: 26s - loss: 1.5881 - acc: 0.43 - ETA: 26s - loss: 1.5869 - acc: 0.43 - ETA: 25s - loss: 1.5862 - acc: 0.43 - ETA: 25s - loss: 1.5855 - acc: 0.43 - ETA: 25s - loss: 1.5847 - acc: 0.4374"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41120/50000 [=======================>......] - ETA: 25s - loss: 1.5844 - acc: 0.43 - ETA: 25s - loss: 1.5834 - acc: 0.43 - ETA: 25s - loss: 1.5823 - acc: 0.43 - ETA: 25s - loss: 1.5818 - acc: 0.43 - ETA: 25s - loss: 1.5813 - acc: 0.43 - ETA: 25s - loss: 1.5815 - acc: 0.43 - ETA: 25s - loss: 1.5807 - acc: 0.43 - ETA: 25s - loss: 1.5802 - acc: 0.43 - ETA: 25s - loss: 1.5793 - acc: 0.43 - ETA: 25s - loss: 1.5791 - acc: 0.43 - ETA: 24s - loss: 1.5783 - acc: 0.43 - ETA: 24s - loss: 1.5773 - acc: 0.43 - ETA: 24s - loss: 1.5767 - acc: 0.43 - ETA: 24s - loss: 1.5765 - acc: 0.43 - ETA: 24s - loss: 1.5758 - acc: 0.44 - ETA: 24s - loss: 1.5749 - acc: 0.44 - ETA: 24s - loss: 1.5740 - acc: 0.44 - ETA: 24s - loss: 1.5732 - acc: 0.44 - ETA: 24s - loss: 1.5726 - acc: 0.44 - ETA: 24s - loss: 1.5720 - acc: 0.44 - ETA: 24s - loss: 1.5712 - acc: 0.44 - ETA: 24s - loss: 1.5712 - acc: 0.44 - ETA: 24s - loss: 1.5710 - acc: 0.44 - ETA: 24s - loss: 1.5703 - acc: 0.44 - ETA: 23s - loss: 1.5694 - acc: 0.44 - ETA: 23s - loss: 1.5686 - acc: 0.44 - ETA: 23s - loss: 1.5679 - acc: 0.44 - ETA: 23s - loss: 1.5676 - acc: 0.44 - ETA: 23s - loss: 1.5666 - acc: 0.44 - ETA: 23s - loss: 1.5661 - acc: 0.44 - ETA: 23s - loss: 1.5658 - acc: 0.44 - ETA: 23s - loss: 1.5655 - acc: 0.44 - ETA: 23s - loss: 1.5650 - acc: 0.44 - ETA: 23s - loss: 1.5647 - acc: 0.44 - ETA: 23s - loss: 1.5637 - acc: 0.44 - ETA: 23s - loss: 1.5630 - acc: 0.44 - ETA: 23s - loss: 1.5627 - acc: 0.44 - ETA: 22s - loss: 1.5619 - acc: 0.44 - ETA: 22s - loss: 1.5612 - acc: 0.44 - ETA: 22s - loss: 1.5606 - acc: 0.44 - ETA: 22s - loss: 1.5603 - acc: 0.44 - ETA: 22s - loss: 1.5593 - acc: 0.44 - ETA: 22s - loss: 1.5587 - acc: 0.44 - ETA: 22s - loss: 1.5580 - acc: 0.44 - ETA: 22s - loss: 1.5584 - acc: 0.44 - ETA: 22s - loss: 1.5578 - acc: 0.44 - ETA: 22s - loss: 1.5574 - acc: 0.44 - ETA: 22s - loss: 1.5567 - acc: 0.44 - ETA: 22s - loss: 1.5565 - acc: 0.44 - ETA: 22s - loss: 1.5560 - acc: 0.44 - ETA: 22s - loss: 1.5549 - acc: 0.44 - ETA: 21s - loss: 1.5540 - acc: 0.44 - ETA: 21s - loss: 1.5532 - acc: 0.44 - ETA: 21s - loss: 1.5533 - acc: 0.44 - ETA: 21s - loss: 1.5527 - acc: 0.44 - ETA: 21s - loss: 1.5518 - acc: 0.44 - ETA: 21s - loss: 1.5514 - acc: 0.44 - ETA: 21s - loss: 1.5509 - acc: 0.44 - ETA: 21s - loss: 1.5504 - acc: 0.44 - ETA: 21s - loss: 1.5495 - acc: 0.45 - ETA: 21s - loss: 1.5486 - acc: 0.45 - ETA: 21s - loss: 1.5473 - acc: 0.45 - ETA: 21s - loss: 1.5468 - acc: 0.45 - ETA: 21s - loss: 1.5460 - acc: 0.45 - ETA: 21s - loss: 1.5448 - acc: 0.45 - ETA: 20s - loss: 1.5438 - acc: 0.45 - ETA: 20s - loss: 1.5435 - acc: 0.45 - ETA: 20s - loss: 1.5436 - acc: 0.45 - ETA: 20s - loss: 1.5425 - acc: 0.45 - ETA: 20s - loss: 1.5419 - acc: 0.45 - ETA: 20s - loss: 1.5414 - acc: 0.45 - ETA: 20s - loss: 1.5404 - acc: 0.45 - ETA: 20s - loss: 1.5400 - acc: 0.45 - ETA: 20s - loss: 1.5400 - acc: 0.45 - ETA: 20s - loss: 1.5394 - acc: 0.45 - ETA: 20s - loss: 1.5392 - acc: 0.45 - ETA: 20s - loss: 1.5385 - acc: 0.45 - ETA: 20s - loss: 1.5374 - acc: 0.45 - ETA: 19s - loss: 1.5367 - acc: 0.45 - ETA: 19s - loss: 1.5363 - acc: 0.45 - ETA: 19s - loss: 1.5359 - acc: 0.45 - ETA: 19s - loss: 1.5352 - acc: 0.45 - ETA: 19s - loss: 1.5348 - acc: 0.45 - ETA: 19s - loss: 1.5339 - acc: 0.45 - ETA: 19s - loss: 1.5331 - acc: 0.45 - ETA: 19s - loss: 1.5323 - acc: 0.45 - ETA: 19s - loss: 1.5316 - acc: 0.45 - ETA: 19s - loss: 1.5317 - acc: 0.45 - ETA: 19s - loss: 1.5306 - acc: 0.45 - ETA: 19s - loss: 1.5300 - acc: 0.45 - ETA: 19s - loss: 1.5299 - acc: 0.45 - ETA: 19s - loss: 1.5301 - acc: 0.45 - ETA: 18s - loss: 1.5292 - acc: 0.45 - ETA: 18s - loss: 1.5287 - acc: 0.45 - ETA: 18s - loss: 1.5283 - acc: 0.45 - ETA: 18s - loss: 1.5276 - acc: 0.45 - ETA: 18s - loss: 1.5274 - acc: 0.45 - ETA: 18s - loss: 1.5268 - acc: 0.45 - ETA: 18s - loss: 1.5261 - acc: 0.45 - ETA: 18s - loss: 1.5259 - acc: 0.45 - ETA: 18s - loss: 1.5253 - acc: 0.45 - ETA: 18s - loss: 1.5252 - acc: 0.45 - ETA: 18s - loss: 1.5245 - acc: 0.45 - ETA: 18s - loss: 1.5240 - acc: 0.45 - ETA: 18s - loss: 1.5234 - acc: 0.45 - ETA: 18s - loss: 1.5226 - acc: 0.45 - ETA: 17s - loss: 1.5221 - acc: 0.45 - ETA: 17s - loss: 1.5212 - acc: 0.45 - ETA: 17s - loss: 1.5207 - acc: 0.46 - ETA: 17s - loss: 1.5208 - acc: 0.46 - ETA: 17s - loss: 1.5200 - acc: 0.46 - ETA: 17s - loss: 1.5195 - acc: 0.46 - ETA: 17s - loss: 1.5187 - acc: 0.46 - ETA: 17s - loss: 1.5178 - acc: 0.46 - ETA: 17s - loss: 1.5174 - acc: 0.46 - ETA: 17s - loss: 1.5167 - acc: 0.46 - ETA: 17s - loss: 1.5160 - acc: 0.46 - ETA: 17s - loss: 1.5152 - acc: 0.46 - ETA: 17s - loss: 1.5146 - acc: 0.46 - ETA: 17s - loss: 1.5142 - acc: 0.46 - ETA: 16s - loss: 1.5137 - acc: 0.46 - ETA: 16s - loss: 1.5133 - acc: 0.46 - ETA: 16s - loss: 1.5135 - acc: 0.46 - ETA: 16s - loss: 1.5131 - acc: 0.46 - ETA: 16s - loss: 1.5127 - acc: 0.46 - ETA: 16s - loss: 1.5123 - acc: 0.46 - ETA: 16s - loss: 1.5119 - acc: 0.46 - ETA: 16s - loss: 1.5115 - acc: 0.46 - ETA: 16s - loss: 1.5111 - acc: 0.46 - ETA: 16s - loss: 1.5105 - acc: 0.46 - ETA: 16s - loss: 1.5097 - acc: 0.46 - ETA: 16s - loss: 1.5099 - acc: 0.46 - ETA: 16s - loss: 1.5095 - acc: 0.46 - ETA: 15s - loss: 1.5090 - acc: 0.46 - ETA: 15s - loss: 1.5083 - acc: 0.46 - ETA: 15s - loss: 1.5080 - acc: 0.46 - ETA: 15s - loss: 1.5072 - acc: 0.46 - ETA: 15s - loss: 1.5067 - acc: 0.46 - ETA: 15s - loss: 1.5061 - acc: 0.46 - ETA: 15s - loss: 1.5055 - acc: 0.46 - ETA: 15s - loss: 1.5047 - acc: 0.46 - ETA: 15s - loss: 1.5042 - acc: 0.46 - ETA: 15s - loss: 1.5039 - acc: 0.46 - ETA: 15s - loss: 1.5033 - acc: 0.46 - ETA: 15s - loss: 1.5030 - acc: 0.46 - ETA: 15s - loss: 1.5033 - acc: 0.46 - ETA: 15s - loss: 1.5028 - acc: 0.46 - ETA: 14s - loss: 1.5022 - acc: 0.46 - ETA: 14s - loss: 1.5015 - acc: 0.46 - ETA: 14s - loss: 1.5009 - acc: 0.46 - ETA: 14s - loss: 1.5004 - acc: 0.46 - ETA: 14s - loss: 1.4998 - acc: 0.46 - ETA: 14s - loss: 1.4992 - acc: 0.46 - ETA: 14s - loss: 1.4986 - acc: 0.46 - ETA: 14s - loss: 1.4979 - acc: 0.46 - ETA: 14s - loss: 1.4984 - acc: 0.46 - ETA: 14s - loss: 1.4980 - acc: 0.46 - ETA: 14s - loss: 1.4976 - acc: 0.46 - ETA: 14s - loss: 1.4974 - acc: 0.46 - ETA: 14s - loss: 1.4971 - acc: 0.46 - ETA: 14s - loss: 1.4967 - acc: 0.47 - ETA: 13s - loss: 1.4961 - acc: 0.47 - ETA: 13s - loss: 1.4958 - acc: 0.47 - ETA: 13s - loss: 1.4955 - acc: 0.47 - ETA: 13s - loss: 1.4950 - acc: 0.47 - ETA: 13s - loss: 1.4943 - acc: 0.47 - ETA: 13s - loss: 1.4937 - acc: 0.47 - ETA: 13s - loss: 1.4931 - acc: 0.47 - ETA: 13s - loss: 1.4926 - acc: 0.47 - ETA: 13s - loss: 1.4915 - acc: 0.47 - ETA: 13s - loss: 1.4909 - acc: 0.47 - ETA: 13s - loss: 1.4903 - acc: 0.47 - ETA: 13s - loss: 1.4903 - acc: 0.47 - ETA: 13s - loss: 1.4900 - acc: 0.47 - ETA: 13s - loss: 1.4897 - acc: 0.47 - ETA: 12s - loss: 1.4890 - acc: 0.47 - ETA: 12s - loss: 1.4883 - acc: 0.47 - ETA: 12s - loss: 1.4876 - acc: 0.47 - ETA: 12s - loss: 1.4874 - acc: 0.47 - ETA: 12s - loss: 1.4870 - acc: 0.47 - ETA: 12s - loss: 1.4871 - acc: 0.47 - ETA: 12s - loss: 1.4864 - acc: 0.47 - ETA: 12s - loss: 1.4860 - acc: 0.47 - ETA: 12s - loss: 1.4856 - acc: 0.47 - ETA: 12s - loss: 1.4851 - acc: 0.47 - ETA: 12s - loss: 1.4852 - acc: 0.47 - ETA: 12s - loss: 1.4850 - acc: 0.47 - ETA: 12s - loss: 1.4846 - acc: 0.47 - ETA: 11s - loss: 1.4844 - acc: 0.47 - ETA: 11s - loss: 1.4842 - acc: 0.47 - ETA: 11s - loss: 1.4839 - acc: 0.47 - ETA: 11s - loss: 1.4833 - acc: 0.47 - ETA: 11s - loss: 1.4824 - acc: 0.47 - ETA: 11s - loss: 1.4819 - acc: 0.47 - ETA: 11s - loss: 1.4813 - acc: 0.47 - ETA: 11s - loss: 1.4809 - acc: 0.47 - ETA: 11s - loss: 1.4805 - acc: 0.47 - ETA: 11s - loss: 1.4800 - acc: 0.47 - ETA: 11s - loss: 1.4794 - acc: 0.47 - ETA: 11s - loss: 1.4789 - acc: 0.47 - ETA: 11s - loss: 1.4786 - acc: 0.47 - ETA: 11s - loss: 1.4786 - acc: 0.47 - ETA: 10s - loss: 1.4783 - acc: 0.47 - ETA: 10s - loss: 1.4778 - acc: 0.47 - ETA: 10s - loss: 1.4775 - acc: 0.47 - ETA: 10s - loss: 1.4770 - acc: 0.47 - ETA: 10s - loss: 1.4764 - acc: 0.47 - ETA: 10s - loss: 1.4762 - acc: 0.47 - ETA: 10s - loss: 1.4755 - acc: 0.47 - ETA: 10s - loss: 1.4755 - acc: 0.47 - ETA: 10s - loss: 1.4750 - acc: 0.47 - ETA: 10s - loss: 1.4743 - acc: 0.47 - ETA: 10s - loss: 1.4743 - acc: 0.47 - ETA: 10s - loss: 1.4734 - acc: 0.47 - ETA: 10s - loss: 1.4730 - acc: 0.4784"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - ETA: 10s - loss: 1.4729 - acc: 0.47 - ETA: 9s - loss: 1.4725 - acc: 0.4786 - ETA: 9s - loss: 1.4719 - acc: 0.478 - ETA: 9s - loss: 1.4713 - acc: 0.479 - ETA: 9s - loss: 1.4706 - acc: 0.479 - ETA: 9s - loss: 1.4704 - acc: 0.479 - ETA: 9s - loss: 1.4701 - acc: 0.479 - ETA: 9s - loss: 1.4694 - acc: 0.479 - ETA: 9s - loss: 1.4690 - acc: 0.479 - ETA: 9s - loss: 1.4685 - acc: 0.480 - ETA: 9s - loss: 1.4680 - acc: 0.480 - ETA: 9s - loss: 1.4679 - acc: 0.480 - ETA: 9s - loss: 1.4674 - acc: 0.480 - ETA: 9s - loss: 1.4673 - acc: 0.480 - ETA: 9s - loss: 1.4671 - acc: 0.480 - ETA: 8s - loss: 1.4666 - acc: 0.480 - ETA: 8s - loss: 1.4662 - acc: 0.480 - ETA: 8s - loss: 1.4659 - acc: 0.480 - ETA: 8s - loss: 1.4655 - acc: 0.481 - ETA: 8s - loss: 1.4651 - acc: 0.481 - ETA: 8s - loss: 1.4646 - acc: 0.481 - ETA: 8s - loss: 1.4639 - acc: 0.481 - ETA: 8s - loss: 1.4635 - acc: 0.482 - ETA: 8s - loss: 1.4631 - acc: 0.482 - ETA: 8s - loss: 1.4630 - acc: 0.482 - ETA: 8s - loss: 1.4628 - acc: 0.482 - ETA: 8s - loss: 1.4624 - acc: 0.482 - ETA: 8s - loss: 1.4618 - acc: 0.482 - ETA: 7s - loss: 1.4610 - acc: 0.482 - ETA: 7s - loss: 1.4606 - acc: 0.483 - ETA: 7s - loss: 1.4603 - acc: 0.483 - ETA: 7s - loss: 1.4598 - acc: 0.483 - ETA: 7s - loss: 1.4594 - acc: 0.483 - ETA: 7s - loss: 1.4590 - acc: 0.483 - ETA: 7s - loss: 1.4584 - acc: 0.483 - ETA: 7s - loss: 1.4578 - acc: 0.483 - ETA: 7s - loss: 1.4576 - acc: 0.483 - ETA: 7s - loss: 1.4571 - acc: 0.484 - ETA: 7s - loss: 1.4567 - acc: 0.484 - ETA: 7s - loss: 1.4561 - acc: 0.484 - ETA: 7s - loss: 1.4555 - acc: 0.484 - ETA: 7s - loss: 1.4549 - acc: 0.484 - ETA: 6s - loss: 1.4547 - acc: 0.484 - ETA: 6s - loss: 1.4540 - acc: 0.485 - ETA: 6s - loss: 1.4535 - acc: 0.485 - ETA: 6s - loss: 1.4532 - acc: 0.485 - ETA: 6s - loss: 1.4528 - acc: 0.485 - ETA: 6s - loss: 1.4527 - acc: 0.485 - ETA: 6s - loss: 1.4523 - acc: 0.485 - ETA: 6s - loss: 1.4520 - acc: 0.485 - ETA: 6s - loss: 1.4516 - acc: 0.486 - ETA: 6s - loss: 1.4511 - acc: 0.486 - ETA: 6s - loss: 1.4507 - acc: 0.486 - ETA: 6s - loss: 1.4504 - acc: 0.486 - ETA: 6s - loss: 1.4499 - acc: 0.486 - ETA: 6s - loss: 1.4492 - acc: 0.487 - ETA: 5s - loss: 1.4491 - acc: 0.487 - ETA: 5s - loss: 1.4488 - acc: 0.487 - ETA: 5s - loss: 1.4482 - acc: 0.487 - ETA: 5s - loss: 1.4481 - acc: 0.487 - ETA: 5s - loss: 1.4478 - acc: 0.487 - ETA: 5s - loss: 1.4476 - acc: 0.487 - ETA: 5s - loss: 1.4471 - acc: 0.488 - ETA: 5s - loss: 1.4462 - acc: 0.488 - ETA: 5s - loss: 1.4460 - acc: 0.488 - ETA: 5s - loss: 1.4456 - acc: 0.488 - ETA: 5s - loss: 1.4452 - acc: 0.488 - ETA: 5s - loss: 1.4449 - acc: 0.488 - ETA: 5s - loss: 1.4449 - acc: 0.489 - ETA: 5s - loss: 1.4448 - acc: 0.489 - ETA: 4s - loss: 1.4440 - acc: 0.489 - ETA: 4s - loss: 1.4439 - acc: 0.489 - ETA: 4s - loss: 1.4437 - acc: 0.489 - ETA: 4s - loss: 1.4434 - acc: 0.489 - ETA: 4s - loss: 1.4431 - acc: 0.490 - ETA: 4s - loss: 1.4427 - acc: 0.490 - ETA: 4s - loss: 1.4424 - acc: 0.490 - ETA: 4s - loss: 1.4425 - acc: 0.490 - ETA: 4s - loss: 1.4421 - acc: 0.490 - ETA: 4s - loss: 1.4415 - acc: 0.490 - ETA: 4s - loss: 1.4411 - acc: 0.490 - ETA: 4s - loss: 1.4404 - acc: 0.490 - ETA: 4s - loss: 1.4402 - acc: 0.490 - ETA: 3s - loss: 1.4398 - acc: 0.491 - ETA: 3s - loss: 1.4394 - acc: 0.491 - ETA: 3s - loss: 1.4391 - acc: 0.491 - ETA: 3s - loss: 1.4388 - acc: 0.491 - ETA: 3s - loss: 1.4386 - acc: 0.491 - ETA: 3s - loss: 1.4381 - acc: 0.491 - ETA: 3s - loss: 1.4377 - acc: 0.491 - ETA: 3s - loss: 1.4374 - acc: 0.492 - ETA: 3s - loss: 1.4371 - acc: 0.492 - ETA: 3s - loss: 1.4366 - acc: 0.492 - ETA: 3s - loss: 1.4365 - acc: 0.492 - ETA: 3s - loss: 1.4365 - acc: 0.492 - ETA: 3s - loss: 1.4359 - acc: 0.492 - ETA: 3s - loss: 1.4354 - acc: 0.492 - ETA: 2s - loss: 1.4350 - acc: 0.493 - ETA: 2s - loss: 1.4347 - acc: 0.493 - ETA: 2s - loss: 1.4342 - acc: 0.493 - ETA: 2s - loss: 1.4337 - acc: 0.493 - ETA: 2s - loss: 1.4331 - acc: 0.493 - ETA: 2s - loss: 1.4330 - acc: 0.494 - ETA: 2s - loss: 1.4324 - acc: 0.494 - ETA: 2s - loss: 1.4318 - acc: 0.494 - ETA: 2s - loss: 1.4311 - acc: 0.494 - ETA: 2s - loss: 1.4310 - acc: 0.494 - ETA: 2s - loss: 1.4309 - acc: 0.494 - ETA: 2s - loss: 1.4308 - acc: 0.494 - ETA: 2s - loss: 1.4302 - acc: 0.494 - ETA: 2s - loss: 1.4300 - acc: 0.494 - ETA: 1s - loss: 1.4298 - acc: 0.495 - ETA: 1s - loss: 1.4291 - acc: 0.495 - ETA: 1s - loss: 1.4287 - acc: 0.495 - ETA: 1s - loss: 1.4281 - acc: 0.495 - ETA: 1s - loss: 1.4278 - acc: 0.496 - ETA: 1s - loss: 1.4278 - acc: 0.496 - ETA: 1s - loss: 1.4273 - acc: 0.496 - ETA: 1s - loss: 1.4273 - acc: 0.496 - ETA: 1s - loss: 1.4268 - acc: 0.496 - ETA: 1s - loss: 1.4268 - acc: 0.496 - ETA: 1s - loss: 1.4265 - acc: 0.496 - ETA: 1s - loss: 1.4260 - acc: 0.496 - ETA: 1s - loss: 1.4254 - acc: 0.497 - ETA: 1s - loss: 1.4251 - acc: 0.497 - ETA: 0s - loss: 1.4249 - acc: 0.497 - ETA: 0s - loss: 1.4246 - acc: 0.497 - ETA: 0s - loss: 1.4241 - acc: 0.497 - ETA: 0s - loss: 1.4233 - acc: 0.497 - ETA: 0s - loss: 1.4227 - acc: 0.497 - ETA: 0s - loss: 1.4225 - acc: 0.498 - ETA: 0s - loss: 1.4222 - acc: 0.498 - ETA: 0s - loss: 1.4220 - acc: 0.498 - ETA: 0s - loss: 1.4218 - acc: 0.498 - ETA: 0s - loss: 1.4214 - acc: 0.498 - ETA: 0s - loss: 1.4210 - acc: 0.498 - ETA: 0s - loss: 1.4206 - acc: 0.498 - ETA: 0s - loss: 1.4204 - acc: 0.498 - 57s 1ms/step - loss: 1.4200 - acc: 0.4988\n",
      "10000/10000 [==============================] - ETA: 1: - ETA: 16s - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 4s 389us/step\n",
      "Learning rate:  0.001\n",
      "Epoch 1/1\n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 204/1563 [==>...........................] - ETA: 1:36:17 - loss: 3.7496 - acc: 0.21 - ETA: 54:57 - loss: 3.4035 - acc: 0.1719 - ETA: 41:43 - loss: 3.1079 - acc: 0.16 - ETA: 35:07 - loss: 2.9935 - acc: 0.18 - ETA: 31:07 - loss: 2.9088 - acc: 0.18 - ETA: 28:32 - loss: 2.8286 - acc: 0.17 - ETA: 26:38 - loss: 2.8036 - acc: 0.16 - ETA: 25:10 - loss: 2.7917 - acc: 0.16 - ETA: 24:08 - loss: 2.8423 - acc: 0.16 - ETA: 23:15 - loss: 2.8019 - acc: 0.16 - ETA: 22:32 - loss: 2.7592 - acc: 0.17 - ETA: 21:54 - loss: 2.7400 - acc: 0.17 - ETA: 21:23 - loss: 2.7076 - acc: 0.19 - ETA: 20:57 - loss: 2.6634 - acc: 0.19 - ETA: 20:32 - loss: 2.6604 - acc: 0.19 - ETA: 20:11 - loss: 2.6533 - acc: 0.19 - ETA: 19:52 - loss: 2.6525 - acc: 0.20 - ETA: 19:36 - loss: 2.6390 - acc: 0.21 - ETA: 19:19 - loss: 2.6195 - acc: 0.21 - ETA: 19:06 - loss: 2.6149 - acc: 0.21 - ETA: 18:55 - loss: 2.5845 - acc: 0.22 - ETA: 18:45 - loss: 2.5562 - acc: 0.22 - ETA: 18:35 - loss: 2.5379 - acc: 0.23 - ETA: 18:27 - loss: 2.5158 - acc: 0.24 - ETA: 18:18 - loss: 2.5074 - acc: 0.23 - ETA: 18:10 - loss: 2.4912 - acc: 0.23 - ETA: 18:03 - loss: 2.4742 - acc: 0.24 - ETA: 17:56 - loss: 2.4517 - acc: 0.24 - ETA: 17:49 - loss: 2.4437 - acc: 0.25 - ETA: 17:43 - loss: 2.4341 - acc: 0.25 - ETA: 17:38 - loss: 2.4346 - acc: 0.25 - ETA: 17:32 - loss: 2.4269 - acc: 0.25 - ETA: 17:26 - loss: 2.4185 - acc: 0.25 - ETA: 17:21 - loss: 2.4074 - acc: 0.25 - ETA: 17:16 - loss: 2.4088 - acc: 0.25 - ETA: 17:12 - loss: 2.3987 - acc: 0.25 - ETA: 17:07 - loss: 2.3897 - acc: 0.25 - ETA: 17:04 - loss: 2.3805 - acc: 0.25 - ETA: 17:00 - loss: 2.3688 - acc: 0.26 - ETA: 16:56 - loss: 2.3627 - acc: 0.26 - ETA: 16:54 - loss: 2.3555 - acc: 0.26 - ETA: 16:50 - loss: 2.3446 - acc: 0.26 - ETA: 16:47 - loss: 2.3425 - acc: 0.27 - ETA: 16:44 - loss: 2.3441 - acc: 0.26 - ETA: 16:41 - loss: 2.3388 - acc: 0.26 - ETA: 16:38 - loss: 2.3365 - acc: 0.26 - ETA: 16:36 - loss: 2.3334 - acc: 0.26 - ETA: 16:32 - loss: 2.3253 - acc: 0.27 - ETA: 16:29 - loss: 2.3187 - acc: 0.27 - ETA: 16:27 - loss: 2.3135 - acc: 0.27 - ETA: 16:24 - loss: 2.3039 - acc: 0.27 - ETA: 16:21 - loss: 2.2920 - acc: 0.28 - ETA: 16:19 - loss: 2.2846 - acc: 0.28 - ETA: 16:16 - loss: 2.2785 - acc: 0.28 - ETA: 16:14 - loss: 2.2718 - acc: 0.28 - ETA: 16:12 - loss: 2.2685 - acc: 0.28 - ETA: 16:09 - loss: 2.2627 - acc: 0.28 - ETA: 16:08 - loss: 2.2577 - acc: 0.29 - ETA: 16:06 - loss: 2.2514 - acc: 0.29 - ETA: 16:03 - loss: 2.2513 - acc: 0.29 - ETA: 16:02 - loss: 2.2488 - acc: 0.29 - ETA: 16:01 - loss: 2.2465 - acc: 0.29 - ETA: 15:59 - loss: 2.2433 - acc: 0.29 - ETA: 15:58 - loss: 2.2385 - acc: 0.29 - ETA: 15:56 - loss: 2.2324 - acc: 0.29 - ETA: 15:55 - loss: 2.2299 - acc: 0.29 - ETA: 15:53 - loss: 2.2258 - acc: 0.30 - ETA: 15:51 - loss: 2.2248 - acc: 0.29 - ETA: 15:50 - loss: 2.2202 - acc: 0.30 - ETA: 15:48 - loss: 2.2119 - acc: 0.30 - ETA: 15:47 - loss: 2.2056 - acc: 0.30 - ETA: 15:45 - loss: 2.2032 - acc: 0.30 - ETA: 15:44 - loss: 2.1986 - acc: 0.30 - ETA: 15:42 - loss: 2.1981 - acc: 0.30 - ETA: 15:41 - loss: 2.1911 - acc: 0.31 - ETA: 15:39 - loss: 2.1841 - acc: 0.31 - ETA: 15:38 - loss: 2.1804 - acc: 0.31 - ETA: 15:36 - loss: 2.1819 - acc: 0.31 - ETA: 15:34 - loss: 2.1825 - acc: 0.31 - ETA: 15:33 - loss: 2.1784 - acc: 0.31 - ETA: 15:32 - loss: 2.1701 - acc: 0.31 - ETA: 15:30 - loss: 2.1669 - acc: 0.31 - ETA: 15:29 - loss: 2.1598 - acc: 0.31 - ETA: 15:28 - loss: 2.1546 - acc: 0.31 - ETA: 15:26 - loss: 2.1527 - acc: 0.31 - ETA: 15:25 - loss: 2.1502 - acc: 0.31 - ETA: 15:24 - loss: 2.1464 - acc: 0.31 - ETA: 15:23 - loss: 2.1408 - acc: 0.31 - ETA: 15:22 - loss: 2.1367 - acc: 0.32 - ETA: 15:22 - loss: 2.1327 - acc: 0.32 - ETA: 15:21 - loss: 2.1294 - acc: 0.32 - ETA: 15:21 - loss: 2.1274 - acc: 0.32 - ETA: 15:21 - loss: 2.1257 - acc: 0.32 - ETA: 15:21 - loss: 2.1227 - acc: 0.32 - ETA: 15:20 - loss: 2.1222 - acc: 0.32 - ETA: 15:19 - loss: 2.1195 - acc: 0.32 - ETA: 15:18 - loss: 2.1133 - acc: 0.32 - ETA: 15:17 - loss: 2.1073 - acc: 0.32 - ETA: 15:16 - loss: 2.1048 - acc: 0.32 - ETA: 15:15 - loss: 2.1011 - acc: 0.32 - ETA: 15:14 - loss: 2.1006 - acc: 0.32 - ETA: 15:13 - loss: 2.0963 - acc: 0.32 - ETA: 15:12 - loss: 2.0924 - acc: 0.32 - ETA: 15:10 - loss: 2.0891 - acc: 0.32 - ETA: 15:09 - loss: 2.0889 - acc: 0.32 - ETA: 15:08 - loss: 2.0874 - acc: 0.33 - ETA: 15:07 - loss: 2.0865 - acc: 0.32 - ETA: 15:06 - loss: 2.0844 - acc: 0.32 - ETA: 15:04 - loss: 2.0836 - acc: 0.32 - ETA: 15:03 - loss: 2.0811 - acc: 0.32 - ETA: 15:02 - loss: 2.0784 - acc: 0.33 - ETA: 15:01 - loss: 2.0783 - acc: 0.33 - ETA: 15:01 - loss: 2.0779 - acc: 0.33 - ETA: 15:00 - loss: 2.0744 - acc: 0.33 - ETA: 14:59 - loss: 2.0744 - acc: 0.33 - ETA: 14:57 - loss: 2.0739 - acc: 0.33 - ETA: 14:56 - loss: 2.0727 - acc: 0.33 - ETA: 14:55 - loss: 2.0714 - acc: 0.33 - ETA: 14:54 - loss: 2.0712 - acc: 0.33 - ETA: 14:53 - loss: 2.0701 - acc: 0.33 - ETA: 14:52 - loss: 2.0697 - acc: 0.33 - ETA: 14:51 - loss: 2.0686 - acc: 0.33 - ETA: 14:50 - loss: 2.0672 - acc: 0.33 - ETA: 14:50 - loss: 2.0684 - acc: 0.33 - ETA: 14:48 - loss: 2.0669 - acc: 0.33 - ETA: 14:48 - loss: 2.0648 - acc: 0.33 - ETA: 14:47 - loss: 2.0645 - acc: 0.33 - ETA: 14:46 - loss: 2.0622 - acc: 0.33 - ETA: 14:45 - loss: 2.0611 - acc: 0.33 - ETA: 14:44 - loss: 2.0609 - acc: 0.33 - ETA: 14:43 - loss: 2.0594 - acc: 0.33 - ETA: 14:42 - loss: 2.0580 - acc: 0.33 - ETA: 14:41 - loss: 2.0561 - acc: 0.33 - ETA: 14:40 - loss: 2.0538 - acc: 0.33 - ETA: 14:39 - loss: 2.0527 - acc: 0.33 - ETA: 14:38 - loss: 2.0502 - acc: 0.33 - ETA: 14:38 - loss: 2.0470 - acc: 0.34 - ETA: 14:37 - loss: 2.0460 - acc: 0.34 - ETA: 14:36 - loss: 2.0422 - acc: 0.34 - ETA: 14:35 - loss: 2.0407 - acc: 0.34 - ETA: 14:34 - loss: 2.0377 - acc: 0.34 - ETA: 14:33 - loss: 2.0360 - acc: 0.34 - ETA: 14:33 - loss: 2.0337 - acc: 0.34 - ETA: 14:32 - loss: 2.0306 - acc: 0.34 - ETA: 14:31 - loss: 2.0292 - acc: 0.34 - ETA: 14:30 - loss: 2.0272 - acc: 0.34 - ETA: 14:29 - loss: 2.0256 - acc: 0.34 - ETA: 14:28 - loss: 2.0256 - acc: 0.34 - ETA: 14:27 - loss: 2.0249 - acc: 0.34 - ETA: 14:27 - loss: 2.0225 - acc: 0.34 - ETA: 14:26 - loss: 2.0196 - acc: 0.34 - ETA: 14:25 - loss: 2.0195 - acc: 0.34 - ETA: 14:24 - loss: 2.0178 - acc: 0.34 - ETA: 14:23 - loss: 2.0187 - acc: 0.34 - ETA: 14:22 - loss: 2.0153 - acc: 0.34 - ETA: 14:22 - loss: 2.0115 - acc: 0.34 - ETA: 14:21 - loss: 2.0108 - acc: 0.34 - ETA: 14:20 - loss: 2.0094 - acc: 0.34 - ETA: 14:19 - loss: 2.0066 - acc: 0.35 - ETA: 14:19 - loss: 2.0044 - acc: 0.35 - ETA: 14:18 - loss: 2.0028 - acc: 0.35 - ETA: 14:17 - loss: 2.0031 - acc: 0.35 - ETA: 14:16 - loss: 1.9995 - acc: 0.35 - ETA: 14:16 - loss: 1.9965 - acc: 0.35 - ETA: 14:15 - loss: 1.9945 - acc: 0.35 - ETA: 14:14 - loss: 1.9943 - acc: 0.35 - ETA: 14:13 - loss: 1.9936 - acc: 0.35 - ETA: 14:13 - loss: 1.9917 - acc: 0.35 - ETA: 14:12 - loss: 1.9911 - acc: 0.35 - ETA: 14:11 - loss: 1.9929 - acc: 0.35 - ETA: 14:10 - loss: 1.9925 - acc: 0.35 - ETA: 14:10 - loss: 1.9935 - acc: 0.35 - ETA: 14:09 - loss: 1.9925 - acc: 0.35 - ETA: 14:08 - loss: 1.9919 - acc: 0.35 - ETA: 14:07 - loss: 1.9921 - acc: 0.35 - ETA: 14:06 - loss: 1.9927 - acc: 0.35 - ETA: 14:06 - loss: 1.9896 - acc: 0.35 - ETA: 14:05 - loss: 1.9881 - acc: 0.35 - ETA: 14:04 - loss: 1.9890 - acc: 0.35 - ETA: 14:03 - loss: 1.9889 - acc: 0.35 - ETA: 14:03 - loss: 1.9880 - acc: 0.35 - ETA: 14:02 - loss: 1.9881 - acc: 0.35 - ETA: 14:01 - loss: 1.9863 - acc: 0.35 - ETA: 14:01 - loss: 1.9866 - acc: 0.35 - ETA: 14:00 - loss: 1.9845 - acc: 0.35 - ETA: 13:59 - loss: 1.9837 - acc: 0.35 - ETA: 13:58 - loss: 1.9814 - acc: 0.35 - ETA: 13:57 - loss: 1.9800 - acc: 0.35 - ETA: 13:57 - loss: 1.9796 - acc: 0.35 - ETA: 13:56 - loss: 1.9785 - acc: 0.35 - ETA: 13:55 - loss: 1.9786 - acc: 0.35 - ETA: 13:55 - loss: 1.9769 - acc: 0.35 - ETA: 13:54 - loss: 1.9740 - acc: 0.35 - ETA: 13:53 - loss: 1.9728 - acc: 0.36 - ETA: 13:52 - loss: 1.9705 - acc: 0.36 - ETA: 13:51 - loss: 1.9702 - acc: 0.36 - ETA: 13:51 - loss: 1.9679 - acc: 0.36 - ETA: 13:50 - loss: 1.9673 - acc: 0.36 - ETA: 13:49 - loss: 1.9670 - acc: 0.36 - ETA: 13:48 - loss: 1.9665 - acc: 0.36 - ETA: 13:48 - loss: 1.9644 - acc: 0.36 - ETA: 13:47 - loss: 1.9644 - acc: 0.36 - ETA: 13:46 - loss: 1.9629 - acc: 0.36 - ETA: 13:45 - loss: 1.9613 - acc: 0.3638"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 408/1563 [======>.......................] - ETA: 13:45 - loss: 1.9603 - acc: 0.36 - ETA: 13:44 - loss: 1.9583 - acc: 0.36 - ETA: 13:43 - loss: 1.9561 - acc: 0.36 - ETA: 13:43 - loss: 1.9560 - acc: 0.36 - ETA: 13:42 - loss: 1.9550 - acc: 0.36 - ETA: 13:41 - loss: 1.9535 - acc: 0.36 - ETA: 13:41 - loss: 1.9530 - acc: 0.36 - ETA: 13:40 - loss: 1.9526 - acc: 0.36 - ETA: 13:39 - loss: 1.9512 - acc: 0.36 - ETA: 13:38 - loss: 1.9500 - acc: 0.36 - ETA: 13:38 - loss: 1.9481 - acc: 0.36 - ETA: 13:37 - loss: 1.9473 - acc: 0.36 - ETA: 13:36 - loss: 1.9461 - acc: 0.36 - ETA: 13:35 - loss: 1.9459 - acc: 0.36 - ETA: 13:35 - loss: 1.9462 - acc: 0.36 - ETA: 13:34 - loss: 1.9442 - acc: 0.36 - ETA: 13:33 - loss: 1.9451 - acc: 0.36 - ETA: 13:33 - loss: 1.9441 - acc: 0.36 - ETA: 13:32 - loss: 1.9425 - acc: 0.36 - ETA: 13:31 - loss: 1.9407 - acc: 0.36 - ETA: 13:31 - loss: 1.9391 - acc: 0.36 - ETA: 13:30 - loss: 1.9368 - acc: 0.37 - ETA: 13:29 - loss: 1.9356 - acc: 0.37 - ETA: 13:28 - loss: 1.9352 - acc: 0.37 - ETA: 13:28 - loss: 1.9347 - acc: 0.37 - ETA: 13:27 - loss: 1.9348 - acc: 0.37 - ETA: 13:26 - loss: 1.9347 - acc: 0.37 - ETA: 13:26 - loss: 1.9319 - acc: 0.37 - ETA: 13:25 - loss: 1.9308 - acc: 0.37 - ETA: 13:24 - loss: 1.9304 - acc: 0.37 - ETA: 13:24 - loss: 1.9289 - acc: 0.37 - ETA: 13:23 - loss: 1.9284 - acc: 0.37 - ETA: 13:22 - loss: 1.9271 - acc: 0.37 - ETA: 13:21 - loss: 1.9266 - acc: 0.37 - ETA: 13:21 - loss: 1.9257 - acc: 0.37 - ETA: 13:20 - loss: 1.9248 - acc: 0.37 - ETA: 13:19 - loss: 1.9247 - acc: 0.37 - ETA: 13:18 - loss: 1.9241 - acc: 0.37 - ETA: 13:18 - loss: 1.9233 - acc: 0.37 - ETA: 13:17 - loss: 1.9223 - acc: 0.37 - ETA: 13:16 - loss: 1.9214 - acc: 0.37 - ETA: 13:15 - loss: 1.9214 - acc: 0.37 - ETA: 13:15 - loss: 1.9211 - acc: 0.37 - ETA: 13:14 - loss: 1.9207 - acc: 0.37 - ETA: 13:14 - loss: 1.9194 - acc: 0.37 - ETA: 13:14 - loss: 1.9190 - acc: 0.37 - ETA: 13:14 - loss: 1.9189 - acc: 0.37 - ETA: 13:14 - loss: 1.9174 - acc: 0.37 - ETA: 13:13 - loss: 1.9170 - acc: 0.37 - ETA: 13:13 - loss: 1.9166 - acc: 0.37 - ETA: 13:13 - loss: 1.9155 - acc: 0.37 - ETA: 13:12 - loss: 1.9148 - acc: 0.37 - ETA: 13:12 - loss: 1.9149 - acc: 0.37 - ETA: 13:11 - loss: 1.9132 - acc: 0.37 - ETA: 13:10 - loss: 1.9113 - acc: 0.37 - ETA: 13:10 - loss: 1.9098 - acc: 0.37 - ETA: 13:09 - loss: 1.9094 - acc: 0.37 - ETA: 13:08 - loss: 1.9092 - acc: 0.37 - ETA: 13:08 - loss: 1.9080 - acc: 0.38 - ETA: 13:07 - loss: 1.9070 - acc: 0.38 - ETA: 13:06 - loss: 1.9053 - acc: 0.38 - ETA: 13:06 - loss: 1.9042 - acc: 0.38 - ETA: 13:05 - loss: 1.9046 - acc: 0.38 - ETA: 13:04 - loss: 1.9028 - acc: 0.38 - ETA: 13:04 - loss: 1.9021 - acc: 0.38 - ETA: 13:03 - loss: 1.9002 - acc: 0.38 - ETA: 13:02 - loss: 1.8996 - acc: 0.38 - ETA: 13:02 - loss: 1.8993 - acc: 0.38 - ETA: 13:01 - loss: 1.8984 - acc: 0.38 - ETA: 13:00 - loss: 1.8970 - acc: 0.38 - ETA: 13:00 - loss: 1.8960 - acc: 0.38 - ETA: 12:59 - loss: 1.8953 - acc: 0.38 - ETA: 12:58 - loss: 1.8948 - acc: 0.38 - ETA: 12:58 - loss: 1.8934 - acc: 0.38 - ETA: 12:57 - loss: 1.8925 - acc: 0.38 - ETA: 12:57 - loss: 1.8922 - acc: 0.38 - ETA: 12:56 - loss: 1.8920 - acc: 0.38 - ETA: 12:55 - loss: 1.8915 - acc: 0.38 - ETA: 12:55 - loss: 1.8907 - acc: 0.38 - ETA: 12:54 - loss: 1.8902 - acc: 0.38 - ETA: 12:53 - loss: 1.8897 - acc: 0.38 - ETA: 12:53 - loss: 1.8892 - acc: 0.38 - ETA: 12:52 - loss: 1.8897 - acc: 0.38 - ETA: 12:52 - loss: 1.8894 - acc: 0.38 - ETA: 12:51 - loss: 1.8882 - acc: 0.38 - ETA: 12:51 - loss: 1.8871 - acc: 0.38 - ETA: 12:50 - loss: 1.8855 - acc: 0.38 - ETA: 12:49 - loss: 1.8845 - acc: 0.38 - ETA: 12:49 - loss: 1.8832 - acc: 0.38 - ETA: 12:48 - loss: 1.8842 - acc: 0.38 - ETA: 12:48 - loss: 1.8837 - acc: 0.38 - ETA: 12:47 - loss: 1.8831 - acc: 0.38 - ETA: 12:46 - loss: 1.8824 - acc: 0.38 - ETA: 12:46 - loss: 1.8822 - acc: 0.38 - ETA: 12:45 - loss: 1.8810 - acc: 0.38 - ETA: 12:45 - loss: 1.8809 - acc: 0.38 - ETA: 12:44 - loss: 1.8804 - acc: 0.38 - ETA: 12:43 - loss: 1.8795 - acc: 0.38 - ETA: 12:42 - loss: 1.8784 - acc: 0.38 - ETA: 12:42 - loss: 1.8773 - acc: 0.38 - ETA: 12:41 - loss: 1.8760 - acc: 0.38 - ETA: 12:41 - loss: 1.8749 - acc: 0.38 - ETA: 12:40 - loss: 1.8748 - acc: 0.38 - ETA: 12:39 - loss: 1.8746 - acc: 0.38 - ETA: 12:39 - loss: 1.8745 - acc: 0.38 - ETA: 12:38 - loss: 1.8731 - acc: 0.39 - ETA: 12:38 - loss: 1.8718 - acc: 0.39 - ETA: 12:37 - loss: 1.8707 - acc: 0.39 - ETA: 12:37 - loss: 1.8695 - acc: 0.39 - ETA: 12:36 - loss: 1.8689 - acc: 0.39 - ETA: 12:35 - loss: 1.8680 - acc: 0.39 - ETA: 12:35 - loss: 1.8680 - acc: 0.39 - ETA: 12:34 - loss: 1.8671 - acc: 0.39 - ETA: 12:33 - loss: 1.8658 - acc: 0.39 - ETA: 12:33 - loss: 1.8650 - acc: 0.39 - ETA: 12:32 - loss: 1.8634 - acc: 0.39 - ETA: 12:31 - loss: 1.8629 - acc: 0.39 - ETA: 12:31 - loss: 1.8629 - acc: 0.39 - ETA: 12:30 - loss: 1.8627 - acc: 0.39 - ETA: 12:29 - loss: 1.8621 - acc: 0.39 - ETA: 12:29 - loss: 1.8620 - acc: 0.39 - ETA: 12:28 - loss: 1.8617 - acc: 0.39 - ETA: 12:28 - loss: 1.8613 - acc: 0.39 - ETA: 12:27 - loss: 1.8606 - acc: 0.39 - ETA: 12:26 - loss: 1.8600 - acc: 0.39 - ETA: 12:26 - loss: 1.8593 - acc: 0.39 - ETA: 12:25 - loss: 1.8585 - acc: 0.39 - ETA: 12:24 - loss: 1.8577 - acc: 0.39 - ETA: 12:24 - loss: 1.8572 - acc: 0.39 - ETA: 12:23 - loss: 1.8559 - acc: 0.39 - ETA: 12:22 - loss: 1.8552 - acc: 0.39 - ETA: 12:22 - loss: 1.8550 - acc: 0.39 - ETA: 12:21 - loss: 1.8546 - acc: 0.39 - ETA: 12:20 - loss: 1.8537 - acc: 0.39 - ETA: 12:20 - loss: 1.8526 - acc: 0.39 - ETA: 12:19 - loss: 1.8520 - acc: 0.39 - ETA: 12:18 - loss: 1.8512 - acc: 0.39 - ETA: 12:18 - loss: 1.8509 - acc: 0.39 - ETA: 12:17 - loss: 1.8499 - acc: 0.39 - ETA: 12:16 - loss: 1.8492 - acc: 0.39 - ETA: 12:16 - loss: 1.8484 - acc: 0.39 - ETA: 12:15 - loss: 1.8461 - acc: 0.40 - ETA: 12:14 - loss: 1.8451 - acc: 0.40 - ETA: 12:14 - loss: 1.8434 - acc: 0.40 - ETA: 12:13 - loss: 1.8440 - acc: 0.40 - ETA: 12:12 - loss: 1.8432 - acc: 0.40 - ETA: 12:12 - loss: 1.8426 - acc: 0.40 - ETA: 12:11 - loss: 1.8420 - acc: 0.40 - ETA: 12:10 - loss: 1.8420 - acc: 0.40 - ETA: 12:10 - loss: 1.8415 - acc: 0.40 - ETA: 12:09 - loss: 1.8402 - acc: 0.40 - ETA: 12:08 - loss: 1.8399 - acc: 0.40 - ETA: 12:08 - loss: 1.8386 - acc: 0.40 - ETA: 12:07 - loss: 1.8384 - acc: 0.40 - ETA: 12:06 - loss: 1.8376 - acc: 0.40 - ETA: 12:06 - loss: 1.8368 - acc: 0.40 - ETA: 12:05 - loss: 1.8376 - acc: 0.40 - ETA: 12:04 - loss: 1.8372 - acc: 0.40 - ETA: 12:04 - loss: 1.8363 - acc: 0.40 - ETA: 12:03 - loss: 1.8355 - acc: 0.40 - ETA: 12:02 - loss: 1.8343 - acc: 0.40 - ETA: 12:02 - loss: 1.8333 - acc: 0.40 - ETA: 12:01 - loss: 1.8322 - acc: 0.40 - ETA: 12:00 - loss: 1.8322 - acc: 0.40 - ETA: 12:00 - loss: 1.8318 - acc: 0.40 - ETA: 11:59 - loss: 1.8309 - acc: 0.40 - ETA: 11:58 - loss: 1.8308 - acc: 0.40 - ETA: 11:58 - loss: 1.8306 - acc: 0.40 - ETA: 11:57 - loss: 1.8298 - acc: 0.40 - ETA: 11:56 - loss: 1.8305 - acc: 0.40 - ETA: 11:56 - loss: 1.8301 - acc: 0.40 - ETA: 11:55 - loss: 1.8297 - acc: 0.40 - ETA: 11:55 - loss: 1.8287 - acc: 0.40 - ETA: 11:54 - loss: 1.8278 - acc: 0.40 - ETA: 11:53 - loss: 1.8268 - acc: 0.40 - ETA: 11:53 - loss: 1.8261 - acc: 0.40 - ETA: 11:52 - loss: 1.8260 - acc: 0.40 - ETA: 11:51 - loss: 1.8255 - acc: 0.40 - ETA: 11:51 - loss: 1.8248 - acc: 0.40 - ETA: 11:50 - loss: 1.8243 - acc: 0.40 - ETA: 11:49 - loss: 1.8238 - acc: 0.40 - ETA: 11:49 - loss: 1.8233 - acc: 0.40 - ETA: 11:48 - loss: 1.8220 - acc: 0.40 - ETA: 11:48 - loss: 1.8214 - acc: 0.40 - ETA: 11:47 - loss: 1.8208 - acc: 0.40 - ETA: 11:46 - loss: 1.8202 - acc: 0.40 - ETA: 11:46 - loss: 1.8196 - acc: 0.41 - ETA: 11:45 - loss: 1.8192 - acc: 0.41 - ETA: 11:44 - loss: 1.8190 - acc: 0.41 - ETA: 11:44 - loss: 1.8189 - acc: 0.41 - ETA: 11:43 - loss: 1.8183 - acc: 0.41 - ETA: 11:42 - loss: 1.8180 - acc: 0.41 - ETA: 11:42 - loss: 1.8173 - acc: 0.41 - ETA: 11:41 - loss: 1.8163 - acc: 0.41 - ETA: 11:41 - loss: 1.8159 - acc: 0.41 - ETA: 11:40 - loss: 1.8145 - acc: 0.41 - ETA: 11:39 - loss: 1.8149 - acc: 0.41 - ETA: 11:39 - loss: 1.8145 - acc: 0.41 - ETA: 11:38 - loss: 1.8140 - acc: 0.41 - ETA: 11:37 - loss: 1.8143 - acc: 0.41 - ETA: 11:37 - loss: 1.8145 - acc: 0.41 - ETA: 11:36 - loss: 1.8151 - acc: 0.41 - ETA: 11:35 - loss: 1.8152 - acc: 0.41 - ETA: 11:35 - loss: 1.8153 - acc: 0.4112"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 612/1563 [==========>...................] - ETA: 11:34 - loss: 1.8146 - acc: 0.41 - ETA: 11:34 - loss: 1.8147 - acc: 0.41 - ETA: 11:33 - loss: 1.8143 - acc: 0.41 - ETA: 11:32 - loss: 1.8136 - acc: 0.41 - ETA: 11:32 - loss: 1.8122 - acc: 0.41 - ETA: 11:31 - loss: 1.8112 - acc: 0.41 - ETA: 11:30 - loss: 1.8105 - acc: 0.41 - ETA: 11:30 - loss: 1.8106 - acc: 0.41 - ETA: 11:29 - loss: 1.8101 - acc: 0.41 - ETA: 11:29 - loss: 1.8095 - acc: 0.41 - ETA: 11:28 - loss: 1.8091 - acc: 0.41 - ETA: 11:27 - loss: 1.8090 - acc: 0.41 - ETA: 11:27 - loss: 1.8081 - acc: 0.41 - ETA: 11:26 - loss: 1.8082 - acc: 0.41 - ETA: 11:25 - loss: 1.8067 - acc: 0.41 - ETA: 11:25 - loss: 1.8068 - acc: 0.41 - ETA: 11:24 - loss: 1.8059 - acc: 0.41 - ETA: 11:23 - loss: 1.8057 - acc: 0.41 - ETA: 11:23 - loss: 1.8051 - acc: 0.41 - ETA: 11:22 - loss: 1.8047 - acc: 0.41 - ETA: 11:21 - loss: 1.8040 - acc: 0.41 - ETA: 11:21 - loss: 1.8031 - acc: 0.41 - ETA: 11:20 - loss: 1.8022 - acc: 0.41 - ETA: 11:20 - loss: 1.8020 - acc: 0.41 - ETA: 11:19 - loss: 1.8022 - acc: 0.41 - ETA: 11:18 - loss: 1.8023 - acc: 0.41 - ETA: 11:18 - loss: 1.8012 - acc: 0.41 - ETA: 11:17 - loss: 1.8005 - acc: 0.41 - ETA: 11:16 - loss: 1.8001 - acc: 0.41 - ETA: 11:16 - loss: 1.7994 - acc: 0.41 - ETA: 11:15 - loss: 1.7985 - acc: 0.41 - ETA: 11:15 - loss: 1.7979 - acc: 0.41 - ETA: 11:14 - loss: 1.7969 - acc: 0.41 - ETA: 11:13 - loss: 1.7966 - acc: 0.41 - ETA: 11:13 - loss: 1.7965 - acc: 0.41 - ETA: 11:12 - loss: 1.7973 - acc: 0.41 - ETA: 11:11 - loss: 1.7964 - acc: 0.41 - ETA: 11:11 - loss: 1.7956 - acc: 0.41 - ETA: 11:10 - loss: 1.7948 - acc: 0.41 - ETA: 11:09 - loss: 1.7936 - acc: 0.42 - ETA: 11:09 - loss: 1.7929 - acc: 0.42 - ETA: 11:08 - loss: 1.7927 - acc: 0.42 - ETA: 11:07 - loss: 1.7915 - acc: 0.42 - ETA: 11:07 - loss: 1.7912 - acc: 0.42 - ETA: 11:06 - loss: 1.7913 - acc: 0.42 - ETA: 11:06 - loss: 1.7905 - acc: 0.42 - ETA: 11:05 - loss: 1.7904 - acc: 0.42 - ETA: 11:04 - loss: 1.7897 - acc: 0.42 - ETA: 11:04 - loss: 1.7891 - acc: 0.42 - ETA: 11:03 - loss: 1.7888 - acc: 0.42 - ETA: 11:03 - loss: 1.7881 - acc: 0.42 - ETA: 11:02 - loss: 1.7883 - acc: 0.42 - ETA: 11:01 - loss: 1.7874 - acc: 0.42 - ETA: 11:01 - loss: 1.7875 - acc: 0.42 - ETA: 11:00 - loss: 1.7872 - acc: 0.42 - ETA: 10:59 - loss: 1.7866 - acc: 0.42 - ETA: 10:59 - loss: 1.7863 - acc: 0.42 - ETA: 10:58 - loss: 1.7861 - acc: 0.42 - ETA: 10:58 - loss: 1.7857 - acc: 0.42 - ETA: 10:57 - loss: 1.7857 - acc: 0.42 - ETA: 10:56 - loss: 1.7857 - acc: 0.42 - ETA: 10:56 - loss: 1.7856 - acc: 0.42 - ETA: 10:55 - loss: 1.7852 - acc: 0.42 - ETA: 10:54 - loss: 1.7850 - acc: 0.42 - ETA: 10:54 - loss: 1.7847 - acc: 0.42 - ETA: 10:53 - loss: 1.7840 - acc: 0.42 - ETA: 10:53 - loss: 1.7837 - acc: 0.42 - ETA: 10:52 - loss: 1.7832 - acc: 0.42 - ETA: 10:51 - loss: 1.7824 - acc: 0.42 - ETA: 10:51 - loss: 1.7820 - acc: 0.42 - ETA: 10:50 - loss: 1.7813 - acc: 0.42 - ETA: 10:49 - loss: 1.7808 - acc: 0.42 - ETA: 10:49 - loss: 1.7800 - acc: 0.42 - ETA: 10:48 - loss: 1.7799 - acc: 0.42 - ETA: 10:48 - loss: 1.7789 - acc: 0.42 - ETA: 10:47 - loss: 1.7792 - acc: 0.42 - ETA: 10:46 - loss: 1.7783 - acc: 0.42 - ETA: 10:46 - loss: 1.7776 - acc: 0.42 - ETA: 10:45 - loss: 1.7773 - acc: 0.42 - ETA: 10:44 - loss: 1.7766 - acc: 0.42 - ETA: 10:44 - loss: 1.7759 - acc: 0.42 - ETA: 10:43 - loss: 1.7767 - acc: 0.42 - ETA: 10:43 - loss: 1.7763 - acc: 0.42 - ETA: 10:42 - loss: 1.7759 - acc: 0.42 - ETA: 10:41 - loss: 1.7749 - acc: 0.42 - ETA: 10:41 - loss: 1.7746 - acc: 0.42 - ETA: 10:40 - loss: 1.7744 - acc: 0.42 - ETA: 10:40 - loss: 1.7742 - acc: 0.42 - ETA: 10:39 - loss: 1.7734 - acc: 0.42 - ETA: 10:38 - loss: 1.7727 - acc: 0.42 - ETA: 10:38 - loss: 1.7718 - acc: 0.42 - ETA: 10:37 - loss: 1.7712 - acc: 0.42 - ETA: 10:36 - loss: 1.7702 - acc: 0.42 - ETA: 10:36 - loss: 1.7692 - acc: 0.42 - ETA: 10:35 - loss: 1.7687 - acc: 0.42 - ETA: 10:35 - loss: 1.7685 - acc: 0.42 - ETA: 10:34 - loss: 1.7683 - acc: 0.42 - ETA: 10:33 - loss: 1.7675 - acc: 0.42 - ETA: 10:33 - loss: 1.7665 - acc: 0.42 - ETA: 10:32 - loss: 1.7653 - acc: 0.43 - ETA: 10:31 - loss: 1.7651 - acc: 0.43 - ETA: 10:31 - loss: 1.7639 - acc: 0.43 - ETA: 10:30 - loss: 1.7635 - acc: 0.43 - ETA: 10:30 - loss: 1.7631 - acc: 0.43 - ETA: 10:29 - loss: 1.7627 - acc: 0.43 - ETA: 10:28 - loss: 1.7621 - acc: 0.43 - ETA: 10:28 - loss: 1.7619 - acc: 0.43 - ETA: 10:27 - loss: 1.7623 - acc: 0.43 - ETA: 10:26 - loss: 1.7621 - acc: 0.43 - ETA: 10:26 - loss: 1.7618 - acc: 0.43 - ETA: 10:25 - loss: 1.7609 - acc: 0.43 - ETA: 10:25 - loss: 1.7602 - acc: 0.43 - ETA: 10:24 - loss: 1.7602 - acc: 0.43 - ETA: 10:23 - loss: 1.7599 - acc: 0.43 - ETA: 10:23 - loss: 1.7608 - acc: 0.43 - ETA: 10:22 - loss: 1.7608 - acc: 0.43 - ETA: 10:22 - loss: 1.7602 - acc: 0.43 - ETA: 10:21 - loss: 1.7595 - acc: 0.43 - ETA: 10:20 - loss: 1.7589 - acc: 0.43 - ETA: 10:20 - loss: 1.7583 - acc: 0.43 - ETA: 10:19 - loss: 1.7575 - acc: 0.43 - ETA: 10:18 - loss: 1.7569 - acc: 0.43 - ETA: 10:18 - loss: 1.7563 - acc: 0.43 - ETA: 10:17 - loss: 1.7556 - acc: 0.43 - ETA: 10:17 - loss: 1.7556 - acc: 0.43 - ETA: 10:16 - loss: 1.7553 - acc: 0.43 - ETA: 10:15 - loss: 1.7550 - acc: 0.43 - ETA: 10:15 - loss: 1.7545 - acc: 0.43 - ETA: 10:14 - loss: 1.7547 - acc: 0.43 - ETA: 10:14 - loss: 1.7551 - acc: 0.43 - ETA: 10:13 - loss: 1.7547 - acc: 0.43 - ETA: 10:12 - loss: 1.7545 - acc: 0.43 - ETA: 10:12 - loss: 1.7545 - acc: 0.43 - ETA: 10:11 - loss: 1.7539 - acc: 0.43 - ETA: 10:10 - loss: 1.7538 - acc: 0.43 - ETA: 10:10 - loss: 1.7531 - acc: 0.43 - ETA: 10:09 - loss: 1.7533 - acc: 0.43 - ETA: 10:09 - loss: 1.7529 - acc: 0.43 - ETA: 10:08 - loss: 1.7527 - acc: 0.43 - ETA: 10:07 - loss: 1.7522 - acc: 0.43 - ETA: 10:07 - loss: 1.7516 - acc: 0.43 - ETA: 10:06 - loss: 1.7514 - acc: 0.43 - ETA: 10:05 - loss: 1.7513 - acc: 0.43 - ETA: 10:05 - loss: 1.7510 - acc: 0.43 - ETA: 10:04 - loss: 1.7508 - acc: 0.43 - ETA: 10:04 - loss: 1.7506 - acc: 0.43 - ETA: 10:03 - loss: 1.7496 - acc: 0.43 - ETA: 10:02 - loss: 1.7489 - acc: 0.43 - ETA: 10:02 - loss: 1.7488 - acc: 0.43 - ETA: 10:01 - loss: 1.7486 - acc: 0.43 - ETA: 10:01 - loss: 1.7484 - acc: 0.43 - ETA: 10:00 - loss: 1.7478 - acc: 0.43 - ETA: 9:59 - loss: 1.7477 - acc: 0.4359 - ETA: 9:59 - loss: 1.7473 - acc: 0.436 - ETA: 9:58 - loss: 1.7465 - acc: 0.436 - ETA: 9:57 - loss: 1.7456 - acc: 0.436 - ETA: 9:57 - loss: 1.7454 - acc: 0.436 - ETA: 9:56 - loss: 1.7450 - acc: 0.436 - ETA: 9:56 - loss: 1.7442 - acc: 0.437 - ETA: 9:55 - loss: 1.7440 - acc: 0.437 - ETA: 9:54 - loss: 1.7434 - acc: 0.437 - ETA: 9:54 - loss: 1.7425 - acc: 0.437 - ETA: 9:53 - loss: 1.7419 - acc: 0.438 - ETA: 9:53 - loss: 1.7414 - acc: 0.438 - ETA: 9:52 - loss: 1.7406 - acc: 0.438 - ETA: 9:51 - loss: 1.7405 - acc: 0.438 - ETA: 9:51 - loss: 1.7398 - acc: 0.438 - ETA: 9:50 - loss: 1.7396 - acc: 0.438 - ETA: 9:50 - loss: 1.7394 - acc: 0.438 - ETA: 9:49 - loss: 1.7390 - acc: 0.439 - ETA: 9:48 - loss: 1.7382 - acc: 0.439 - ETA: 9:48 - loss: 1.7377 - acc: 0.439 - ETA: 9:47 - loss: 1.7369 - acc: 0.439 - ETA: 9:46 - loss: 1.7360 - acc: 0.440 - ETA: 9:46 - loss: 1.7356 - acc: 0.440 - ETA: 9:45 - loss: 1.7353 - acc: 0.440 - ETA: 9:45 - loss: 1.7348 - acc: 0.440 - ETA: 9:44 - loss: 1.7339 - acc: 0.440 - ETA: 9:43 - loss: 1.7335 - acc: 0.441 - ETA: 9:43 - loss: 1.7330 - acc: 0.441 - ETA: 9:42 - loss: 1.7328 - acc: 0.441 - ETA: 9:42 - loss: 1.7319 - acc: 0.441 - ETA: 9:41 - loss: 1.7320 - acc: 0.441 - ETA: 9:40 - loss: 1.7316 - acc: 0.442 - ETA: 9:40 - loss: 1.7318 - acc: 0.442 - ETA: 9:39 - loss: 1.7309 - acc: 0.442 - ETA: 9:39 - loss: 1.7304 - acc: 0.442 - ETA: 9:38 - loss: 1.7301 - acc: 0.442 - ETA: 9:38 - loss: 1.7296 - acc: 0.442 - ETA: 9:37 - loss: 1.7294 - acc: 0.442 - ETA: 9:36 - loss: 1.7293 - acc: 0.442 - ETA: 9:36 - loss: 1.7287 - acc: 0.442 - ETA: 9:35 - loss: 1.7281 - acc: 0.442 - ETA: 9:35 - loss: 1.7278 - acc: 0.443 - ETA: 9:34 - loss: 1.7272 - acc: 0.443 - ETA: 9:33 - loss: 1.7263 - acc: 0.443 - ETA: 9:33 - loss: 1.7262 - acc: 0.443 - ETA: 9:32 - loss: 1.7258 - acc: 0.443 - ETA: 9:31 - loss: 1.7260 - acc: 0.443 - ETA: 9:31 - loss: 1.7254 - acc: 0.444 - ETA: 9:30 - loss: 1.7256 - acc: 0.444 - ETA: 9:30 - loss: 1.7258 - acc: 0.443 - ETA: 9:29 - loss: 1.7255 - acc: 0.444 - ETA: 9:28 - loss: 1.7256 - acc: 0.4440"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 816/1563 [==============>...............] - ETA: 9:28 - loss: 1.7250 - acc: 0.444 - ETA: 9:27 - loss: 1.7245 - acc: 0.444 - ETA: 9:27 - loss: 1.7240 - acc: 0.444 - ETA: 9:26 - loss: 1.7246 - acc: 0.444 - ETA: 9:25 - loss: 1.7236 - acc: 0.444 - ETA: 9:25 - loss: 1.7233 - acc: 0.445 - ETA: 9:24 - loss: 1.7224 - acc: 0.445 - ETA: 9:23 - loss: 1.7221 - acc: 0.445 - ETA: 9:23 - loss: 1.7217 - acc: 0.445 - ETA: 9:22 - loss: 1.7219 - acc: 0.445 - ETA: 9:22 - loss: 1.7213 - acc: 0.445 - ETA: 9:21 - loss: 1.7209 - acc: 0.445 - ETA: 9:20 - loss: 1.7209 - acc: 0.445 - ETA: 9:20 - loss: 1.7205 - acc: 0.446 - ETA: 9:19 - loss: 1.7207 - acc: 0.446 - ETA: 9:18 - loss: 1.7206 - acc: 0.446 - ETA: 9:18 - loss: 1.7202 - acc: 0.446 - ETA: 9:17 - loss: 1.7198 - acc: 0.446 - ETA: 9:17 - loss: 1.7193 - acc: 0.446 - ETA: 9:16 - loss: 1.7190 - acc: 0.446 - ETA: 9:15 - loss: 1.7185 - acc: 0.446 - ETA: 9:15 - loss: 1.7178 - acc: 0.447 - ETA: 9:14 - loss: 1.7173 - acc: 0.447 - ETA: 9:14 - loss: 1.7167 - acc: 0.447 - ETA: 9:13 - loss: 1.7167 - acc: 0.447 - ETA: 9:12 - loss: 1.7166 - acc: 0.447 - ETA: 9:12 - loss: 1.7168 - acc: 0.447 - ETA: 9:11 - loss: 1.7168 - acc: 0.447 - ETA: 9:11 - loss: 1.7165 - acc: 0.447 - ETA: 9:10 - loss: 1.7163 - acc: 0.447 - ETA: 9:10 - loss: 1.7160 - acc: 0.447 - ETA: 9:09 - loss: 1.7157 - acc: 0.447 - ETA: 9:08 - loss: 1.7152 - acc: 0.447 - ETA: 9:08 - loss: 1.7146 - acc: 0.447 - ETA: 9:07 - loss: 1.7143 - acc: 0.447 - ETA: 9:07 - loss: 1.7145 - acc: 0.447 - ETA: 9:06 - loss: 1.7142 - acc: 0.448 - ETA: 9:06 - loss: 1.7137 - acc: 0.448 - ETA: 9:05 - loss: 1.7136 - acc: 0.448 - ETA: 9:04 - loss: 1.7133 - acc: 0.448 - ETA: 9:04 - loss: 1.7130 - acc: 0.448 - ETA: 9:03 - loss: 1.7129 - acc: 0.447 - ETA: 9:03 - loss: 1.7128 - acc: 0.447 - ETA: 9:02 - loss: 1.7126 - acc: 0.447 - ETA: 9:01 - loss: 1.7124 - acc: 0.448 - ETA: 9:01 - loss: 1.7124 - acc: 0.447 - ETA: 9:00 - loss: 1.7124 - acc: 0.448 - ETA: 9:00 - loss: 1.7120 - acc: 0.448 - ETA: 8:59 - loss: 1.7116 - acc: 0.448 - ETA: 8:58 - loss: 1.7113 - acc: 0.448 - ETA: 8:58 - loss: 1.7112 - acc: 0.448 - ETA: 8:57 - loss: 1.7106 - acc: 0.448 - ETA: 8:57 - loss: 1.7104 - acc: 0.449 - ETA: 8:56 - loss: 1.7101 - acc: 0.449 - ETA: 8:55 - loss: 1.7096 - acc: 0.449 - ETA: 8:55 - loss: 1.7095 - acc: 0.449 - ETA: 8:54 - loss: 1.7088 - acc: 0.449 - ETA: 8:54 - loss: 1.7089 - acc: 0.449 - ETA: 8:53 - loss: 1.7088 - acc: 0.449 - ETA: 8:52 - loss: 1.7087 - acc: 0.449 - ETA: 8:52 - loss: 1.7084 - acc: 0.450 - ETA: 8:51 - loss: 1.7078 - acc: 0.450 - ETA: 8:51 - loss: 1.7076 - acc: 0.450 - ETA: 8:50 - loss: 1.7071 - acc: 0.450 - ETA: 8:49 - loss: 1.7065 - acc: 0.450 - ETA: 8:49 - loss: 1.7061 - acc: 0.450 - ETA: 8:48 - loss: 1.7053 - acc: 0.451 - ETA: 8:47 - loss: 1.7055 - acc: 0.451 - ETA: 8:47 - loss: 1.7053 - acc: 0.451 - ETA: 8:46 - loss: 1.7054 - acc: 0.451 - ETA: 8:46 - loss: 1.7050 - acc: 0.451 - ETA: 8:45 - loss: 1.7045 - acc: 0.451 - ETA: 8:44 - loss: 1.7045 - acc: 0.451 - ETA: 8:44 - loss: 1.7041 - acc: 0.451 - ETA: 8:43 - loss: 1.7038 - acc: 0.451 - ETA: 8:43 - loss: 1.7033 - acc: 0.452 - ETA: 8:42 - loss: 1.7026 - acc: 0.452 - ETA: 8:41 - loss: 1.7022 - acc: 0.452 - ETA: 8:41 - loss: 1.7018 - acc: 0.452 - ETA: 8:40 - loss: 1.7015 - acc: 0.452 - ETA: 8:40 - loss: 1.7014 - acc: 0.452 - ETA: 8:39 - loss: 1.7014 - acc: 0.452 - ETA: 8:38 - loss: 1.7017 - acc: 0.452 - ETA: 8:38 - loss: 1.7014 - acc: 0.452 - ETA: 8:37 - loss: 1.7009 - acc: 0.453 - ETA: 8:37 - loss: 1.7007 - acc: 0.453 - ETA: 8:36 - loss: 1.7003 - acc: 0.453 - ETA: 8:35 - loss: 1.7000 - acc: 0.453 - ETA: 8:35 - loss: 1.6994 - acc: 0.453 - ETA: 8:34 - loss: 1.6996 - acc: 0.453 - ETA: 8:34 - loss: 1.6989 - acc: 0.453 - ETA: 8:33 - loss: 1.6985 - acc: 0.453 - ETA: 8:33 - loss: 1.6985 - acc: 0.453 - ETA: 8:32 - loss: 1.6977 - acc: 0.454 - ETA: 8:31 - loss: 1.6968 - acc: 0.454 - ETA: 8:31 - loss: 1.6965 - acc: 0.454 - ETA: 8:30 - loss: 1.6960 - acc: 0.455 - ETA: 8:29 - loss: 1.6956 - acc: 0.455 - ETA: 8:29 - loss: 1.6953 - acc: 0.455 - ETA: 8:28 - loss: 1.6948 - acc: 0.455 - ETA: 8:28 - loss: 1.6941 - acc: 0.455 - ETA: 8:27 - loss: 1.6938 - acc: 0.455 - ETA: 8:26 - loss: 1.6932 - acc: 0.456 - ETA: 8:26 - loss: 1.6932 - acc: 0.456 - ETA: 8:25 - loss: 1.6932 - acc: 0.456 - ETA: 8:25 - loss: 1.6929 - acc: 0.456 - ETA: 8:24 - loss: 1.6921 - acc: 0.456 - ETA: 8:23 - loss: 1.6914 - acc: 0.456 - ETA: 8:23 - loss: 1.6913 - acc: 0.456 - ETA: 8:22 - loss: 1.6909 - acc: 0.456 - ETA: 8:22 - loss: 1.6904 - acc: 0.456 - ETA: 8:21 - loss: 1.6899 - acc: 0.457 - ETA: 8:20 - loss: 1.6895 - acc: 0.457 - ETA: 8:20 - loss: 1.6889 - acc: 0.457 - ETA: 8:19 - loss: 1.6887 - acc: 0.457 - ETA: 8:19 - loss: 1.6883 - acc: 0.457 - ETA: 8:18 - loss: 1.6877 - acc: 0.458 - ETA: 8:17 - loss: 1.6873 - acc: 0.458 - ETA: 8:17 - loss: 1.6867 - acc: 0.458 - ETA: 8:16 - loss: 1.6863 - acc: 0.458 - ETA: 8:16 - loss: 1.6858 - acc: 0.458 - ETA: 8:15 - loss: 1.6856 - acc: 0.459 - ETA: 8:14 - loss: 1.6853 - acc: 0.459 - ETA: 8:14 - loss: 1.6848 - acc: 0.459 - ETA: 8:13 - loss: 1.6850 - acc: 0.459 - ETA: 8:13 - loss: 1.6849 - acc: 0.459 - ETA: 8:12 - loss: 1.6846 - acc: 0.459 - ETA: 8:11 - loss: 1.6841 - acc: 0.459 - ETA: 8:11 - loss: 1.6838 - acc: 0.459 - ETA: 8:10 - loss: 1.6832 - acc: 0.459 - ETA: 8:10 - loss: 1.6824 - acc: 0.460 - ETA: 8:09 - loss: 1.6819 - acc: 0.460 - ETA: 8:08 - loss: 1.6815 - acc: 0.460 - ETA: 8:08 - loss: 1.6810 - acc: 0.460 - ETA: 8:07 - loss: 1.6804 - acc: 0.461 - ETA: 8:07 - loss: 1.6798 - acc: 0.461 - ETA: 8:06 - loss: 1.6791 - acc: 0.461 - ETA: 8:05 - loss: 1.6788 - acc: 0.461 - ETA: 8:05 - loss: 1.6785 - acc: 0.461 - ETA: 8:04 - loss: 1.6778 - acc: 0.461 - ETA: 8:04 - loss: 1.6772 - acc: 0.461 - ETA: 8:03 - loss: 1.6768 - acc: 0.462 - ETA: 8:02 - loss: 1.6763 - acc: 0.462 - ETA: 8:02 - loss: 1.6761 - acc: 0.462 - ETA: 8:01 - loss: 1.6758 - acc: 0.462 - ETA: 8:01 - loss: 1.6759 - acc: 0.462 - ETA: 8:00 - loss: 1.6755 - acc: 0.462 - ETA: 8:00 - loss: 1.6751 - acc: 0.462 - ETA: 7:59 - loss: 1.6746 - acc: 0.462 - ETA: 7:58 - loss: 1.6743 - acc: 0.462 - ETA: 7:58 - loss: 1.6744 - acc: 0.462 - ETA: 7:57 - loss: 1.6745 - acc: 0.462 - ETA: 7:57 - loss: 1.6741 - acc: 0.462 - ETA: 7:56 - loss: 1.6739 - acc: 0.463 - ETA: 7:55 - loss: 1.6735 - acc: 0.463 - ETA: 7:55 - loss: 1.6734 - acc: 0.463 - ETA: 7:54 - loss: 1.6728 - acc: 0.463 - ETA: 7:54 - loss: 1.6729 - acc: 0.463 - ETA: 7:53 - loss: 1.6727 - acc: 0.463 - ETA: 7:52 - loss: 1.6727 - acc: 0.463 - ETA: 7:52 - loss: 1.6724 - acc: 0.463 - ETA: 7:51 - loss: 1.6723 - acc: 0.463 - ETA: 7:50 - loss: 1.6717 - acc: 0.463 - ETA: 7:50 - loss: 1.6715 - acc: 0.463 - ETA: 7:49 - loss: 1.6712 - acc: 0.463 - ETA: 7:49 - loss: 1.6708 - acc: 0.463 - ETA: 7:48 - loss: 1.6704 - acc: 0.464 - ETA: 7:48 - loss: 1.6697 - acc: 0.464 - ETA: 7:47 - loss: 1.6694 - acc: 0.464 - ETA: 7:46 - loss: 1.6689 - acc: 0.464 - ETA: 7:46 - loss: 1.6687 - acc: 0.464 - ETA: 7:45 - loss: 1.6685 - acc: 0.465 - ETA: 7:45 - loss: 1.6681 - acc: 0.465 - ETA: 7:44 - loss: 1.6682 - acc: 0.465 - ETA: 7:43 - loss: 1.6680 - acc: 0.465 - ETA: 7:43 - loss: 1.6679 - acc: 0.465 - ETA: 7:42 - loss: 1.6675 - acc: 0.465 - ETA: 7:42 - loss: 1.6671 - acc: 0.465 - ETA: 7:41 - loss: 1.6668 - acc: 0.465 - ETA: 7:40 - loss: 1.6667 - acc: 0.466 - ETA: 7:40 - loss: 1.6664 - acc: 0.466 - ETA: 7:39 - loss: 1.6660 - acc: 0.466 - ETA: 7:39 - loss: 1.6653 - acc: 0.466 - ETA: 7:38 - loss: 1.6648 - acc: 0.466 - ETA: 7:37 - loss: 1.6643 - acc: 0.466 - ETA: 7:37 - loss: 1.6639 - acc: 0.467 - ETA: 7:36 - loss: 1.6637 - acc: 0.467 - ETA: 7:36 - loss: 1.6629 - acc: 0.467 - ETA: 7:35 - loss: 1.6624 - acc: 0.467 - ETA: 7:34 - loss: 1.6619 - acc: 0.467 - ETA: 7:34 - loss: 1.6618 - acc: 0.467 - ETA: 7:33 - loss: 1.6614 - acc: 0.468 - ETA: 7:32 - loss: 1.6610 - acc: 0.468 - ETA: 7:32 - loss: 1.6604 - acc: 0.468 - ETA: 7:31 - loss: 1.6603 - acc: 0.468 - ETA: 7:31 - loss: 1.6599 - acc: 0.468 - ETA: 7:30 - loss: 1.6595 - acc: 0.468 - ETA: 7:29 - loss: 1.6589 - acc: 0.468 - ETA: 7:29 - loss: 1.6587 - acc: 0.468 - ETA: 7:28 - loss: 1.6584 - acc: 0.468 - ETA: 7:28 - loss: 1.6579 - acc: 0.468 - ETA: 7:27 - loss: 1.6579 - acc: 0.468 - ETA: 7:26 - loss: 1.6578 - acc: 0.469 - ETA: 7:26 - loss: 1.6572 - acc: 0.4692"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1020/1563 [==================>...........] - ETA: 7:25 - loss: 1.6570 - acc: 0.469 - ETA: 7:25 - loss: 1.6567 - acc: 0.469 - ETA: 7:24 - loss: 1.6563 - acc: 0.469 - ETA: 7:23 - loss: 1.6564 - acc: 0.469 - ETA: 7:23 - loss: 1.6561 - acc: 0.469 - ETA: 7:22 - loss: 1.6557 - acc: 0.469 - ETA: 7:22 - loss: 1.6552 - acc: 0.470 - ETA: 7:21 - loss: 1.6546 - acc: 0.470 - ETA: 7:20 - loss: 1.6549 - acc: 0.470 - ETA: 7:20 - loss: 1.6546 - acc: 0.470 - ETA: 7:19 - loss: 1.6545 - acc: 0.470 - ETA: 7:19 - loss: 1.6542 - acc: 0.470 - ETA: 7:18 - loss: 1.6540 - acc: 0.470 - ETA: 7:17 - loss: 1.6540 - acc: 0.470 - ETA: 7:17 - loss: 1.6535 - acc: 0.470 - ETA: 7:16 - loss: 1.6529 - acc: 0.470 - ETA: 7:16 - loss: 1.6525 - acc: 0.470 - ETA: 7:15 - loss: 1.6523 - acc: 0.470 - ETA: 7:14 - loss: 1.6521 - acc: 0.471 - ETA: 7:14 - loss: 1.6515 - acc: 0.471 - ETA: 7:13 - loss: 1.6512 - acc: 0.471 - ETA: 7:13 - loss: 1.6511 - acc: 0.471 - ETA: 7:12 - loss: 1.6509 - acc: 0.471 - ETA: 7:11 - loss: 1.6505 - acc: 0.471 - ETA: 7:11 - loss: 1.6499 - acc: 0.471 - ETA: 7:10 - loss: 1.6494 - acc: 0.472 - ETA: 7:10 - loss: 1.6491 - acc: 0.472 - ETA: 7:09 - loss: 1.6487 - acc: 0.472 - ETA: 7:08 - loss: 1.6481 - acc: 0.472 - ETA: 7:08 - loss: 1.6478 - acc: 0.472 - ETA: 7:07 - loss: 1.6479 - acc: 0.472 - ETA: 7:07 - loss: 1.6475 - acc: 0.473 - ETA: 7:06 - loss: 1.6470 - acc: 0.473 - ETA: 7:05 - loss: 1.6468 - acc: 0.473 - ETA: 7:05 - loss: 1.6465 - acc: 0.473 - ETA: 7:04 - loss: 1.6459 - acc: 0.473 - ETA: 7:03 - loss: 1.6453 - acc: 0.473 - ETA: 7:03 - loss: 1.6451 - acc: 0.473 - ETA: 7:02 - loss: 1.6450 - acc: 0.473 - ETA: 7:02 - loss: 1.6446 - acc: 0.474 - ETA: 7:01 - loss: 1.6444 - acc: 0.474 - ETA: 7:00 - loss: 1.6439 - acc: 0.474 - ETA: 7:00 - loss: 1.6436 - acc: 0.474 - ETA: 6:59 - loss: 1.6434 - acc: 0.474 - ETA: 6:59 - loss: 1.6433 - acc: 0.474 - ETA: 6:58 - loss: 1.6431 - acc: 0.474 - ETA: 6:57 - loss: 1.6428 - acc: 0.474 - ETA: 6:57 - loss: 1.6426 - acc: 0.475 - ETA: 6:56 - loss: 1.6425 - acc: 0.475 - ETA: 6:56 - loss: 1.6422 - acc: 0.475 - ETA: 6:55 - loss: 1.6421 - acc: 0.475 - ETA: 6:54 - loss: 1.6418 - acc: 0.475 - ETA: 6:54 - loss: 1.6417 - acc: 0.475 - ETA: 6:53 - loss: 1.6416 - acc: 0.475 - ETA: 6:53 - loss: 1.6410 - acc: 0.475 - ETA: 6:52 - loss: 1.6408 - acc: 0.475 - ETA: 6:51 - loss: 1.6406 - acc: 0.475 - ETA: 6:51 - loss: 1.6409 - acc: 0.475 - ETA: 6:50 - loss: 1.6406 - acc: 0.475 - ETA: 6:50 - loss: 1.6401 - acc: 0.476 - ETA: 6:49 - loss: 1.6401 - acc: 0.476 - ETA: 6:48 - loss: 1.6398 - acc: 0.476 - ETA: 6:48 - loss: 1.6399 - acc: 0.476 - ETA: 6:47 - loss: 1.6396 - acc: 0.476 - ETA: 6:47 - loss: 1.6392 - acc: 0.476 - ETA: 6:46 - loss: 1.6393 - acc: 0.476 - ETA: 6:45 - loss: 1.6388 - acc: 0.476 - ETA: 6:45 - loss: 1.6384 - acc: 0.476 - ETA: 6:44 - loss: 1.6379 - acc: 0.476 - ETA: 6:43 - loss: 1.6380 - acc: 0.476 - ETA: 6:43 - loss: 1.6376 - acc: 0.476 - ETA: 6:42 - loss: 1.6370 - acc: 0.476 - ETA: 6:42 - loss: 1.6372 - acc: 0.476 - ETA: 6:41 - loss: 1.6368 - acc: 0.476 - ETA: 6:40 - loss: 1.6364 - acc: 0.477 - ETA: 6:40 - loss: 1.6360 - acc: 0.477 - ETA: 6:39 - loss: 1.6357 - acc: 0.477 - ETA: 6:39 - loss: 1.6354 - acc: 0.477 - ETA: 6:38 - loss: 1.6355 - acc: 0.477 - ETA: 6:37 - loss: 1.6350 - acc: 0.477 - ETA: 6:37 - loss: 1.6348 - acc: 0.477 - ETA: 6:36 - loss: 1.6347 - acc: 0.477 - ETA: 6:36 - loss: 1.6345 - acc: 0.477 - ETA: 6:35 - loss: 1.6343 - acc: 0.477 - ETA: 6:34 - loss: 1.6339 - acc: 0.477 - ETA: 6:34 - loss: 1.6336 - acc: 0.477 - ETA: 6:33 - loss: 1.6329 - acc: 0.478 - ETA: 6:33 - loss: 1.6326 - acc: 0.478 - ETA: 6:32 - loss: 1.6323 - acc: 0.478 - ETA: 6:31 - loss: 1.6319 - acc: 0.478 - ETA: 6:31 - loss: 1.6319 - acc: 0.478 - ETA: 6:30 - loss: 1.6315 - acc: 0.478 - ETA: 6:30 - loss: 1.6315 - acc: 0.478 - ETA: 6:29 - loss: 1.6312 - acc: 0.478 - ETA: 6:28 - loss: 1.6314 - acc: 0.479 - ETA: 6:28 - loss: 1.6314 - acc: 0.479 - ETA: 6:27 - loss: 1.6313 - acc: 0.479 - ETA: 6:27 - loss: 1.6311 - acc: 0.479 - ETA: 6:26 - loss: 1.6308 - acc: 0.479 - ETA: 6:25 - loss: 1.6307 - acc: 0.479 - ETA: 6:25 - loss: 1.6305 - acc: 0.479 - ETA: 6:24 - loss: 1.6303 - acc: 0.479 - ETA: 6:24 - loss: 1.6303 - acc: 0.479 - ETA: 6:23 - loss: 1.6300 - acc: 0.479 - ETA: 6:22 - loss: 1.6298 - acc: 0.479 - ETA: 6:22 - loss: 1.6299 - acc: 0.479 - ETA: 6:21 - loss: 1.6297 - acc: 0.479 - ETA: 6:21 - loss: 1.6295 - acc: 0.479 - ETA: 6:20 - loss: 1.6292 - acc: 0.480 - ETA: 6:19 - loss: 1.6295 - acc: 0.480 - ETA: 6:19 - loss: 1.6293 - acc: 0.480 - ETA: 6:18 - loss: 1.6291 - acc: 0.480 - ETA: 6:18 - loss: 1.6288 - acc: 0.480 - ETA: 6:17 - loss: 1.6287 - acc: 0.480 - ETA: 6:16 - loss: 1.6285 - acc: 0.480 - ETA: 6:16 - loss: 1.6281 - acc: 0.480 - ETA: 6:15 - loss: 1.6277 - acc: 0.480 - ETA: 6:15 - loss: 1.6278 - acc: 0.480 - ETA: 6:14 - loss: 1.6274 - acc: 0.480 - ETA: 6:13 - loss: 1.6273 - acc: 0.480 - ETA: 6:13 - loss: 1.6272 - acc: 0.480 - ETA: 6:12 - loss: 1.6270 - acc: 0.481 - ETA: 6:12 - loss: 1.6268 - acc: 0.481 - ETA: 6:11 - loss: 1.6266 - acc: 0.481 - ETA: 6:10 - loss: 1.6263 - acc: 0.481 - ETA: 6:10 - loss: 1.6259 - acc: 0.481 - ETA: 6:09 - loss: 1.6255 - acc: 0.481 - ETA: 6:09 - loss: 1.6251 - acc: 0.481 - ETA: 6:08 - loss: 1.6247 - acc: 0.481 - ETA: 6:07 - loss: 1.6243 - acc: 0.482 - ETA: 6:07 - loss: 1.6237 - acc: 0.482 - ETA: 6:06 - loss: 1.6234 - acc: 0.482 - ETA: 6:06 - loss: 1.6232 - acc: 0.482 - ETA: 6:05 - loss: 1.6229 - acc: 0.482 - ETA: 6:04 - loss: 1.6225 - acc: 0.482 - ETA: 6:04 - loss: 1.6219 - acc: 0.482 - ETA: 6:03 - loss: 1.6222 - acc: 0.482 - ETA: 6:02 - loss: 1.6221 - acc: 0.482 - ETA: 6:02 - loss: 1.6217 - acc: 0.483 - ETA: 6:01 - loss: 1.6216 - acc: 0.483 - ETA: 6:01 - loss: 1.6211 - acc: 0.483 - ETA: 6:00 - loss: 1.6209 - acc: 0.483 - ETA: 6:00 - loss: 1.6205 - acc: 0.483 - ETA: 5:59 - loss: 1.6203 - acc: 0.483 - ETA: 5:58 - loss: 1.6200 - acc: 0.483 - ETA: 5:58 - loss: 1.6196 - acc: 0.484 - ETA: 5:57 - loss: 1.6192 - acc: 0.484 - ETA: 5:56 - loss: 1.6192 - acc: 0.484 - ETA: 5:56 - loss: 1.6189 - acc: 0.484 - ETA: 5:55 - loss: 1.6186 - acc: 0.484 - ETA: 5:55 - loss: 1.6183 - acc: 0.484 - ETA: 5:54 - loss: 1.6179 - acc: 0.484 - ETA: 5:53 - loss: 1.6177 - acc: 0.484 - ETA: 5:53 - loss: 1.6173 - acc: 0.484 - ETA: 5:52 - loss: 1.6169 - acc: 0.484 - ETA: 5:52 - loss: 1.6166 - acc: 0.485 - ETA: 5:51 - loss: 1.6164 - acc: 0.485 - ETA: 5:51 - loss: 1.6163 - acc: 0.485 - ETA: 5:50 - loss: 1.6160 - acc: 0.485 - ETA: 5:49 - loss: 1.6159 - acc: 0.485 - ETA: 5:49 - loss: 1.6156 - acc: 0.485 - ETA: 5:48 - loss: 1.6152 - acc: 0.485 - ETA: 5:47 - loss: 1.6152 - acc: 0.485 - ETA: 5:47 - loss: 1.6148 - acc: 0.485 - ETA: 5:46 - loss: 1.6145 - acc: 0.485 - ETA: 5:46 - loss: 1.6144 - acc: 0.485 - ETA: 5:45 - loss: 1.6143 - acc: 0.485 - ETA: 5:44 - loss: 1.6138 - acc: 0.486 - ETA: 5:44 - loss: 1.6135 - acc: 0.486 - ETA: 5:43 - loss: 1.6134 - acc: 0.486 - ETA: 5:43 - loss: 1.6132 - acc: 0.486 - ETA: 5:42 - loss: 1.6129 - acc: 0.486 - ETA: 5:41 - loss: 1.6129 - acc: 0.486 - ETA: 5:41 - loss: 1.6123 - acc: 0.486 - ETA: 5:40 - loss: 1.6118 - acc: 0.486 - ETA: 5:40 - loss: 1.6112 - acc: 0.487 - ETA: 5:39 - loss: 1.6110 - acc: 0.487 - ETA: 5:38 - loss: 1.6110 - acc: 0.487 - ETA: 5:38 - loss: 1.6107 - acc: 0.487 - ETA: 5:37 - loss: 1.6107 - acc: 0.487 - ETA: 5:37 - loss: 1.6103 - acc: 0.487 - ETA: 5:36 - loss: 1.6099 - acc: 0.487 - ETA: 5:35 - loss: 1.6096 - acc: 0.487 - ETA: 5:35 - loss: 1.6093 - acc: 0.488 - ETA: 5:34 - loss: 1.6088 - acc: 0.488 - ETA: 5:34 - loss: 1.6084 - acc: 0.488 - ETA: 5:33 - loss: 1.6080 - acc: 0.488 - ETA: 5:32 - loss: 1.6078 - acc: 0.488 - ETA: 5:32 - loss: 1.6075 - acc: 0.488 - ETA: 5:31 - loss: 1.6073 - acc: 0.488 - ETA: 5:31 - loss: 1.6068 - acc: 0.488 - ETA: 5:30 - loss: 1.6062 - acc: 0.489 - ETA: 5:29 - loss: 1.6061 - acc: 0.489 - ETA: 5:29 - loss: 1.6057 - acc: 0.489 - ETA: 5:28 - loss: 1.6055 - acc: 0.489 - ETA: 5:28 - loss: 1.6048 - acc: 0.489 - ETA: 5:27 - loss: 1.6044 - acc: 0.489 - ETA: 5:26 - loss: 1.6039 - acc: 0.490 - ETA: 5:26 - loss: 1.6034 - acc: 0.490 - ETA: 5:25 - loss: 1.6030 - acc: 0.490 - ETA: 5:25 - loss: 1.6027 - acc: 0.490 - ETA: 5:24 - loss: 1.6022 - acc: 0.490 - ETA: 5:24 - loss: 1.6019 - acc: 0.490 - ETA: 5:23 - loss: 1.6017 - acc: 0.4908"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224/1563 [======================>.......] - ETA: 5:22 - loss: 1.6015 - acc: 0.491 - ETA: 5:22 - loss: 1.6010 - acc: 0.491 - ETA: 5:21 - loss: 1.6005 - acc: 0.491 - ETA: 5:21 - loss: 1.6003 - acc: 0.491 - ETA: 5:20 - loss: 1.6004 - acc: 0.491 - ETA: 5:19 - loss: 1.6001 - acc: 0.491 - ETA: 5:19 - loss: 1.5999 - acc: 0.491 - ETA: 5:18 - loss: 1.5997 - acc: 0.491 - ETA: 5:18 - loss: 1.5995 - acc: 0.491 - ETA: 5:17 - loss: 1.5991 - acc: 0.491 - ETA: 5:16 - loss: 1.5989 - acc: 0.491 - ETA: 5:16 - loss: 1.5986 - acc: 0.491 - ETA: 5:15 - loss: 1.5987 - acc: 0.491 - ETA: 5:15 - loss: 1.5984 - acc: 0.491 - ETA: 5:14 - loss: 1.5983 - acc: 0.491 - ETA: 5:13 - loss: 1.5981 - acc: 0.491 - ETA: 5:13 - loss: 1.5978 - acc: 0.492 - ETA: 5:12 - loss: 1.5981 - acc: 0.492 - ETA: 5:12 - loss: 1.5979 - acc: 0.492 - ETA: 5:11 - loss: 1.5975 - acc: 0.492 - ETA: 5:10 - loss: 1.5971 - acc: 0.492 - ETA: 5:10 - loss: 1.5967 - acc: 0.492 - ETA: 5:09 - loss: 1.5963 - acc: 0.492 - ETA: 5:09 - loss: 1.5959 - acc: 0.492 - ETA: 5:08 - loss: 1.5954 - acc: 0.492 - ETA: 5:07 - loss: 1.5953 - acc: 0.493 - ETA: 5:07 - loss: 1.5949 - acc: 0.493 - ETA: 5:06 - loss: 1.5949 - acc: 0.493 - ETA: 5:06 - loss: 1.5947 - acc: 0.493 - ETA: 5:05 - loss: 1.5946 - acc: 0.493 - ETA: 5:04 - loss: 1.5946 - acc: 0.493 - ETA: 5:04 - loss: 1.5943 - acc: 0.493 - ETA: 5:03 - loss: 1.5943 - acc: 0.493 - ETA: 5:03 - loss: 1.5941 - acc: 0.493 - ETA: 5:02 - loss: 1.5942 - acc: 0.493 - ETA: 5:01 - loss: 1.5942 - acc: 0.493 - ETA: 5:01 - loss: 1.5937 - acc: 0.493 - ETA: 5:00 - loss: 1.5934 - acc: 0.493 - ETA: 5:00 - loss: 1.5934 - acc: 0.493 - ETA: 4:59 - loss: 1.5932 - acc: 0.493 - ETA: 4:58 - loss: 1.5929 - acc: 0.493 - ETA: 4:58 - loss: 1.5929 - acc: 0.493 - ETA: 4:57 - loss: 1.5927 - acc: 0.493 - ETA: 4:57 - loss: 1.5922 - acc: 0.494 - ETA: 4:56 - loss: 1.5917 - acc: 0.494 - ETA: 4:55 - loss: 1.5911 - acc: 0.494 - ETA: 4:55 - loss: 1.5910 - acc: 0.494 - ETA: 4:54 - loss: 1.5908 - acc: 0.494 - ETA: 4:54 - loss: 1.5907 - acc: 0.494 - ETA: 4:53 - loss: 1.5906 - acc: 0.494 - ETA: 4:52 - loss: 1.5903 - acc: 0.494 - ETA: 4:52 - loss: 1.5901 - acc: 0.494 - ETA: 4:51 - loss: 1.5900 - acc: 0.494 - ETA: 4:51 - loss: 1.5898 - acc: 0.495 - ETA: 4:50 - loss: 1.5894 - acc: 0.495 - ETA: 4:49 - loss: 1.5890 - acc: 0.495 - ETA: 4:49 - loss: 1.5887 - acc: 0.495 - ETA: 4:48 - loss: 1.5885 - acc: 0.495 - ETA: 4:48 - loss: 1.5887 - acc: 0.495 - ETA: 4:47 - loss: 1.5883 - acc: 0.495 - ETA: 4:46 - loss: 1.5882 - acc: 0.495 - ETA: 4:46 - loss: 1.5882 - acc: 0.495 - ETA: 4:45 - loss: 1.5880 - acc: 0.495 - ETA: 4:45 - loss: 1.5877 - acc: 0.496 - ETA: 4:44 - loss: 1.5874 - acc: 0.496 - ETA: 4:43 - loss: 1.5872 - acc: 0.496 - ETA: 4:43 - loss: 1.5868 - acc: 0.496 - ETA: 4:42 - loss: 1.5862 - acc: 0.496 - ETA: 4:42 - loss: 1.5859 - acc: 0.496 - ETA: 4:41 - loss: 1.5857 - acc: 0.496 - ETA: 4:40 - loss: 1.5856 - acc: 0.496 - ETA: 4:40 - loss: 1.5854 - acc: 0.496 - ETA: 4:39 - loss: 1.5850 - acc: 0.496 - ETA: 4:39 - loss: 1.5848 - acc: 0.496 - ETA: 4:38 - loss: 1.5847 - acc: 0.497 - ETA: 4:37 - loss: 1.5845 - acc: 0.497 - ETA: 4:37 - loss: 1.5842 - acc: 0.497 - ETA: 4:36 - loss: 1.5839 - acc: 0.497 - ETA: 4:36 - loss: 1.5835 - acc: 0.497 - ETA: 4:35 - loss: 1.5834 - acc: 0.497 - ETA: 4:34 - loss: 1.5831 - acc: 0.497 - ETA: 4:34 - loss: 1.5830 - acc: 0.497 - ETA: 4:33 - loss: 1.5829 - acc: 0.497 - ETA: 4:33 - loss: 1.5825 - acc: 0.497 - ETA: 4:32 - loss: 1.5823 - acc: 0.497 - ETA: 4:31 - loss: 1.5821 - acc: 0.497 - ETA: 4:31 - loss: 1.5819 - acc: 0.497 - ETA: 4:30 - loss: 1.5814 - acc: 0.498 - ETA: 4:30 - loss: 1.5815 - acc: 0.498 - ETA: 4:29 - loss: 1.5816 - acc: 0.498 - ETA: 4:28 - loss: 1.5813 - acc: 0.498 - ETA: 4:28 - loss: 1.5812 - acc: 0.498 - ETA: 4:27 - loss: 1.5809 - acc: 0.498 - ETA: 4:27 - loss: 1.5804 - acc: 0.498 - ETA: 4:26 - loss: 1.5801 - acc: 0.498 - ETA: 4:25 - loss: 1.5800 - acc: 0.498 - ETA: 4:25 - loss: 1.5795 - acc: 0.498 - ETA: 4:24 - loss: 1.5791 - acc: 0.499 - ETA: 4:24 - loss: 1.5791 - acc: 0.499 - ETA: 4:23 - loss: 1.5790 - acc: 0.499 - ETA: 4:22 - loss: 1.5786 - acc: 0.499 - ETA: 4:22 - loss: 1.5784 - acc: 0.499 - ETA: 4:21 - loss: 1.5781 - acc: 0.499 - ETA: 4:21 - loss: 1.5780 - acc: 0.499 - ETA: 4:20 - loss: 1.5777 - acc: 0.499 - ETA: 4:19 - loss: 1.5775 - acc: 0.499 - ETA: 4:19 - loss: 1.5774 - acc: 0.499 - ETA: 4:18 - loss: 1.5773 - acc: 0.499 - ETA: 4:18 - loss: 1.5770 - acc: 0.499 - ETA: 4:17 - loss: 1.5769 - acc: 0.499 - ETA: 4:17 - loss: 1.5768 - acc: 0.499 - ETA: 4:16 - loss: 1.5767 - acc: 0.499 - ETA: 4:15 - loss: 1.5763 - acc: 0.500 - ETA: 4:15 - loss: 1.5762 - acc: 0.499 - ETA: 4:14 - loss: 1.5761 - acc: 0.500 - ETA: 4:14 - loss: 1.5759 - acc: 0.500 - ETA: 4:13 - loss: 1.5756 - acc: 0.500 - ETA: 4:12 - loss: 1.5756 - acc: 0.500 - ETA: 4:12 - loss: 1.5756 - acc: 0.500 - ETA: 4:11 - loss: 1.5753 - acc: 0.500 - ETA: 4:11 - loss: 1.5751 - acc: 0.500 - ETA: 4:10 - loss: 1.5750 - acc: 0.500 - ETA: 4:09 - loss: 1.5746 - acc: 0.500 - ETA: 4:09 - loss: 1.5745 - acc: 0.500 - ETA: 4:08 - loss: 1.5742 - acc: 0.500 - ETA: 4:08 - loss: 1.5739 - acc: 0.500 - ETA: 4:07 - loss: 1.5739 - acc: 0.501 - ETA: 4:06 - loss: 1.5735 - acc: 0.501 - ETA: 4:06 - loss: 1.5733 - acc: 0.501 - ETA: 4:05 - loss: 1.5729 - acc: 0.501 - ETA: 4:05 - loss: 1.5726 - acc: 0.501 - ETA: 4:04 - loss: 1.5723 - acc: 0.501 - ETA: 4:03 - loss: 1.5719 - acc: 0.501 - ETA: 4:03 - loss: 1.5718 - acc: 0.501 - ETA: 4:02 - loss: 1.5715 - acc: 0.501 - ETA: 4:02 - loss: 1.5714 - acc: 0.501 - ETA: 4:01 - loss: 1.5714 - acc: 0.501 - ETA: 4:00 - loss: 1.5713 - acc: 0.501 - ETA: 4:00 - loss: 1.5709 - acc: 0.502 - ETA: 3:59 - loss: 1.5705 - acc: 0.502 - ETA: 3:59 - loss: 1.5702 - acc: 0.502 - ETA: 3:58 - loss: 1.5701 - acc: 0.502 - ETA: 3:57 - loss: 1.5701 - acc: 0.502 - ETA: 3:57 - loss: 1.5698 - acc: 0.502 - ETA: 3:56 - loss: 1.5696 - acc: 0.502 - ETA: 3:56 - loss: 1.5693 - acc: 0.502 - ETA: 3:55 - loss: 1.5690 - acc: 0.502 - ETA: 3:54 - loss: 1.5688 - acc: 0.502 - ETA: 3:54 - loss: 1.5684 - acc: 0.503 - ETA: 3:53 - loss: 1.5681 - acc: 0.503 - ETA: 3:53 - loss: 1.5681 - acc: 0.503 - ETA: 3:52 - loss: 1.5676 - acc: 0.503 - ETA: 3:51 - loss: 1.5673 - acc: 0.503 - ETA: 3:51 - loss: 1.5672 - acc: 0.503 - ETA: 3:50 - loss: 1.5669 - acc: 0.503 - ETA: 3:50 - loss: 1.5665 - acc: 0.503 - ETA: 3:49 - loss: 1.5665 - acc: 0.503 - ETA: 3:48 - loss: 1.5663 - acc: 0.504 - ETA: 3:48 - loss: 1.5658 - acc: 0.504 - ETA: 3:47 - loss: 1.5657 - acc: 0.504 - ETA: 3:47 - loss: 1.5655 - acc: 0.504 - ETA: 3:46 - loss: 1.5651 - acc: 0.504 - ETA: 3:45 - loss: 1.5648 - acc: 0.504 - ETA: 3:45 - loss: 1.5649 - acc: 0.504 - ETA: 3:44 - loss: 1.5644 - acc: 0.504 - ETA: 3:44 - loss: 1.5640 - acc: 0.504 - ETA: 3:43 - loss: 1.5634 - acc: 0.505 - ETA: 3:43 - loss: 1.5632 - acc: 0.505 - ETA: 3:42 - loss: 1.5630 - acc: 0.505 - ETA: 3:41 - loss: 1.5628 - acc: 0.505 - ETA: 3:41 - loss: 1.5625 - acc: 0.505 - ETA: 3:40 - loss: 1.5622 - acc: 0.505 - ETA: 3:40 - loss: 1.5620 - acc: 0.505 - ETA: 3:39 - loss: 1.5617 - acc: 0.505 - ETA: 3:38 - loss: 1.5614 - acc: 0.505 - ETA: 3:38 - loss: 1.5611 - acc: 0.506 - ETA: 3:37 - loss: 1.5609 - acc: 0.506 - ETA: 3:37 - loss: 1.5606 - acc: 0.506 - ETA: 3:36 - loss: 1.5604 - acc: 0.506 - ETA: 3:35 - loss: 1.5599 - acc: 0.506 - ETA: 3:35 - loss: 1.5595 - acc: 0.506 - ETA: 3:34 - loss: 1.5592 - acc: 0.506 - ETA: 3:34 - loss: 1.5591 - acc: 0.506 - ETA: 3:33 - loss: 1.5592 - acc: 0.506 - ETA: 3:32 - loss: 1.5589 - acc: 0.506 - ETA: 3:32 - loss: 1.5587 - acc: 0.506 - ETA: 3:31 - loss: 1.5587 - acc: 0.506 - ETA: 3:31 - loss: 1.5583 - acc: 0.507 - ETA: 3:30 - loss: 1.5580 - acc: 0.507 - ETA: 3:29 - loss: 1.5580 - acc: 0.507 - ETA: 3:29 - loss: 1.5578 - acc: 0.507 - ETA: 3:28 - loss: 1.5573 - acc: 0.507 - ETA: 3:28 - loss: 1.5570 - acc: 0.507 - ETA: 3:27 - loss: 1.5570 - acc: 0.507 - ETA: 3:26 - loss: 1.5568 - acc: 0.507 - ETA: 3:26 - loss: 1.5564 - acc: 0.507 - ETA: 3:25 - loss: 1.5561 - acc: 0.507 - ETA: 3:25 - loss: 1.5560 - acc: 0.507 - ETA: 3:24 - loss: 1.5556 - acc: 0.508 - ETA: 3:23 - loss: 1.5552 - acc: 0.508 - ETA: 3:23 - loss: 1.5548 - acc: 0.508 - ETA: 3:22 - loss: 1.5546 - acc: 0.508 - ETA: 3:22 - loss: 1.5543 - acc: 0.508 - ETA: 3:21 - loss: 1.5542 - acc: 0.5086"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428/1563 [==========================>...] - ETA: 3:20 - loss: 1.5540 - acc: 0.508 - ETA: 3:20 - loss: 1.5537 - acc: 0.508 - ETA: 3:19 - loss: 1.5535 - acc: 0.508 - ETA: 3:19 - loss: 1.5532 - acc: 0.509 - ETA: 3:18 - loss: 1.5532 - acc: 0.508 - ETA: 3:17 - loss: 1.5529 - acc: 0.509 - ETA: 3:17 - loss: 1.5527 - acc: 0.509 - ETA: 3:16 - loss: 1.5523 - acc: 0.509 - ETA: 3:16 - loss: 1.5518 - acc: 0.509 - ETA: 3:15 - loss: 1.5517 - acc: 0.509 - ETA: 3:15 - loss: 1.5515 - acc: 0.509 - ETA: 3:14 - loss: 1.5513 - acc: 0.509 - ETA: 3:13 - loss: 1.5510 - acc: 0.509 - ETA: 3:13 - loss: 1.5507 - acc: 0.509 - ETA: 3:12 - loss: 1.5503 - acc: 0.510 - ETA: 3:12 - loss: 1.5502 - acc: 0.510 - ETA: 3:11 - loss: 1.5500 - acc: 0.510 - ETA: 3:10 - loss: 1.5496 - acc: 0.510 - ETA: 3:10 - loss: 1.5495 - acc: 0.510 - ETA: 3:09 - loss: 1.5493 - acc: 0.510 - ETA: 3:09 - loss: 1.5490 - acc: 0.510 - ETA: 3:08 - loss: 1.5488 - acc: 0.510 - ETA: 3:07 - loss: 1.5486 - acc: 0.510 - ETA: 3:07 - loss: 1.5482 - acc: 0.511 - ETA: 3:06 - loss: 1.5481 - acc: 0.511 - ETA: 3:06 - loss: 1.5477 - acc: 0.511 - ETA: 3:05 - loss: 1.5475 - acc: 0.511 - ETA: 3:04 - loss: 1.5476 - acc: 0.511 - ETA: 3:04 - loss: 1.5473 - acc: 0.511 - ETA: 3:03 - loss: 1.5470 - acc: 0.511 - ETA: 3:03 - loss: 1.5466 - acc: 0.511 - ETA: 3:02 - loss: 1.5466 - acc: 0.511 - ETA: 3:01 - loss: 1.5463 - acc: 0.511 - ETA: 3:01 - loss: 1.5461 - acc: 0.511 - ETA: 3:00 - loss: 1.5458 - acc: 0.511 - ETA: 3:00 - loss: 1.5455 - acc: 0.511 - ETA: 2:59 - loss: 1.5452 - acc: 0.511 - ETA: 2:58 - loss: 1.5448 - acc: 0.512 - ETA: 2:58 - loss: 1.5446 - acc: 0.512 - ETA: 2:57 - loss: 1.5445 - acc: 0.512 - ETA: 2:57 - loss: 1.5445 - acc: 0.512 - ETA: 2:56 - loss: 1.5445 - acc: 0.512 - ETA: 2:55 - loss: 1.5440 - acc: 0.512 - ETA: 2:55 - loss: 1.5438 - acc: 0.512 - ETA: 2:54 - loss: 1.5437 - acc: 0.512 - ETA: 2:54 - loss: 1.5436 - acc: 0.512 - ETA: 2:53 - loss: 1.5437 - acc: 0.512 - ETA: 2:52 - loss: 1.5436 - acc: 0.512 - ETA: 2:52 - loss: 1.5430 - acc: 0.512 - ETA: 2:51 - loss: 1.5427 - acc: 0.512 - ETA: 2:51 - loss: 1.5425 - acc: 0.513 - ETA: 2:50 - loss: 1.5423 - acc: 0.513 - ETA: 2:49 - loss: 1.5419 - acc: 0.513 - ETA: 2:49 - loss: 1.5417 - acc: 0.513 - ETA: 2:48 - loss: 1.5416 - acc: 0.513 - ETA: 2:48 - loss: 1.5413 - acc: 0.513 - ETA: 2:47 - loss: 1.5413 - acc: 0.513 - ETA: 2:46 - loss: 1.5412 - acc: 0.513 - ETA: 2:46 - loss: 1.5409 - acc: 0.513 - ETA: 2:45 - loss: 1.5406 - acc: 0.513 - ETA: 2:45 - loss: 1.5403 - acc: 0.513 - ETA: 2:44 - loss: 1.5404 - acc: 0.513 - ETA: 2:44 - loss: 1.5401 - acc: 0.513 - ETA: 2:43 - loss: 1.5400 - acc: 0.513 - ETA: 2:42 - loss: 1.5397 - acc: 0.513 - ETA: 2:42 - loss: 1.5394 - acc: 0.514 - ETA: 2:41 - loss: 1.5391 - acc: 0.513 - ETA: 2:41 - loss: 1.5388 - acc: 0.514 - ETA: 2:40 - loss: 1.5386 - acc: 0.514 - ETA: 2:39 - loss: 1.5386 - acc: 0.514 - ETA: 2:39 - loss: 1.5387 - acc: 0.514 - ETA: 2:38 - loss: 1.5383 - acc: 0.514 - ETA: 2:38 - loss: 1.5383 - acc: 0.514 - ETA: 2:37 - loss: 1.5380 - acc: 0.514 - ETA: 2:36 - loss: 1.5380 - acc: 0.514 - ETA: 2:36 - loss: 1.5377 - acc: 0.514 - ETA: 2:35 - loss: 1.5375 - acc: 0.514 - ETA: 2:35 - loss: 1.5375 - acc: 0.514 - ETA: 2:34 - loss: 1.5374 - acc: 0.514 - ETA: 2:33 - loss: 1.5370 - acc: 0.514 - ETA: 2:33 - loss: 1.5367 - acc: 0.515 - ETA: 2:32 - loss: 1.5364 - acc: 0.515 - ETA: 2:32 - loss: 1.5361 - acc: 0.515 - ETA: 2:31 - loss: 1.5359 - acc: 0.515 - ETA: 2:30 - loss: 1.5358 - acc: 0.515 - ETA: 2:30 - loss: 1.5356 - acc: 0.515 - ETA: 2:29 - loss: 1.5352 - acc: 0.515 - ETA: 2:29 - loss: 1.5350 - acc: 0.515 - ETA: 2:28 - loss: 1.5347 - acc: 0.515 - ETA: 2:27 - loss: 1.5343 - acc: 0.516 - ETA: 2:27 - loss: 1.5343 - acc: 0.516 - ETA: 2:26 - loss: 1.5341 - acc: 0.516 - ETA: 2:26 - loss: 1.5341 - acc: 0.516 - ETA: 2:25 - loss: 1.5339 - acc: 0.516 - ETA: 2:24 - loss: 1.5338 - acc: 0.516 - ETA: 2:24 - loss: 1.5335 - acc: 0.516 - ETA: 2:23 - loss: 1.5331 - acc: 0.516 - ETA: 2:23 - loss: 1.5327 - acc: 0.516 - ETA: 2:22 - loss: 1.5325 - acc: 0.516 - ETA: 2:21 - loss: 1.5320 - acc: 0.516 - ETA: 2:21 - loss: 1.5318 - acc: 0.516 - ETA: 2:20 - loss: 1.5316 - acc: 0.516 - ETA: 2:20 - loss: 1.5313 - acc: 0.517 - ETA: 2:19 - loss: 1.5312 - acc: 0.517 - ETA: 2:18 - loss: 1.5313 - acc: 0.517 - ETA: 2:18 - loss: 1.5311 - acc: 0.517 - ETA: 2:17 - loss: 1.5307 - acc: 0.517 - ETA: 2:17 - loss: 1.5302 - acc: 0.517 - ETA: 2:16 - loss: 1.5300 - acc: 0.517 - ETA: 2:16 - loss: 1.5300 - acc: 0.517 - ETA: 2:15 - loss: 1.5297 - acc: 0.517 - ETA: 2:14 - loss: 1.5296 - acc: 0.517 - ETA: 2:14 - loss: 1.5293 - acc: 0.517 - ETA: 2:13 - loss: 1.5289 - acc: 0.518 - ETA: 2:13 - loss: 1.5289 - acc: 0.518 - ETA: 2:12 - loss: 1.5288 - acc: 0.518 - ETA: 2:11 - loss: 1.5285 - acc: 0.518 - ETA: 2:11 - loss: 1.5283 - acc: 0.518 - ETA: 2:10 - loss: 1.5279 - acc: 0.518 - ETA: 2:10 - loss: 1.5275 - acc: 0.518 - ETA: 2:09 - loss: 1.5271 - acc: 0.518 - ETA: 2:08 - loss: 1.5268 - acc: 0.518 - ETA: 2:08 - loss: 1.5265 - acc: 0.519 - ETA: 2:07 - loss: 1.5262 - acc: 0.519 - ETA: 2:07 - loss: 1.5260 - acc: 0.519 - ETA: 2:06 - loss: 1.5256 - acc: 0.519 - ETA: 2:05 - loss: 1.5254 - acc: 0.519 - ETA: 2:05 - loss: 1.5251 - acc: 0.519 - ETA: 2:04 - loss: 1.5248 - acc: 0.519 - ETA: 2:04 - loss: 1.5247 - acc: 0.519 - ETA: 2:03 - loss: 1.5245 - acc: 0.519 - ETA: 2:02 - loss: 1.5243 - acc: 0.519 - ETA: 2:02 - loss: 1.5241 - acc: 0.520 - ETA: 2:01 - loss: 1.5240 - acc: 0.520 - ETA: 2:01 - loss: 1.5239 - acc: 0.520 - ETA: 2:00 - loss: 1.5236 - acc: 0.520 - ETA: 1:59 - loss: 1.5233 - acc: 0.520 - ETA: 1:59 - loss: 1.5231 - acc: 0.520 - ETA: 1:58 - loss: 1.5228 - acc: 0.520 - ETA: 1:58 - loss: 1.5225 - acc: 0.520 - ETA: 1:57 - loss: 1.5222 - acc: 0.520 - ETA: 1:56 - loss: 1.5219 - acc: 0.520 - ETA: 1:56 - loss: 1.5216 - acc: 0.520 - ETA: 1:55 - loss: 1.5215 - acc: 0.520 - ETA: 1:55 - loss: 1.5212 - acc: 0.520 - ETA: 1:54 - loss: 1.5209 - acc: 0.521 - ETA: 1:54 - loss: 1.5209 - acc: 0.520 - ETA: 1:53 - loss: 1.5207 - acc: 0.521 - ETA: 1:52 - loss: 1.5206 - acc: 0.521 - ETA: 1:52 - loss: 1.5204 - acc: 0.521 - ETA: 1:51 - loss: 1.5202 - acc: 0.521 - ETA: 1:51 - loss: 1.5199 - acc: 0.521 - ETA: 1:50 - loss: 1.5197 - acc: 0.521 - ETA: 1:49 - loss: 1.5197 - acc: 0.521 - ETA: 1:49 - loss: 1.5194 - acc: 0.521 - ETA: 1:48 - loss: 1.5191 - acc: 0.521 - ETA: 1:48 - loss: 1.5189 - acc: 0.521 - ETA: 1:47 - loss: 1.5186 - acc: 0.522 - ETA: 1:46 - loss: 1.5183 - acc: 0.522 - ETA: 1:46 - loss: 1.5183 - acc: 0.522 - ETA: 1:45 - loss: 1.5179 - acc: 0.522 - ETA: 1:45 - loss: 1.5178 - acc: 0.522 - ETA: 1:44 - loss: 1.5175 - acc: 0.522 - ETA: 1:43 - loss: 1.5172 - acc: 0.522 - ETA: 1:43 - loss: 1.5172 - acc: 0.522 - ETA: 1:42 - loss: 1.5170 - acc: 0.522 - ETA: 1:42 - loss: 1.5168 - acc: 0.522 - ETA: 1:41 - loss: 1.5165 - acc: 0.522 - ETA: 1:40 - loss: 1.5163 - acc: 0.522 - ETA: 1:40 - loss: 1.5160 - acc: 0.523 - ETA: 1:39 - loss: 1.5157 - acc: 0.523 - ETA: 1:39 - loss: 1.5155 - acc: 0.523 - ETA: 1:38 - loss: 1.5150 - acc: 0.523 - ETA: 1:37 - loss: 1.5147 - acc: 0.523 - ETA: 1:37 - loss: 1.5147 - acc: 0.523 - ETA: 1:36 - loss: 1.5144 - acc: 0.523 - ETA: 1:36 - loss: 1.5143 - acc: 0.523 - ETA: 1:35 - loss: 1.5143 - acc: 0.523 - ETA: 1:34 - loss: 1.5140 - acc: 0.523 - ETA: 1:34 - loss: 1.5140 - acc: 0.523 - ETA: 1:33 - loss: 1.5137 - acc: 0.524 - ETA: 1:33 - loss: 1.5134 - acc: 0.524 - ETA: 1:32 - loss: 1.5132 - acc: 0.524 - ETA: 1:32 - loss: 1.5130 - acc: 0.524 - ETA: 1:31 - loss: 1.5129 - acc: 0.524 - ETA: 1:30 - loss: 1.5127 - acc: 0.524 - ETA: 1:30 - loss: 1.5127 - acc: 0.524 - ETA: 1:29 - loss: 1.5127 - acc: 0.524 - ETA: 1:29 - loss: 1.5127 - acc: 0.524 - ETA: 1:28 - loss: 1.5126 - acc: 0.524 - ETA: 1:27 - loss: 1.5125 - acc: 0.524 - ETA: 1:27 - loss: 1.5123 - acc: 0.524 - ETA: 1:26 - loss: 1.5123 - acc: 0.524 - ETA: 1:26 - loss: 1.5121 - acc: 0.524 - ETA: 1:25 - loss: 1.5119 - acc: 0.524 - ETA: 1:24 - loss: 1.5115 - acc: 0.524 - ETA: 1:24 - loss: 1.5114 - acc: 0.524 - ETA: 1:23 - loss: 1.5111 - acc: 0.524 - ETA: 1:23 - loss: 1.5110 - acc: 0.525 - ETA: 1:22 - loss: 1.5107 - acc: 0.525 - ETA: 1:21 - loss: 1.5106 - acc: 0.525 - ETA: 1:21 - loss: 1.5102 - acc: 0.525 - ETA: 1:20 - loss: 1.5100 - acc: 0.525 - ETA: 1:20 - loss: 1.5099 - acc: 0.5254"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - ETA: 1:19 - loss: 1.5096 - acc: 0.525 - ETA: 1:18 - loss: 1.5097 - acc: 0.525 - ETA: 1:18 - loss: 1.5094 - acc: 0.525 - ETA: 1:17 - loss: 1.5093 - acc: 0.525 - ETA: 1:17 - loss: 1.5091 - acc: 0.525 - ETA: 1:16 - loss: 1.5087 - acc: 0.525 - ETA: 1:15 - loss: 1.5087 - acc: 0.525 - ETA: 1:15 - loss: 1.5085 - acc: 0.525 - ETA: 1:14 - loss: 1.5081 - acc: 0.526 - ETA: 1:14 - loss: 1.5080 - acc: 0.526 - ETA: 1:13 - loss: 1.5078 - acc: 0.526 - ETA: 1:13 - loss: 1.5076 - acc: 0.526 - ETA: 1:12 - loss: 1.5074 - acc: 0.526 - ETA: 1:11 - loss: 1.5071 - acc: 0.526 - ETA: 1:11 - loss: 1.5070 - acc: 0.526 - ETA: 1:10 - loss: 1.5068 - acc: 0.526 - ETA: 1:10 - loss: 1.5065 - acc: 0.526 - ETA: 1:09 - loss: 1.5063 - acc: 0.526 - ETA: 1:08 - loss: 1.5061 - acc: 0.526 - ETA: 1:08 - loss: 1.5058 - acc: 0.526 - ETA: 1:07 - loss: 1.5059 - acc: 0.526 - ETA: 1:07 - loss: 1.5056 - acc: 0.526 - ETA: 1:06 - loss: 1.5054 - acc: 0.527 - ETA: 1:05 - loss: 1.5052 - acc: 0.527 - ETA: 1:05 - loss: 1.5050 - acc: 0.527 - ETA: 1:04 - loss: 1.5049 - acc: 0.527 - ETA: 1:04 - loss: 1.5047 - acc: 0.527 - ETA: 1:03 - loss: 1.5045 - acc: 0.527 - ETA: 1:02 - loss: 1.5043 - acc: 0.527 - ETA: 1:02 - loss: 1.5042 - acc: 0.527 - ETA: 1:01 - loss: 1.5040 - acc: 0.527 - ETA: 1:01 - loss: 1.5040 - acc: 0.527 - ETA: 1:00 - loss: 1.5040 - acc: 0.527 - ETA: 59s - loss: 1.5036 - acc: 0.527 - ETA: 59s - loss: 1.5033 - acc: 0.52 - ETA: 58s - loss: 1.5032 - acc: 0.52 - ETA: 58s - loss: 1.5029 - acc: 0.52 - ETA: 57s - loss: 1.5028 - acc: 0.52 - ETA: 56s - loss: 1.5029 - acc: 0.52 - ETA: 56s - loss: 1.5028 - acc: 0.52 - ETA: 55s - loss: 1.5027 - acc: 0.52 - ETA: 55s - loss: 1.5024 - acc: 0.52 - ETA: 54s - loss: 1.5023 - acc: 0.52 - ETA: 54s - loss: 1.5021 - acc: 0.52 - ETA: 53s - loss: 1.5018 - acc: 0.52 - ETA: 52s - loss: 1.5017 - acc: 0.52 - ETA: 52s - loss: 1.5015 - acc: 0.52 - ETA: 51s - loss: 1.5013 - acc: 0.52 - ETA: 51s - loss: 1.5012 - acc: 0.52 - ETA: 50s - loss: 1.5012 - acc: 0.52 - ETA: 49s - loss: 1.5009 - acc: 0.52 - ETA: 49s - loss: 1.5008 - acc: 0.52 - ETA: 48s - loss: 1.5004 - acc: 0.52 - ETA: 48s - loss: 1.5002 - acc: 0.52 - ETA: 47s - loss: 1.5001 - acc: 0.52 - ETA: 46s - loss: 1.5000 - acc: 0.52 - ETA: 46s - loss: 1.4998 - acc: 0.52 - ETA: 45s - loss: 1.4998 - acc: 0.52 - ETA: 45s - loss: 1.4998 - acc: 0.52 - ETA: 44s - loss: 1.4994 - acc: 0.52 - ETA: 43s - loss: 1.4993 - acc: 0.52 - ETA: 43s - loss: 1.4991 - acc: 0.52 - ETA: 42s - loss: 1.4989 - acc: 0.52 - ETA: 42s - loss: 1.4986 - acc: 0.52 - ETA: 41s - loss: 1.4984 - acc: 0.52 - ETA: 40s - loss: 1.4981 - acc: 0.53 - ETA: 40s - loss: 1.4981 - acc: 0.53 - ETA: 39s - loss: 1.4980 - acc: 0.53 - ETA: 39s - loss: 1.4979 - acc: 0.53 - ETA: 38s - loss: 1.4978 - acc: 0.53 - ETA: 37s - loss: 1.4976 - acc: 0.53 - ETA: 37s - loss: 1.4974 - acc: 0.53 - ETA: 36s - loss: 1.4973 - acc: 0.53 - ETA: 36s - loss: 1.4972 - acc: 0.53 - ETA: 35s - loss: 1.4970 - acc: 0.53 - ETA: 35s - loss: 1.4968 - acc: 0.53 - ETA: 34s - loss: 1.4967 - acc: 0.53 - ETA: 33s - loss: 1.4963 - acc: 0.53 - ETA: 33s - loss: 1.4961 - acc: 0.53 - ETA: 32s - loss: 1.4958 - acc: 0.53 - ETA: 32s - loss: 1.4956 - acc: 0.53 - ETA: 31s - loss: 1.4954 - acc: 0.53 - ETA: 30s - loss: 1.4950 - acc: 0.53 - ETA: 30s - loss: 1.4946 - acc: 0.53 - ETA: 29s - loss: 1.4945 - acc: 0.53 - ETA: 29s - loss: 1.4941 - acc: 0.53 - ETA: 28s - loss: 1.4938 - acc: 0.53 - ETA: 27s - loss: 1.4939 - acc: 0.53 - ETA: 27s - loss: 1.4936 - acc: 0.53 - ETA: 26s - loss: 1.4932 - acc: 0.53 - ETA: 26s - loss: 1.4929 - acc: 0.53 - ETA: 25s - loss: 1.4928 - acc: 0.53 - ETA: 24s - loss: 1.4925 - acc: 0.53 - ETA: 24s - loss: 1.4922 - acc: 0.53 - ETA: 23s - loss: 1.4921 - acc: 0.53 - ETA: 23s - loss: 1.4921 - acc: 0.53 - ETA: 22s - loss: 1.4918 - acc: 0.53 - ETA: 21s - loss: 1.4915 - acc: 0.53 - ETA: 21s - loss: 1.4913 - acc: 0.53 - ETA: 20s - loss: 1.4910 - acc: 0.53 - ETA: 20s - loss: 1.4908 - acc: 0.53 - ETA: 19s - loss: 1.4907 - acc: 0.53 - ETA: 18s - loss: 1.4904 - acc: 0.53 - ETA: 18s - loss: 1.4902 - acc: 0.53 - ETA: 17s - loss: 1.4901 - acc: 0.53 - ETA: 17s - loss: 1.4898 - acc: 0.53 - ETA: 16s - loss: 1.4896 - acc: 0.53 - ETA: 16s - loss: 1.4895 - acc: 0.53 - ETA: 15s - loss: 1.4891 - acc: 0.53 - ETA: 14s - loss: 1.4893 - acc: 0.53 - ETA: 14s - loss: 1.4890 - acc: 0.53 - ETA: 13s - loss: 1.4887 - acc: 0.53 - ETA: 13s - loss: 1.4886 - acc: 0.53 - ETA: 12s - loss: 1.4884 - acc: 0.53 - ETA: 11s - loss: 1.4883 - acc: 0.53 - ETA: 11s - loss: 1.4884 - acc: 0.53 - ETA: 10s - loss: 1.4880 - acc: 0.53 - ETA: 10s - loss: 1.4878 - acc: 0.53 - ETA: 9s - loss: 1.4876 - acc: 0.5340 - ETA: 8s - loss: 1.4876 - acc: 0.533 - ETA: 8s - loss: 1.4875 - acc: 0.534 - ETA: 7s - loss: 1.4874 - acc: 0.534 - ETA: 7s - loss: 1.4872 - acc: 0.534 - ETA: 6s - loss: 1.4872 - acc: 0.534 - ETA: 5s - loss: 1.4871 - acc: 0.534 - ETA: 5s - loss: 1.4869 - acc: 0.534 - ETA: 4s - loss: 1.4868 - acc: 0.534 - ETA: 4s - loss: 1.4865 - acc: 0.534 - ETA: 3s - loss: 1.4862 - acc: 0.534 - ETA: 2s - loss: 1.4861 - acc: 0.534 - ETA: 2s - loss: 1.4860 - acc: 0.534 - ETA: 1s - loss: 1.4857 - acc: 0.534 - ETA: 1s - loss: 1.4856 - acc: 0.534 - ETA: 0s - loss: 1.4855 - acc: 0.534 - 927s 593ms/step - loss: 1.4856 - acc: 0.5348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacco\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 59 - ETA: 59 - ETA: 58 - ETA: 58 - ETA: 58 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 56 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 54 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 78s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 1 artists>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#CNN for CIFAR10\n",
    "#Zachary Cosenza\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "#import\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.datasets import cifar10\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import to_categorical\n",
    "from heapq import heappush, heappop, heapify\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from heapq import heappush, heappop, heapify\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "def encode(symb2freq):\n",
    "    \"\"\"Huffman encode the given dict mapping symbols to weights\"\"\"\n",
    "    heap = [[wt, [sym, \"\"]] for sym, wt in symb2freq.items()]\n",
    "    heapify(heap)\n",
    "    while len(heap) > 1:\n",
    "        lo = heappop(heap)\n",
    "        hi = heappop(heap)\n",
    "        for pair in lo[1:]:\n",
    "            pair[1] = '0' + pair[1]\n",
    "        for pair in hi[1:]:\n",
    "            pair[1] = '1' + pair[1]\n",
    "        heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
    "    return sorted(heappop(heap)[1:], key=lambda p: (len(p[-1]), p))\n",
    "  \n",
    "def HuffmanBW(x_test):\n",
    "    \"\"\" input: x_test\n",
    "        output: average BW/image in Bytes\n",
    "    \"\"\"\n",
    "    bitstream=0\n",
    "    for imnum in range(0,x_test.shape[0]):\n",
    "        img = x_test[imnum,]\n",
    "        bitcount=0\n",
    "        \n",
    "        if(np.min(img) < 0):\n",
    "            img = img - np.min(img)\n",
    "        if(np.max(img)==0):\n",
    "            img[0,0]=1\n",
    "            \n",
    "        txt = \"\".join(map(chr, img.flatten()))\n",
    "        symb2freq = defaultdict(int)\n",
    "        \n",
    "        for ch in txt:\n",
    "            symb2freq[ch] += 1\n",
    "      \n",
    "        huff = encode(symb2freq)\n",
    "      ##### Calculating the number of bits needed\n",
    "    \n",
    "        for p in huff:\n",
    "            bitcount = bitcount+  symb2freq[p[0]]*len(p[1])\n",
    "        bitstream = bitstream + bitcount\n",
    "    BW = np.round(bitstream/(x_test.shape[0]*8))            ## BW in bytes\n",
    "    return BW\n",
    "\n",
    "n=1 #this is an indicator or depth (See keras Resnet implementation for CIFAR-10 for more details)\n",
    "#the following are the n values of the models a,b,c,d,e,f\n",
    "#a: n= 4\n",
    "#b: n= 3\n",
    "#c: n= 2\n",
    "#d: n= 3\n",
    "#e: n= 2\n",
    "#f: n= 1\n",
    "\n",
    "depth = n * 6 + 2 #model depth\n",
    "\n",
    "# Model name, depth and version\n",
    "model_type = 'ResNet%d' % (depth)\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=64,\n",
    "                 kernel_size=3, ######################### try to change this and see\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 64 \n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters  = int(num_filters*1.5)\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=4)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def creategen(X,Y,batch_size):\n",
    "    while True:\n",
    "        # suffled indices    \n",
    "        #idx = np.random.permutation( X.shape[0])\n",
    "        # create image generator\n",
    "        datagen = ImageDataGenerator(\n",
    "                \n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False)\n",
    "\n",
    "        batches= datagen.flow( X, Y, batch_size=batch_size,shuffle=True)\n",
    "       \n",
    "        idx0 = 0\n",
    "        for batch in batches:\n",
    "            idx1 = idx0 + batch[0].shape[0]\n",
    "            yield  batch[0], batch[1]\n",
    "\n",
    "            idx0 = idx1\n",
    "            if idx1 >= X.shape[0]:\n",
    "                break\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-2\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-1\n",
    "    elif epoch > 80:\n",
    "        lr *= 0.5\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr\n",
    "                \n",
    "def CrappyCNN(train_images,train_labels,test_images,test_labels):\n",
    "    \n",
    "    train_images = train_images.astype('float32') / max_image\n",
    "\n",
    "    test_images = test_images.astype('float32') / max_image\n",
    "    \n",
    "    train_labels = to_categorical(train_labels)\n",
    "    test_labels = to_categorical(test_labels)\n",
    "    \n",
    "    input_shape = train_images.shape[1:]\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size)\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "def CoolCNN(train_images,train_labels,test_images,test_labels):\n",
    "    \n",
    "    train_images = train_images.astype('float32') / max_image\n",
    "\n",
    "    test_images = test_images.astype('float32') / max_image\n",
    "    \n",
    "    train_labels = to_categorical(train_labels)\n",
    "    test_labels = to_categorical(test_labels)\n",
    "    \n",
    "    input_shape = train_images.shape[1:]\n",
    "    depth = 8\n",
    "    num_classes = 10\n",
    "    \n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth,num_classes=num_classes)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=lr_schedule(0)),metrics=['accuracy'])\n",
    "    \n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "    lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "    callbacks = [lr_reducer, lr_scheduler]\n",
    "    \n",
    "    model.fit_generator(creategen(train_images, train_labels, batch_size=batch_size),\n",
    "                        steps_per_epoch=int(np.ceil(train_images.shape[0]/32.0)),\n",
    "                        epochs=epochs, verbose=1, workers=1,\n",
    "                        callbacks=callbacks)    \n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "#Code for NN\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "image_size = 32\n",
    "channel_num = 3\n",
    "max_image = 255\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "BWtest = HuffmanBW(test_images)\n",
    "BWtrain = HuffmanBW(train_images)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.bar('RGB',(BWtest+BWtrain)/2)\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Bandwidth')\n",
    "plt.title('BW for RGB')\n",
    "\n",
    "test_loss_CNN, test_acc_CNN = CrappyCNN(train_images,train_labels,test_images,test_labels)\n",
    "#test_loss_Resnet, test_acc_Resnet = CoolCNN(train_images,train_labels,test_images,test_labels)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.bar('RGB',test_acc_CNN)\n",
    "#plt.bar('RGB ResNet',test_acc_Resnet)\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Acc.')\n",
    "plt.title('Acc. for RGB')\n",
    "\n",
    "#fit model with PCA\n",
    "\n",
    "train_images_pca = train_images.reshape(train_images.shape[0],-1).astype('float32')\n",
    "test_images_pca = test_images.reshape(test_images.shape[0],-1).astype('float32')\n",
    "\n",
    "pca = PCA(0.99)\n",
    "pca.fit_transform(train_images_pca)\n",
    "\n",
    "train_images_pca_proj = pca.fit_transform(train_images_pca)\n",
    "train_images_recon = pca.inverse_transform(train_images_pca_proj)\n",
    "\n",
    "train_images_recon = train_images_recon.reshape(train_images.shape).astype('float32')\n",
    "\n",
    "test_images_pca_proj = pca.fit_transform(test_images_pca)\n",
    "test_images_recon = pca.inverse_transform(test_images_pca_proj)\n",
    "\n",
    "test_images_recon = test_images_recon.reshape(test_images.shape).astype('float32')\n",
    "\n",
    "test_loss_CNN_PCA, test_acc_CNN_PCA = CrappyCNN(train_images_recon,train_labels,test_images_recon,test_labels)\n",
    "test_loss_Resnet_PCA, test_acc_Resnet_PCA = CoolCNN(train_images_recon,train_labels,test_images_recon,test_labels)\n",
    "\n",
    "BWtest_PCA = HuffmanBW(test_images_pca_proj)\n",
    "BWtrain_PCA = HuffmanBW(train_images_pca_proj)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.bar('RGB PCA',(BWtest_PCA+BWtrain_PCA)/2)\n",
    "\n",
    "plt.figure(2)\n",
    "plt.bar('RGB PCA',test_acc_CNN_PCA)\n",
    "plt.bar('RGB PCA / ResNet',test_acc_Resnet_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5854\n",
      "0.6012\n",
      "0.5051\n",
      "590.5\n",
      "2799.5\n"
     ]
    }
   ],
   "source": [
    "print(test_acc_CNN)\n",
    "print(test_acc_CNN_PCA)\n",
    "print(test_acc_Resnet_PCA)\n",
    "print((BWtest_PCA+BWtrain_PCA)/2)\n",
    "print((BWtest+BWtrain)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the following results.\n",
    "\n",
    "| Method | Test Accuracy (%) | Bandwidth (Bytes) |\n",
    "| --- | --- | --- |\n",
    "| Original RGB | 58.54 | 2799.5 |\n",
    "| PCA RGB | 60.12 | 590.5 |\n",
    "| PCA RGB / Resnet | 50.51 | 590.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests in DWT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In  DWT domain we primarily focus on 2 experiments.\n",
    "\n",
    "## Experiment 1\n",
    "\n",
    "Assume a cloud based image classification scenario. Here source device (mobile phone) sends an inference image over a bandlimited channel to the server to get the class label. Server receives the inference image and feeds it to a trained classifier to predict the class label. In order to conserve limited channel bandwidth and storage capacity, \n",
    "source devices often encode and compress the images before transmitting to the cloud by utilizing standardized\n",
    "compression techniques such as JPEG2000.Because most neural networks are designed to classify images in the spatial RBG domain, the cloud currently receives and decodes the compressed j2k images back into the RGB domain before forwarding them to trained neural networks for further processing, as illustrated in the top part of the follwing figure. Thus, a natural question arises is to how to achieve faster training and inference with improved accuracy in a cloud based image classification under bandwidth, storage and computation constraints. \n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/j2kcoder2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We claim that the conventional use of image reconstruction is unnecessary for JPEG2000 encoded classification by constructing and training a deep CNN model with the DWT coefficients with CDF 9/7 wavelets. See the bottom part of the above figure. Furthermore, we establish that more accurate classification is also possible by deploying shallower models to benefit from faster training and classification in comparison to models trained fo spatial RGB image inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result - 1\n",
    "\n",
    "We trained a set of ResNet models for CIFAR-10 dataset and following figures compare the test accuracy and speed for training and inference process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/result12.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure, (a) illustrates test accuracy vs inference speed for the CIFAR-10 data set. The blue lines represent results using reconstructed RGB images. Red curve is the result using DWT coefficients with CDF 9/7 wavelets. (b) shows the Test error vs training speed/epoch. Here rate is the number of images that go through the model in each epoch.  The proposed model delivers fast and accurate classification for both training and inference. The points a,b,c,d,e and f correspond to 6 different ResNet models. The following table summerices these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/figures/tabelresnet.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the code for the above implementation.\n",
    "1. Import the neccessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We used the following ResNet model (reference - keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1 #this is an indicator or depth (See keras Resnet implementation for CIFAR-10 for more details)\n",
    "#the following are the n values of the models a,b,c,d,e,f\n",
    "#a: n= 4\n",
    "#b: n= 3\n",
    "#c: n= 2\n",
    "#d: n= 3\n",
    "#e: n= 2\n",
    "#f: n= 1\n",
    "\n",
    "depth = n * 6 + 2 #model depth\n",
    "\n",
    "# Model name, depth and version\n",
    "model_type = 'ResNet%d' % (depth)\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=64,\n",
    "                 kernel_size=3, ######################### try to change this and see\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 64 \n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters  = int(num_filters*1.5)\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=4)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We used the following data augmentation method which gives flexibility to manipulate mini batches as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creategen(X,Y,batch_size):\n",
    "    while True:\n",
    "        # suffled indices    \n",
    "        #idx = np.random.permutation( X.shape[0])\n",
    "        # create image generator\n",
    "        datagen = ImageDataGenerator(\n",
    "                \n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False)\n",
    "\n",
    "        batches= datagen.flow( X, Y, batch_size=batch_size,shuffle=True)\n",
    "       \n",
    "        idx0 = 0\n",
    "        for batch in batches:\n",
    "            idx1 = idx0 + batch[0].shape[0]\n",
    "            yield  batch[0], batch[1]\n",
    "\n",
    "            idx0 = idx1\n",
    "            if idx1 >= X.shape[0]:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We used an initial learning rate of 0.001 which is reduced progressively at 80, 120 and 160 over 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-2\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-1\n",
    "    elif epoch > 80:\n",
    "        lr *= 0.5\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we used the following methods for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "#RGB2YCbCr - RGB to YCbCr conversion\n",
    "def batchRGB2YCRCB(x_batch):\n",
    "    alpha_R = 0.299\n",
    "    alpha_G = 0.587\n",
    "    alpha_B = 0.114\n",
    "    x_batchnew = np.zeros((x_batch.shape)).astype('float32')\n",
    "    for i in range(0,x_batch.shape[0]):\n",
    "        #Y\n",
    "        x_batchnew[i,:,:,0] = alpha_R*x_batch[i,:,:,0] + alpha_G*x_batch[i,:,:,1] + alpha_B*x_batch[i,:,:,2]\n",
    "        #Cb\n",
    "        x_batchnew[i,:,:,1] = (0.5/(1-alpha_B))*(x_batch[i,:,:,2]-x_batchnew[i,:,:,0])\n",
    "        #Cr\n",
    "        x_batchnew[i,:,:,2] = (0.5/(1-alpha_R))*(x_batch[i,:,:,0]-x_batchnew[i,:,:,0])\n",
    "    return x_batchnew\n",
    "\n",
    "\n",
    "#generate the matrix for CDF 9/7 transform\n",
    "def getTcdf97(height):\n",
    "    a1 = -1.586134342\n",
    "    a2 = -0.05298011854\n",
    "    a3 = 0.8829110762\n",
    "    a4 = 0.4435068522\n",
    "\n",
    "    # Scale coeff:\n",
    "    k1 = 0.8128662109 # 1/1.230174104914 // 0,2,4,6\n",
    "    k2 = 0.6149902344 # 1.230174104914/2 // 5038 1,3,5,7\n",
    "    X1 = np.identity(height)\n",
    "    X2 = np.identity(height)\n",
    "    X3 = np.identity(height)\n",
    "    X4 = np.identity(height)\n",
    "    X5 = np.zeros((height,height)).astype('float32')\n",
    "    for col in range(1,height-2,2):\n",
    "        X1[col-1,col]=X1[col+1,col]=a1\n",
    "    X1[height-2,height-1] = 2*a1\n",
    "    \n",
    "    #print(X1)\n",
    "    for col in range(2,height-1,2):\n",
    "        X2[col-1,col]=X2[col+1,col]=a2\n",
    "    X2[1,0] = 2*a2\n",
    "    #print(X2)\n",
    "    for col in range(1,height-2,2):\n",
    "        X3[col-1,col]=X3[col+1,col]=a3\n",
    "    X3[height-2,height-1] = 2*a3\n",
    "    \n",
    "    #print(X1)\n",
    "    for col in range(2,height-1,2):\n",
    "        X4[col-1,col]=X4[col+1,col]=a4\n",
    "    X4[1,0] = 2*a4\n",
    "    \n",
    "    for col in range(0,height,1):\n",
    "        if(col%2==0 ):\n",
    "            #print(col)\n",
    "            X5[col,int(col/2)]=k1\n",
    "        else:\n",
    "            X5[col,int(height/2 + (col-1)/2)]=k2\n",
    "    #print(X3)\n",
    "    X =np.matmul(np.matmul(np.matmul(np.matmul(X1,X2),X3),X4),X5)\n",
    "    return X,inv(X)\n",
    "\n",
    "#take Level 1 DWT\n",
    "def batchwaveletcdf97mat(x_batch,X,dimhalf):\n",
    "    x_batchnew = np.zeros((x_batch.shape[0],dimhalf,dimhalf,12)).astype('float32')\n",
    "    for i in range(0,x_batch.shape[0]):\n",
    "        for j in range(0,x_batch.shape[3]):\n",
    "            coeff_array = np.matmul(np.matmul(X.transpose(),x_batch[i,:,:,j]),X)\n",
    "            x_batchnew[i,:,:,j*4+0]=coeff_array[0:dimhalf,0:dimhalf]\n",
    "            x_batchnew[i,:,:,j*4+1]=coeff_array[0:dimhalf,dimhalf:2*dimhalf]\n",
    "            x_batchnew[i,:,:,j*4+2]=coeff_array[dimhalf:2*dimhalf,0:dimhalf]\n",
    "            x_batchnew[i,:,:,j*4+3]=coeff_array[dimhalf:2*dimhalf,dimhalf:2*dimhalf]\n",
    "    return x_batchnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. load the dataset and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape to resnet:  (16, 16, 12)\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR10 data.\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "num_classes = 10\n",
    "#convert to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "#level offset\n",
    "X_train = X_train - 128.0\n",
    "X_test = X_test - 128.0\n",
    "\n",
    "#RGB2YCbCr - This converts RGB images to YCbCr format to facilitate compression - optional\n",
    "X_train = batchRGB2YCRCB(X_train)\n",
    "X_test = batchRGB2YCRCB(X_test)\n",
    "\n",
    "#generate necessary matrices for DWT cdf9/7 trandformation\n",
    "M,M_inv = getTcdf97(32)\n",
    "\n",
    "#take level-1 DWT with CDF 9/7\n",
    "x_train = batchwaveletcdf97mat(X_train.astype('float32'),M,16)\n",
    "x_test = batchwaveletcdf97mat(X_test.astype('float32'),M,16)\n",
    "\n",
    "### max normalization\n",
    "x_train=x_train/np.max(np.abs(x_train))\n",
    "x_test=x_test/np.max(np.abs(x_test))\n",
    "\n",
    "input_shape = x_test.shape[1:]\n",
    "print('input shape to resnet: ',input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. convert labels to one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. compile the model: we used Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 16, 16, 12)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 64)   6976        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 16, 16, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 64)   36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 64)   36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 64)   0           activation_1[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 16, 16, 64)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 96)     55392       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 8, 8, 96)     384         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 8, 96)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 8, 96)     83040       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 8, 8, 96)     6240        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 8, 8, 96)     384         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 8, 8, 96)     0           conv2d_6[0][0]                   \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 8, 96)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 4, 4, 144)    124560      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 4, 4, 144)    576         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 4, 4, 144)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 4, 4, 144)    186768      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 4, 4, 144)    13968       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 4, 4, 144)    576         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 4, 4, 144)    0           conv2d_9[0][0]                   \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 4, 4, 144)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 144)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 144)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           1450        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 554,938\n",
      "Trainable params: 553,594\n",
      "Non-trainable params: 1,344\n",
      "__________________________________________________________________________________________________\n",
      "ResNet8\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  \n",
    "epochs = 1\n",
    "\n",
    "model = resnet_v1(input_shape=input_shape, depth=depth,num_classes=num_classes)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=lr_schedule(0)),metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Set callback methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [lr_reducer, lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Train the model - We used a server with Titan-V GPU. (We train only for 1 epoch to show the code works. We used 200 epochs to train both RGB and  DWT and models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lahiru d. chamain\\anaconda3\\envs\\tfgpumy\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (50000, 16, 16, 12) (12 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "1563/1563 [==============================] - 48s 31ms/step - loss: 1.6007 - acc: 0.4849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lahiru d. chamain\\anaconda3\\envs\\tfgpumy\\lib\\site-packages\\keras\\callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x160183b2748>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(creategen(x_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=int(np.ceil(x_train.shape[0]/32.0)),\n",
    "                        epochs=epochs, verbose=1, workers=1,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Evaluate the test set - The results below is after 1 eopoch.(to show the code works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 279us/step\n",
      "time per image : 0.27921199798583984  ms\n",
      "Test loss: 1.5340371086120606\n",
      "Test accuracy: 0.5315\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('time per image :',(time.time()-start)*1000/10000,' ms')\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we start with Level 1 DWT coefficents with CDF 9/7 wavelet and explre methods to reduce the required bandwidth () \n",
    "in cloud based image classification. To create a baseline, we train a ResNet-8 with quantized CDF 9/7 DWT coefficients BW\n",
    "and measure the classification accuracy of CIFAR-10 dataset (say $a_1$) and calculate the BW with Huffman encoding. (Say $BW_1$). Then we perform a principal component analysis (PCA) on the vectorized dataset to reduce the dimentionality of data with the perpose of reducing bandwidth. We can calculate the average BW of these quantized PCA projections (say $BW_2$). Then we reconstruct the wavelet coefficients to the original dimension and train a ResNet-8 to obtain $a_2$ accuracy. We observed that $BW_2$ is much smaller than $BW_1$ and $a_1$ and $a_2$ are considerably close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This observation implies than we can save bandwidth by only sending the projected DWT coefficients rather than sending the complete image\n",
    "with a negligible accuracy loss. We can apply this in to practice like this. We can do a PAC analysis on a large dataset like\n",
    "ImageNet and store the principal components at the server. Source device can calculate the projections and transmit these projections\n",
    "consuming smaller BW. At the server end, high dimentional image can be reconstructed using the stored PCA coefficients and feed to \n",
    "a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained the following results.\n",
    "\n",
    "| Method | Test Accuracy (%) | Bandwidth (Bytes) |\n",
    "| --- | --- | --- |\n",
    "| Original DWT quantized | 90 | 1552 |\n",
    "| PCA DWT quantized | 90 | 317 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the code for the implementation.\n",
    "Let's load the data set and calculate PCA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following Huffman encoder to calculate the required BW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop, heapify\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "def encode(symb2freq):\n",
    "    \"\"\"Huffman encode the given dict mapping symbols to weights\"\"\"\n",
    "    heap = [[wt, [sym, \"\"]] for sym, wt in symb2freq.items()]\n",
    "    heapify(heap)\n",
    "    while len(heap) > 1:\n",
    "        lo = heappop(heap)\n",
    "        hi = heappop(heap)\n",
    "        for pair in lo[1:]:\n",
    "            pair[1] = '0' + pair[1]\n",
    "        for pair in hi[1:]:\n",
    "            pair[1] = '1' + pair[1]\n",
    "        heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
    "    return sorted(heappop(heap)[1:], key=lambda p: (len(p[-1]), p))\n",
    "  \n",
    "def HuffmanBW(x_test):\n",
    "    \"\"\" input: x_test\n",
    "        output: average BW/image in Bytes\n",
    "    \"\"\"\n",
    "    bitstream=0\n",
    "    for imnum in range(0,x_test.shape[0]):\n",
    "        img = x_test[imnum,]\n",
    "        bitcount=0\n",
    "        \n",
    "        if(np.min(img) < 0):\n",
    "            img = img - np.min(img)\n",
    "        if(np.max(img)==0):\n",
    "            img[0,0]=1\n",
    "            \n",
    "        txt = \"\".join(map(chr, img.flatten()))\n",
    "        symb2freq = defaultdict(int)\n",
    "        \n",
    "        for ch in txt:\n",
    "            symb2freq[ch] += 1\n",
    "      \n",
    "        huff = encode(symb2freq)\n",
    "      ##### Calculating the number of bits needed\n",
    "    \n",
    "        for p in huff:\n",
    "            bitcount = bitcount+  symb2freq[p[0]]*len(p[1])\n",
    "        bitstream = bitstream + bitcount\n",
    "    BW = np.round(bitstream/(x_test.shape[0]*8))            ## BW in bytes\n",
    "    return BW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR-10, tahe DWT and flatten the data for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape to resnet:  (16, 16, 12)\n",
      "Dataset shape after vectorizing:  (50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR10 data.\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "num_classes = 10\n",
    "#convert to float32\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "\n",
    "#level offset\n",
    "X_train = X_train - 128.0\n",
    "X_test = X_test - 128.0\n",
    "\n",
    "#RGB2YCbCr - This converts RGB images to YCbCr format to facilitate compression - optional\n",
    "X_train = batchRGB2YCRCB(X_train)\n",
    "X_test = batchRGB2YCRCB(X_test)\n",
    "\n",
    "#generate necessary matrices for DWT cdf9/7 trandformation\n",
    "M,M_inv = getTcdf97(32)\n",
    "\n",
    "#take level-1 DWT with CDF 9/7\n",
    "x_train = batchwaveletcdf97mat(X_train.astype('float32'),M,16)\n",
    "x_test = batchwaveletcdf97mat(X_test.astype('float32'),M,16)\n",
    "\n",
    "#flattening for PCA\n",
    "x_train_ori = x_train.copy()\n",
    "x_train_ori = x_train_ori.astype('float32')\n",
    "x_test_ori = x_test.copy()\n",
    "x_test_ori = x_test_ori.astype('float32')\n",
    "x_train = x_train.reshape(x_train.shape[0],-1).astype('float32')\n",
    "x_test = x_test.reshape(X_test.shape[0], -1).astype('float32')\n",
    "\n",
    "input_shape = x_train_ori.shape[1:]\n",
    "print('input shape to resnet: ',input_shape)\n",
    "print('Dataset shape after vectorizing: ',x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate PCA and choose the components to preserve 99% of the variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original representation (50000, 3072)\n",
      "reduced representation:  (50000, 687)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(0.99)\n",
    "x_train_proj= pca.fit_transform(x_train)\n",
    "x_train_proj = np.floor(x_train_proj/5)*5 #quantization\n",
    "\n",
    "x_test_proj= pca.transform(x_test)\n",
    "x_test_proj = np.floor(x_test_proj/5)*5\n",
    "\n",
    "\n",
    "print('original representation',x_train.shape)\n",
    "print('reduced representation: ',x_train_proj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PCA we reduced the feature size from 3072 to 687. Let's display the first few principal compenents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Setup a figure 8 inches by 8 inches\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "# plot the components, each image is 26 by 26 pixels\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 5, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.reshape(pca.components_[i,:]/np.max(pca.components_[i,:]), (32,32,3)), cmap=plt.cm.bone, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now recontruct the images after dim reduction.For the display purposes we only show the 1st subband of 12 subbands of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original images:\n",
      "reconstructed images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAD4CAYAAADFGxOrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XeUZGd5JvC3cuzqrq7OuXtmeoJG05M0yiiOUECIYNbIC0YsXhuvfLC9h2N2F2Mf5/XaxsgiWGvAOAgFywKBsISExKAwozA59uTOqbqru6or1626+wf2ObLme15Nz157F87z+/M+ulW36t773W9K/b6fy7ZtISIiIqL/O+7/1wdARERE9NOAkyoiIiIiB3BSRUREROQATqqIiIiIHMBJFREREZEDOKkiIiIicgAnVUREREQO4KSKiIiIyAGcVBERERE5wLuS/9jtdtsej3mXWg13Zg8GwzgLRZT3c8HMqlgwc7nwfqiBvG3X4D4i+PW8Po+y36Vxe/BcV/tsVatq3J5dXpJiMX/BjpFond2QaDbuU8gW4Pv4g36YRWL4XOeX8zDT3i8YDsIs2hA1bvd58HkpW/jaWU4tw6xSxvtVKkWY1Wr42vL78WerVvH7ZbOL87Zt/6uT19TUZPf09hr/e23lBO2acitZ7d9xNYaKcs4qVfN1L6JfBwGfD2ZV5Zxpn9qj3Z/Kax46ePCC8yki4nK5bbd75f/2Vc+pG38n2jWnjZFeLx4TvF7z91ws4vHAo5w3TTBoHg9E9M9Wq+FrSPu+0POwWMxJpVK64CTEGhrs5vZ28/so58yrfB/auYYPPBHRnngq8JqWcn1rlKMX7dpX79FLHJu0MW3k5EnjPfp2K5pUeTxeaWhoNWblMn6orFmzDWYbhnbATHt4p6ZTMPP68ceqgYlHuVyG+2gntr6pAWaXKtKAJ5r+AH4QpJMZ4/bvfush4/aGRLPc/9nfN2ZHfnQEvk/P+m6Ybd+Jz/W+Fw/A7Oirh2C2dvsGmN3w3muN29vq6+E+55NJmP3o8R/BbHZ0DmaTk6dgVirhh0dn5yDM0ul5mL3yyhOjb9/W09srr+zebfzvtQHPo/zjJRLAk76Ccs9og5qtTEvQv81m02m4z9TiIsw64nGY9Tfj8XG5eGmT5EggALNMAf/DoaW+/oLzKfLjsScUwhMFxOfD5017vVx2CWaFYhZmTU1dK87OnNkP94lGLm1c3bDhGphlMgswyxfMY6eI/n3FYk3G7fv3P2/c3tzeLn/4139tzLRrJ1FXh49P+ceBpfyDo6xkGvSaC1l8fWgTP238iQVDMEvlcjC71AlXsYTHtJ+/4V3Ge/Tt+L//iIiIiBzASRURERGRAzipIiIiInIAJ1VEREREDuCkioiIiMgBK6r+s6yKzM9PGDNUFSgiEgziajathH1pCVdbaRV5WnlvNGquKimXcWVOpVJSjkMrt8XHqJUTa60H7CquaiiVzJ8BtZ+wazUpglYGic4EfB+vH1ebFIr4u1q1eRXMrrv9SphpVVrNMXNVTKaAq7daYjGYZW7FVUBPPPAYzNraBmA2NnYCZlWlAqdYxNUtCKqs01oLaC0JSpXKio9BOw4REbcL3xc+cM8cHMWFNy888iLM3nvfHTBLRHFVV8CLh0Zbua+1qqNoEFfkIS6XC1byNTa2wf1SqRmYaWPnpqGbYKZVuHZ1rYWZz2euaruUcVpEJBRSKuGUSr14o7mVgYhIoYCfQ9rzKxg2V6f5jpo/s1WpyuKcucLy7DSuTkwncfVrYwceq/MZ/CyxlBYxoSiuultOmcfIy669DO7T3IjPp1aNl1zG50Xbr6q2SMJ8SueAi8VfqoiIiIgcwEkVERERkQM4qSIiIiJyACdVRERERA7gpIqIiIjIAZxUERERETlghQsq+6ShwVzePjCwGe4XDuMSdm2xWW0Rxoiy4GY+j8tPK2VzyX8qNQ33icVwyarWNqGilKNH6vB3kl7Ei0VnMniR3WRy3Lgdlebbti1WxVxOH64Lw/fRSmdLBdxSIbeEWwS8sPs4zNKgBFlEJBo3l1f/xm/cB/c5PjkJs7kx3MajPo6vg9Q8LmGPx3G7EdSiREQvOUdcYM13bfFjn9I+QLsHKxYuydZaElg1pYUDKPPWFuPWHHj9GMy29PXBzKO0HdAWgM0r33NIaUWCBAJhGRgYMmZ+0KpARKS/fxPM0CLAInobm6Ghm2GmLWKfWTS3CmhoaIH71JRrRGtxU6vhaxKN/SIiAaVtwszMOZghZdDeZnlxWXY9usuYaS0CtDYYU2fxs6tcxNej1lIhGsetKdCYMHIMtz3pXtcNs76NfTBr7sTXqldpE1PI4pY62vesjXcXi79UERERETmAkyoiIiIiB3BSRUREROQATqqIiIiIHMBJFREREZEDOKkiIiIicsCKWiqEQlHZsOFacxbEJZgRZcXxSgWXPmorjmuls6iFgIjI+PiwcXtHxxq4z9zcGMxOnnwTZqtAKbSISHMAl5hGIngV9qVFXLrv95tXs3e5zHNnl8slvoC5zHv02Ah8n/OHcYnx1BTOtPYZ5TK+Dvp6N8LMBcrbf3DgMNzn1i243HxyNW5ZcWgXfk3t+NF5ERFpbGyHWaGAV2g3cQluBaC1MQi4Lm1ldquKX7Naw6vEa+0KULnz6b2n4D7Hju2G2Zqz22HW1IFbZHxk540wKymtJOrDuBXJcsFcZv9OPB7z+Skp15zkMsrr4dYOloVL8Ccn8TkIBPDnjsfbzEEJl69rbXiqFm5Vk0zisbq5uQdm2Rxu21JTrmV8b5s/m12zxSqbj1+p9Bd/ELdX8SntLPIZPOa6ldYgCzO4tUa82dzmwOvDLQ5mzuPn1uhx3Ioh3hqH2cCmAZh1DnbCzK99X8uXdo++FX+pIiIiInIAJ1VEREREDuCkioiIiMgBnFQREREROYCTKiIiIiIHcFJFRERE5IAV1VL7fH5p6+gzZnYVl52WlZXbXcq8zqutdK+U/p44sUd5TXNp6rU33gX36Sj0wuzA3l0wm1ZWN/d4cVlzWydut5BowqWibe2rjNtRKXS5WJbxE+YS5L2v/wC+T319M8w2DF0Bs7PDx2C248abYRauw+Xarz7znHG7tkp5W3MjzFCLCRGRgU39MGtsw695bN8+mBWLuIUDulYRW3DrBB8oyxcRqSml3LlSCWZlpaWCR1ntvaCMB4mouY3KjruuhPs8++zXYKa1s3jxmxGYXbv5Mpg1x3DLE638/lJYVlnm5swl59EoLjcPh/ExxhP4/q2v4TYTLuWcap+7UsLnG7EsfN0Fg3g8CGmtGJQ2PD7lXos3tK74Nd2gbUitVpVCwdzyx1JaRfhK+PhKhRDMPEqbA+2ZrZ1r1KZBa5kTCuF7raYMQPMTeHycG5uFmT8YgJnWpkEb/y8Wf6kiIiIicgAnVUREREQO4KSKiIiIyAGcVBERERE5gJMqIiIiIgdwUkVERETkgJW1VAj4pH2g3ZhZFVyump7DK4CXS7iMtK4RlwUffhO3Tejv3wSzlhZze4TMAl7VvVLBx3jNTXfi/ZTPNn7uLMy01crDUfydlArmNgLo9YqFvJw4fMCYvWvnPfB9etbj1d7T82mY2fYGmDV14FLunLLSurjM/y7IL5vLlkVENnXjlhX/NIXbHyTHkzC79ed3wmxhegFmhVwWZpUyLis3sW1bqqA82VYuqpJyfddsXHb9+b94GGZrtq2G2SfuwN/V+IL5u/rkh++G+xx95VMw+8dHH4CZ349L0R/64uMw+/An8LGMLeAS8IWpFMwuRTqN38vnwyX42rWwuIDL1LfegNtaHHvtCMxicXMJe1uf+VkiIlK1cLuOyXPmNjAiIj4fLqUPBHArhlgDLrO3yso4Pn7SuL0GWpvUalXJZhfh6yGlIm5xEI7gezTkMrcoEREJ1eHrX2tz4PWbpw0uF/5+tbZK5TJuf2Mr4492/y6n8ZwjOTsJs7Ex3PbnYvGXKiIiIiIHcFJFRERE5ABOqoiIiIgcwEkVERERkQM4qSIiIiJyACdVRERERA5YUUsF27Zh6wStbYILrNgtIjJyDpcwRmYbYNbbux5m66/GpfsdqzuM20v5lZWvX4y5sTmYJadmYGYr5aweD151HK3M7Xaby3H9/qB09w4as8UZXP4diuJS1nIRl84GI0GYFXP4+w+EcJl0NFpv3O4L4JLyXAm/lw+UC4uI+IP4NZeU6/+6918Hs+nz0zB77dldMDOp2Tb8bMUKPi9PfO9HMBt+Yxhm0yMTMDv48pswCyrXD7pn/Mrq8TvuuAJm//QULik/c2Y/zCoVXOa9lMTnWmsDo7VYQUKhmGzefIsxq2/C42MhW4DZ1MQ5mHnceHzR2iZo32V//5Bxe3YJt7HxePH5ro83wswFxjoRkVoVl+dXq7iFw/w8vkcjEfM5cIPv0bZtqVbNz9BwOAbfR2stMDNzHmaXDeE2GNPjozCr1fD7NTa2GrcvLeGWM/GEeZ8fvxdux7Scwc+huTncWqNUxC11YjHcvmc+ice0i8VfqoiIiIgcwEkVERERkQM4qSIiIiJyACdVRERERA7gpIqIiIjIAZxUERERETlghS0VRKoVc+lppAGXLs+M4JLUpSXcdkAra77t594Ds7lRvNL6ujW9xu1r2/GK6TPpNMyW8rh0M7JjI8yGbtgEs8f+1+MwCysri1tl3IrByOUSN2h3EajDbQxQWwoREX8Itx3Ip/MwW7fd3NpBROTy7m6YJdpBebVSWp1XVkyvr8cl8YEw/k4iDRGYlQv4/QYu74eZVoL/o5ceu2Db7FxKvvDFbxr/+0RnE3wtjwf/22p+EpdJb7p+C8yefQxfww9++n/CrKWlx7g9HMXnpaWnBWY/+/Ffg9mX//QzMHO58PVTLuNxKRqNw2xuDpewI16fB7ZOaOrC5zQYxu1Ljh5+BWao3F9E5M7rfw5mfRtWwQzeo4qP3HsnzE7P4HY0jz74JMy8Xtwuoqi01GnrMF+TIiKppPlZ4xLUxiYg7e3me76qtHyo1XDLh+a2TphpbWyaW/F+WmuKmUlzK4NAIAz3mZo4C7PLr9gBsyNv4nN95sw+mJVK+FnT2orHXKu68rYnb8dfqoiIiIgcwEkVERERkQM4qSIiIiJyACdVRERERA7gpIqIiIjIASteULlSMlcyWaAqUESvpAkGcdXUnfd+GGa9G8xVfCIi6XlcrecGx9IRx1U7NRtX1Z2YmIRZLIIr9YZ6cEXJrl5czeRSKrWS47hSyyTR1igf+8y9xmygBR9Dcx2uxIoEcIUcqjQUEfEpC0VrprabF8+uC+Kql6qyWGjQhxdy1WjXeDiGr4PT+87ArGcdvkZMcpllef35l4zZVe++Ae6nLSauLVo6+xReyNXrxVWgt37wHpi9+K3vGrd3BXF1qLb4d1NXM8ze855Pwuzpp78CM+06DgTwuQ6F8H2D5HNZObJ/jzHrmMEVd12r8fjY0oyvq6pSZfaee3fCTBsjexLmBWyPT+Kxc2R+HmYbOnHV2ror18Est4QrtX1BfN8vTC7ALFxvrnjzHTSPg+FYWLbets2Yfexnbofvcz6Jx3ZtPJvL4EWr8zm86Lb2DJ0dHTBuP3sAV/j5/Xg8PnX4KMw2X3M1zNZt2gyzN155AWaLi7gbgWXhSu2LxV+qiIiIiBzASRURERGRAzipIiIiInIAJ1VEREREDuCkioiIiMgBnFQREREROWBFLRXEtqVqmcs3F2bwwsjxZrzwZ2+vuSReROS3fv0+mH3xse/A7I734fLxsaS5VPd3X/hbuI9Wst3a1waza67DJZ/TS0sw01ojZLO41LW9x7zwsBu0YfC63dIYNbe0KCiLDp+bw+daK/0tV/BiregYRURO7T0Fs4Zm82KziU5zGbeIyGAHXjw7V8LtBdC1L6KXa2sLMZcK+P08vpW1mfB6/ZJoMpebH919GO43fOI1mBWKWZhls/gaXrsWL5J6+OUDMJudNS86PDmBrwGXG39P2ewizDRaS4hMBpfYT0+fg9nGjdfDbHjYfA4CgbAMDFxuzLRF7MMxvLhtXQwvcBwM4xY36zvwQupRpYVJOm9e3HZLL2778NDX8MLIXzuMW3loi5Bv3YkXANdaomiLxKMMLd5crdYklzaPFVOL+FrV2kgEvCt7jP+LsNL+5tAYbqXSc6d5bD0yPg73yRbxIuSz8/hznzuE7yftOXnz3e+DWTqJn6EH3zS3pBEROXTohzB7K/5SRUREROQATqqIiIiIHMBJFREREZEDOKkiIiIicgAnVUREREQO4KSKiIiIyAErqsWsWlXJLJhXvXa7cFlzJoVLGD/wqx+C2fA0Xk06OYbL+t9QSt+PvHzEuN0fxGWzmvwyLu/tWtsFs6VZXEZaVsr6vV68mvpyynxuUCuA2cmk/Nln/7f5tdK4XH5mBq9GrrV8WLUKt5joHMDl1T3re/CxjMwYt+9+ajfcx+fHl/3dv3w3zDrX4LJmTTCCy823374dZjWruqL38fg80tBqbjGxNIv3q29ogVlp1lwOLyLS3Gxu4SEicv48buEwPj6Mj6Xe3H6lWMQtK1w1/D3Ztg2zQt58v4iI+P34nPl8uBRdW+U+tYDHM6RWq0o+b25r0dAah/tp40tLN24p4vMr44tSFt8Rx8cSC4WM2/PKOLf7mRdhZlm4bUJrax/MllPLOFvErUPmJ8xteETw9VUumq+DwnJeDu7aZ8xSU7hVx/H9B2F2zR03wWzjdRth5vfhcVA7n6gdUFNdHdynPoxbfDz021+HWVsfvlY334yfJy89jlsjaDZf8S6YsaUCERER0b8jTqqIiIiIHMBJFREREZEDOKkiIiIicgAnVUREREQO4KSKiIiIyAEraqngcrnEC8rRKxVcbhuO4NXUn/7K0zA7uW01zBId5tJrEZFoHL9fQ7O55Dw9j1sB+AK4zLhcwCXUz371GZgtL+Hy3kR7M8yUxdSld2OfcfvuN8wrvodjYdl661Zjtu/7e+H7BAK4PFZrqVBXj8t0Q1Fz2bWISLgOv5/bbf53wdad5s8lIlKtWDB77m+fh9nrLz0Hs3i8DWaXbdsGs+ZufK7b+vFrmlQrlqSmU8asprQdyOVw+4yJyVMwa2nBbTByOXwdaG0HyuUCzJBgMAIznxe3SqkG8DWntU1wKTdhQ0Mrfr8avu6QUikv586Zy+lLJdzuor0LtyGZHBuBGbqfREQe/POHYaa1KUknzdfCHf/pdrhPrYbb4uze/S2YDQ3h9gLVKv7+MxnczsDWWnaIuaVCpWK+xm3bhp/t6F5zqwURkTs++n6Yaf7sU78Ds1VrhmCmtRhq7TNf49F6fB+ePXQOZtr9dPiN12HWsx63dPEF8TN7fhr3l+mM4/vmYvGXKiIiIiIHcFJFRERE5ABOqoiIiIgcwEkVERERkQM4qSIiIiJyACdVRERERA5YcUsFVDrrcuEVquOt5jYGIiIuD57X+QK4rDOXwavWd67phFn3OnMZZnYPbnGgtVsIhHDp9fEjb8KsqQkfYyiKs7mxuRUfi9ttLlmtlCoyfW7amM3OjsH3iUaVFeljuNXFQtL8XiIig9sGYYaOUUTk4Yc+b9xeqZTgPlrZ+Natt8HM7wvCzLIqMHvy4S/DLDk/DrMtm2+FmUmtWpU8aGWQVdomVKu4ZFxrm2Db5nJyEZHGRtwOwqd8j6g9gm3jEnuPB5dPa20HAkXcUsHl9sDM78fH7/XiY/H78ftBti0WKM/XStFnJvF1pX0n2vEf2oPL22dnR2CWaGw3bnf9Db4P40priq5OPFYsLuJy+Y6ONTBbWpqBmXZ/9PZuMG73eMzPyWrVkkxm3pi5lWtu/3O43UKlgsee8fFhmI2OHoVZNovHi1hdwrg9EsXP+WQSP0++9J1vw+zIy/gY//L3fw9mVaUNhtZaY2KiD2YXi79UERERETmAkyoiIiIiB3BSRUREROQATqqIiIiIHMBJFREREZEDOKkiIiIicsCKWioEI0FZd+V6Y3Z893G43+SZCZjVN+Hy/EIGr1ivtVQYP4HLN0t5c6l9uYTLUkNRXAqNXk9EZPt1N8IsGMZl2VNnJmG24RpzCa+IiFU2l4qiyndf0C9dg13GLDm+Gr5PWz8ul88uZWGmaelpgdnLT7yEj6Wt37hda5swOoqv1XIJX3NxtU0Abq3RmDCXlIuI1Gxc+hurx+0pTKo1XK6dL+CWIQnl+EKhKMxqStmy1m5BawUQVcqykQpoOSCCWzSIiNTq8NijldFbFn4/rbWG9n0htuDy8KUl3F4lHsctCbTM68VtbObm8Lg6OLgdZsj0CH4udK/pg9nUtPmeF3mH8618/y4XbmfQ1tYDM9TCAV0HHo9HIhHzNZ5T2p5MT5+H2fw8/h4jkXqYaa0FtPYfxaL52asdRw60ehEReeHhF2E2M45bg8wobTy0Fiz/1vhLFREREZEDOKkiIiIicgAnVUREREQO4KSKiIiIyAGcVBERERE5gJMqIiIiIgesqKWCL+CTtgFzWblWgjw3glcOL2SLMLMquOSzmMMrrWstEGo1c6l3uYhbI9Q34bJUzcSZUZh1DuAy3WoVl4N6fPiUPfKlvzRuT83h0mukphzD+Elc5qqJt+IS9vwyPp9aCX4yaT6WW959r/J6uHw6l8/ArFTCx9jUZG5NISLS1bUWZqnUDMzKZXxvmPh8QenoXGPMtFL/5eUUzIJB3FKhoLRp0L4rtxtfw+g4tRYNWqYdR7GI239ox6hdj1qmtWJAAoGQ9PdvMmbVKj6n6nestKAIh/H5tpX2H9r1VSyYv+eI0j5jaQ63F7h869UwS07h+6mpHbeSaGrFWSppblMiIhKNmsc0j8f8/btcbgmAdgU+H25nEQrhVhEdXatgVq3gc1Yp42deRblWvV7zZ9Nam5SUVjXDR/bBrACuHRGRTjDWvZNLuQ9Xgr9UERERETmAkyoiIiIiB3BSRUREROQATqqIiIiIHMBJFREREZEDOKkiIiIicsCKWipUypYkx5PGTCslXrV1NX7NIi7FnTozBTN/CJef1izcDsDtMc8jG1sb4T5VpbWDVcaZx4NL9xdncBn7oUN41W6tvcOGjeZS4+npM8btNasq2UVzWfymG81l3CIirz71Esw8Hh/MtGMvF3GZq3au4w3mUuh9r78A99FK8KcmT8NMK89Hq9WLiLS3D8Csr28jzAYGL4PZnj3fvmCbbduwtF1rz5DL4fJ1rUza7w/AzOcLwkxTq5lLwAOBMNxHG3vKZVzKHYslYGZZ+L7WXtMPyuVFRIJBXBaPucTrNd9TqJxfRCSbXYSZ34/PDWo5IyLS2TkIs0gD/mw+v/kxU1bGfo8Xj53a/Ztoa4GZNsZotOsLtQqo1czPINu2xQKtMLT3yedxawGtDYbHh79Hrx+fs6C98vtNO2flAm7fEI/jdhZoPBDRx7RqFd+/Gm1Mm1SeDW/FX6qIiIiIHMBJFREREZEDOKkiIiIicgAnVUREREQO4KSKiIiIyAGcVBERERE5wKWVp17wH7tcSREZ/bc7HPo30mvbdvPbN/J8/kS74JzyfP5E4z3604Xn86eP8Zy+3YomVURERERkxv/9R0REROQATqqIiIiIHMBJFREREZEDOKkiIiIicsCKFlQOhiJ2XazBmLlceH6mLeqIl5AUSafwoqDa4pPasYSj5kUkIw14UcpaFS/QvDSHF6NVj9GNs6qymKvbjb/LGlhEslDMSrlcvOAN6+rr7USLeTHLzEIGvk8wiheNDStZyI8XRi6W8WKnAR9epLkGCi2068rjxteH34tviUoVL+65XMCL7FoVvJ+2WHdJWQB2ITk1b6j+s93gs2nXjdeLFxHVFjTVrm9t8eBKBS+u6vGYv3/1+H34nJVL+DusgoVtRUQsC++njS/aceoL5mYuOJ8i/3yPtrYZ97HBor3//Gb4GMGi8iIiVQufb3Rt/TjD74cWadaKpJTDF+3u1va75KIsZTf0mgtzc5LNpC84mlAkatc3NBr30b5fbbHiSASPuaUKvsbTyTTMtAXF0X3j9eLxPagcoz+Ix/dgEI9N2ljt9eDvy6dk6HkiInJg/37jPXrBe7/Tf/BWdbEGef/P3m/MAiH84eub62GmDTTPPPIPMEMrt4uIBAN4QN9y/VXG7TvuuhLuk8/kYfadLz8FM58yGfAF8AW4uJCEWTSKv8vljHkS+tpr3zFuT7S0ymf/4kvG7LlvPAffZ+N1G2G25drLYXZZVxfMjk9OwGwNeKiIiOTBZEy7qepD+AbvbWqC2UwaD0IvHDkKs8WZFM5m8aR85NgIzL7+ld+6oCzb7XZLMGj+x0EkjK+b1rZ+mOXz+DP7fPieHxzcAbPp6bMwq6szP3DC4Tq4T3MHvj4mR0Zgtrg4C7O5OVz1HgiEYaYdp88XhNnevc8Y3zDR2iafe/DLxn1KBTw59SoP4VAMH39mXvnHVAQffyCMr4VKyfwQrij/aHArx689M3x+bYKNJxgabfJaKZsnH3/86V81bq9vaJSP/tJnjJn2/TYoz9ArduDxeGQaX+NPP/Q9mC2l8DNoaWnOuL2xsR3us/4K/FzoWoufC+s3roKZNlbHI3gO0BGPw2y5WIRZLBS6qFYY/N9/RERERA7gpIqIiIjIAZxUERERETmAkyoiIiIiB6zoD9XLxbJMnh0zZlpFT1Mr/gO2QhZXTWUyCzBDf9AqIlIu4T82O7xnn3F7Y3sC7tPYjt9L+4P5/k0DMOtY3QGzw7sOw0z7g9DuaLdx+8Ejz8N9kK23boWZ9oebT/3V0zCbu+camG0exN/VeAr/oTeqbrlyFf4DR+0PXRdzOZh98+kXYDY+PA6znvU9MEspf8ReWMb3hpkbVuAUilm4l3YvFQrLMAuHYjDbcTsu/BgbxuPBllu2GLe3deKim0ut5hwdnYaZpVRlRhvwH8FGoviPwKNB/IfIdww9Y9xu2zasEPUoxRh55drRKo+1CjltrNZeE9H+GL2mVCH6grjIR/vcWhV6zcJjmlZRCM8B2KmuPio33G0eB4tKpd62/j6YgeJKERGJBPDz4voHPw2zN86eg9nk2Snjdq168dEHvgazhUfxfRgK4ap8bdyKxfAfsXf04T+MX72g3U/dAAAXt0lEQVR1DcwuFn+pIiIiInIAJ1VEREREDuCkioiIiMgBnFQREREROYCTKiIiIiIHcFJFRERE5IAVtVRwuXDJdiiEy4y1dYtmJs0tGkT0RVm1dgtaaWdH52rj9olTeO25VZtxef6v/sF/htmXf/cbMPuT3zKvoSgi8qGndsOsmMPtIpq6zGWkqHR5ObUsux75oTFbu2MdfB+tJUFzNy59P/bqMZillYWpW/rMiz6LiKztNLemQGsCiojsOXMaZoNtuNwfrWEmIlIXx6W/6Xm8fl5qGrdUSCXNa2wh0WiDXHvtB4zZ+Pgw3G9sTDkv6XmYpVK4FPrvHvgLmHV2DuJsTadx+7u24bXD+ppx+fSZWfwdnj2D22DMjszArJDArST8g7hcu73BvBj9O0FdDspF3MZGu0fRenXvxOfH7WNspa7fC1oZlPLawtp4DNfaXWiZx4tfs6a0ialVlXYLqJUEOGlV24bryyXH8Xp72WW8/uz2tfj5NJfBazkmonjM0loxWOD6+eBdN8J9tOfWA5/7LN5PaemysDAJM8vCY/XLL+Nnjf8xvC7sxeIvVUREREQO4KSKiIiIyAGcVBERERE5gJMqIiIiIgdwUkVERETkAE6qiIiIiBywopYKXr9XmjrM5ct+ZeXwhtY4zOJxXC5/+VXb8H5t+DXPHjgLs5beFuP27rXdcJ/je47D7M10DmanTx7A2Qwu2b7jF+6A2etPvw6z6XPmEnfUCqBarUo6tWjMzh7E32EwEoTZZddeBrOOvjaYTYJjF9HbLZxAq9n3wl3kqlXmthoiIn/z6D/B7PO/iVd1f/cdH4dZ+wBu0xCK4hLeSAS3IjG+T3er/LfP/7ox+9SH/wvcr1QqwExra9LTswFmjY34XPv9+DMffeWocftyCpdWX33nlTCbn8EtK4Zfx20mRo+fh9nMLM62XXs9zDp+8R6YIeV8Sc4fOWfMalXcxiCqtPjQrjlNxY/L1ANhXIJfq5lbMWj7VNF9LSLlAm6XUi7iTGszYSstFUJ1YZgVsuZ7B7XBKGYL8Lrb/b1d8H223XQVzKLK8WltPGYzuNXLo194AmZT46PG7VuuwOPBHbddA7Mrrv4OzEanZmE2cx4/Q/d89xWYrR7CLV0SYH4jIvK5+z8Ks7fiL1VEREREDuCkioiIiMgBnFQREREROYCTKiIiIiIHcFJFRERE5ABOqoiIiIgcsKKWCi6XS3wBc+uEcD0uvc5ncNuBNdtwebu2srW2onddog5mHo95xfTZUVy66fXhr0krC95+3Y0w23sel2Xv3DoEs9xSFmbppHlF8sD3zMdoWRVZXDR/7tZu3AZA88NHfgiz5u5mmF12DW7FoF0H69cPGLd7wXkW+fFK8Ug6icuM77v/f8Bsyy1bYDb8+gmYndh7GGahEL6OTWwRscFnKxTwdRMI4BJ7bbX3jo5VMEun52EWCsVgFgyb23WgtiAiIntf2A+zpVlzyxARvcQ+3pqA2cTESZgNHzwCs5J1F8zgPoWSnD8yYsxqStsBn9Lixq7h69/jw/fNqiHzvSYismozvhYscJz7n8fnzePFx6GcNqlauDWC14/H8cyCeewUEalT2lMgpbx5zPL6vZLoMF9bLhf+jePM/jMwe/IbX4dZOIzHkGgUtyVaWpqDWVOi07i9qrSlmE3jcXWwDbdfaavHbWW+/Nw+mNUprSQiDfh85pS5ysXiL1VEREREDuCkioiIiMgBnFQREREROYCTKiIiIiIHcFJFRERE5ABOqoiIiIgcsKKWCuViUc6fOG1+Ia95JXIRkdVDa2GWmcelrGWljDreistB2wZwiebY8THj9uYELvdv6sIrV2u23LQZZhOnJmD2J0/tgVl9My4xnTo7ZdxeKpSM22u1Kiy1P33sKHyfvjXrYKaVLbf2tsJs8vQkzHa8ezvMCmXzqvRdjY1wn4Oj5lXWRURO7jsOs/v/4JMwqwvhtgRl8P2LiOQz5lXuRfTydjEs7G5Vq5JcXjb+514vLrGvr2+B2cT4MMyWl3G7gngc34Prr8Kr2e970Xzta/dgbgmXQc9PLcAsvYjbPtTH8fvNL+BrdWwcXz8zqV+C2aXw+vGYW6vi8naNP4BfM5MyX1siIodfwq0klsF+2jGifUREPF78W4BbaaVyqSYXzOOqiIiA9hSoBUgg4JfVgz3GbH4eX1d+v7nViIiIx4PH3OnpczCLx/F4XK3idh1T02eN25/86tNwH9RGQkTkm2enYXZ476swOz+Cr7lyGY+r1uPmZ4ZT+EsVERERkQM4qSIiIiJyACdVRERERA7gpIqIiIjIAZxUERERETmAkyoiIiIiB6yopYLH45WGRnOp8ZlTB+F+O26/GmaHfngIZm43Xo68mDOvAi4i4lPKgkN15tL3zkHzytsiIktzSzBbUEq2R4/j0v2e9eayWhGRwe1rYBaqC8OsUjKXivp85tPscrnEBZZ8D4dj8H2qFWvFxyAicmrvKZjt/NhOmHnceO5fHzZ/H48+8RzcZ+yEua2GiEgqNQOz6Vl8roPduIXATduHYPaRnTfCbDyVgtkXfv/XLthWq9Ukl82bd7DNpd8iIrk8XkH+V37zj2D23b9/GGY9q/A1nFbup0rFfP1oLUg0M1MjMMvl8OcOhepgVixe2kr2buU6Rlwul3hBa41yAd9rWrsFlzKu1kCLABH9vPVvGoDZ2h3mljqrejvgPocO4rFi3/f3wsztUdotKN9/uYjbngRCAZhZZTAWgnE15PfL5d3dxqyrC7cempg4CbNCAbefcLtxi4lkchxmgQB+ztRq5nYLX/3i5+A+WtsH9AwSEQkGIzDT2kzUarhdRzCAX9Oq4jZO5TKec7wVf6kiIiIicgAnVUREREQO4KSKiIiIyAGcVBERERE5gJMqIiIiIgdwUkVERETkgBW1VHB73BKMmMsYm5q64H5a+4MXX/x7mG3ffjvMAgVc8pmaS8LMBqXl2kr30XgUZuUiLmueG52D2bHdR2HW2oPL831BXCrdv7HfuN3jxy0VfD6/MfP7cLnq4jxuLZDL4bLrhBu3rdAUlDYNszVzWfzqLavhPs3dzTA7e+wYzLavx20CzifxNbfnJC4Pv2YdLqM+NYNXbzdxud3iB+XfkWgD3O/jn7mwPcO/+Mj7boNZS08LzB75wldhlstlYLa8bL626uricJ+FBfw9VSq4VN6y8HU1PPwazJaXcauLUAiPFS0N9TBDajVbijnzZ/CAVgsiIlYZl4bn81mYxRrw97z2SnytbrxyPcyWlswl//veOA73aWxvhJnWViY9j9tk+MA4KCJiVcxtAkRErDK+htxe8LsEeM4sFwrywsEjxqy5E49LxSI+Z1q7lHQGj0taKwP0nPzxsZiflV6v0sZDeS+N1r5Ea3GgHYst+LNpn/ti8ZcqIiIiIgdwUkVERETkAE6qiIiIiBzASRURERGRAzipIiIiInLAiqr/ajVbSnlzJQSqIhMRmTmHF6m9+eaPwMwfxK+JqhBFRArZwopf88jreJHOjTu2wezEPrwgdNXCFThuZYFJ6xyuSkKLWYqITJ42LzqbXTRXjvj9QenuXWfMovW4imnV5lUwGxvGixWHwWLWIiKzI/ga0cxPmqvFalW8oGb7QDvMPErVyJFR/NkCygK2azrxwrHn5nCF6C0bLoOZiVWqwIrTn/nlj8H9Pv2xD8Hs6ARedPVD734XzB79wtdgNjqKK1+Hhm42bo/GcOXc8PAbMEPVhCIiQ5tugpm22OzOnffB7NVX/xFmM6lFmCG2bUu1al60NxDGC/2WLVyx5vPh/bxKhdzx3bhabwHchyIi+Yy5giudxJV6q7fi6t2hm/AC5dqCygMDuEI9HsEVhWULj7nfeOBx83F4zZWZyck5+epvP2jMGuN4XCqX8DOtp3cDzM6fPwyzYBCP8YuLeDwugWPRKvxcLm2ha1zFqlXvaost28qCytUaWARbRGrgXlsJ/lJFRERE5ABOqoiIiIgcwEkVERERkQM4qSIiIiJyACdVRERERA7gpIqIiIjIAStqqVApl2R68rwxGx1VFqINvhtmt3zkFpjtemwXzKaV8vaKslAqWvBUK/nU2jfoC7bilgp+pfy0VMrDTFuwuFIxf+5KxbzwZEd3q/zen/+6MfN6cJlrZxwvuvrgw9+G2bf/6hGYxeOtMNMWeUWLXZ86gkuJ9/wAL046NzcKs9nzuMx45BjeL7OAFxAeGBrA77eIz7VJY32d/Nyd5jYB+TK+J5aLeGHSxgguu64qZcv3/tdfgNl///iPYBaJmFsnpJKzcJ9AALfqaGrC5fe//Ef3w+wfHsCtEYJR/H7r1l0Fs2Mv41YSiNfnkcZW8+LCuQweJy51AVutFH3NVrygeOcgXiz94IsHV3wcV+zcDrOWWAxm6ztw+xK/Fz/uXjyO20W8+j28uDZqV1MDbRgsqwzbdYTD+HMFlYW6s1ncqiOXw20rSspixRZ4lojg9kn5vLLAsQe3nCkUzAtui4h4vbitkvbsjYRxC5bUIl6AXTsHpTJua/FW/KWKiIiIyAGcVBERERE5gJMqIiIiIgdwUkVERETkAE6qiIiIiBzASRURERGRA1bUUqGhKS7v/YR5Rftn/w63HXjzzWdhdvkNm2C2uIDLqKtVvHK4Vk6cTieN28tlXA76yjPfv6TjWFqagxlq7SAiEg7XwSyfx+WnsVjCuB21i3C7XBINms/b/DJ+n8UcLsXVVrLX2hWcOvUGzEIhXOaaSJhXdh8ZweXr6BoQEdmx4y6YlYq4zFhrjVCt4JXPvX5caozKtZFKtSpTS+Y2DAGlnHxqEZdklyx87JZy7V+5bQPM8nncYuK6995o3P4r930A7jOSxOdTUx/GrRGOXoWPf9/z+2D2/l+8F2Za2w3EqliSnDKPg7Ztw/08Sgm7pbScSXT2weyD/2EnzMYWFmDmv3OHcftdV+G2CdPgOhYROTmNS+Kf/LtnYPbdR/8WZlNTZ2B2883/EWboHKDnQrVakcWUuTVLJNIA32do+7Uw8/hw+5t0eh5mto3bZ2jfhwXa+vj9+H7S2h+EQvh551NaKuSVVgw+X0DJ8FylWsXj3cXiL1VEREREDuCkioiIiMgBnFQREREROYCTKiIiIiIHcFJFRERE5ABOqoiIiIgcsKKWCrYtYoHVt294Py63nfnKCMyWU7gsUm2NoLQrqIuZV3UXEamBVdi190ok8Ars01NnYdba2gsztxuXwWpZNBqHWaLZ3F7g+PFXjdtrti3ZormVRFkppX/j3DmY1TXi8tjBwStgdurUmzDTWiAsL6eM27Wy8bVrzSXeIiLbrr0eZol2c8sKEZFyEZcMX7XjcpjlS3i/yXlcpm5Ss23Jl82fG20XwfeEiEjJqqzoGP5FyI9Lmuvq8P0ZbzNf3wtZ3F6iprQW0FqDaK0kKiX8fUXqIzCrb66HmS+A2xwgHo9H6hrMpfZt/W1wv9VbV8NsfgKX2Te243MTCeBz2tfUBLO7Nm82bv/jhx6B+zz5Vdz+4PTpvTDLZvE59ftxKX1/P27tEwiEYYZaKrjd5t8rwuF62bb9dmOWaGmG7zN8GLfxSDTh51NjHF8jBw6+ALNiEd9v6HvU2hKh70NEH6u1zOPB0xdtP7/SbsHjxfeo1grmrfhLFREREZEDOKkiIiIicgAnVUREREQO4KSKiIiIyAGcVBERERE5gJMqIiIiIgesqKWCy+2CZcF2DZc13/o+vMJ8FbRoEBFpbMLloKdP4xLTxSXzqu4iuBxUa6mQz+Oy7JqyqrVVxeXopVIeZg0NLTBbXMSfbfvNVxu3B14yl5BatZqkcjlj1hjBZeMatDq7iEh9HJdrf/C+X4DZK8/8AGZLoLVGSwtuZ9HfPwQzlwf/O+PoK0dhtnoLLmF/7Y0jMGsfwNd4Q10UZiZly5LJlLnFRFw5nyE/Xgk+FsQrz4/M49J8lwtf35GIuUWAiMjjD5hL6Tt78fdkKdfc7ARuxxGqw5/t6CvHYBZLxGCWTqZhduz1gzBDWjub5dO/94vG7PTMDNzv9P4zMAtF8ee2yng8Ozg6CrPXnn4NZmcPm9vOaO0PtPG4vX0VzLTXzGWXYNbc3IP3y+Fz6gUl+KjVgs/vk9auDmN2/tQJ+D75An4GZc7j8WV+fgJmBeU1tczvN18/VeV553bjqYZdw/ev14fHJo2ltYLRZj1VfN1dLP5SRUREROQATqqIiIiIHMBJFREREZEDOKkiIiIicgAnVUREREQO4KSKiIiIyAEraqng9rglEjOXZmslsF1ru2A2fmIMZvNzUzArFPAq2rEYXjE9k1kwbi+XC3AfjceDV7XWynubmvB3UqvVYNbXdxnMvH5wLG7zubGqVZlfNpfOHhk+B9/n+J7jMDt75CTMtNXlDx/A7QOu2/kemD377b83br/+xvfBfVJJ3Arg5AFcnhxPtMJMO2fjw7gU3R/AK6bf/7ufgJmJ2+WSoM98DahtE0LmNiMiInMZvDJ7wIeHj1IFl+b39V2OszXrjNt/5Z574T6/8fk/hZkbXPsiIktzuMR+dnYEZg0tuCVHagbf841N+PpB8sWi7B82t0c4+jK+Vo+8th9mTS3mkn4RkU/9Dr7mtNYVa7YPwuy6e641br96NW5D4vfia2tyEX/HT/zD8zD71l9/A2aVShFm6aUSzKJ1ceN2NB5YFUsWZsxtYA4efBG+TwC0MRDR2w5YVhlmbrcHZj4fHhMqFfP34fVe2nF4lONArSlE9NZDVaXVkTZ3qAPnU0Qkk8HPjbfiL1VEREREDuCkioiIiMgBnFQREREROYCTKiIiIiIHcFJFRERE5ABOqoiIiIgcsKKWCmLbsFTU41XKIiu4LDJUF4bZ0aMvw2xuDpepp9N4ZXpUmuoBq42LiHiVtgmovFRExOPBX29RKesMh+vw+5Xx+40Pj5v3KZrLWcvFspw/Zd4nl8bHp3TPkL51uEx6YMMamB3bh0vA1+5YC7MTB8wtJjIpvLL8puu3wOz47qMw8wfwddDc3QyzF777BMwGB3fAbNf3X4OZScWqyuxcypgtZXNwv+6mBMw8LvzvrrKFS+xnJ/A9KDZuPzF0k7ldQXoetz8YPY7Hgu613TArZnEblds+fA/MPF78nWSX8Pe89grcdkAeQYELtqtp6cUtGiKH62F21yfvgll/M76OS1YFZtsH+mHmBSXzbrd2beH3SkRx+5W733cjzKbOTMJs13NP4vdLdMIsnze3pKnVzPeGbdtSBa0pOjvx+Ki1FggF8fdRKuVhllk2jxUiIqUivo7LoP2ErbSViUYbYNbQgK9jrfVQLIbHLe39tHYLPtCSRkTkiSf+DGZvxV+qiIiIiBzASRURERGRAzipIiIiInIAJ1VEREREDuCkioiIiMgBnFQREREROWBFLRXcHrdEYuYWCCVQti8ispwyl52KiMQSMZhdeeXdMDuw/zmYFYq4HUClYj5OrWS1WsMlmBqt5DMUwm0T6urwfrbg4yzlze0WajXzPi63S3xBcwlpwMKrlGtl6iPHcHn79NkpmGWX8crzJ/acgNnA2g3G7Z1rcBl0OIZXfH/PL+FrrpDFK9kvJXHJ/7tuw+X56SRu/ZCex5lJ1bLgcYSi+DO/emIMZlqZdGtfG8xcbtx34wOf/CjMvD7zkHT1e6+B+0TqIzDLZ3BJuXY+NYEwLmFPtDfCrKyMkUgkGJArNphbMZyK4zGkY1UHzK5ei1s7VEC5v4hIfQi3v3EJPt9VcA0FlPJ1SzmOXAm3lfF78SNt58d2wmxmDI9Ni4uzMAsEzN8JahfhdrslGDRfr5cPXQ/fB7XVeCfaNedTWsRo4Gfz4GN0e5SWS8qzNxjBzyGtxU0gHIBZ1cJjmj/kh5ngzjj/Cn+pIiIiInIAJ1VEREREDuCkioiIiMgBnFQREREROYCTKiIiIiIHcFJFRERE5ACXVs54wX/sciVFBNfM0/+vem3bvmD5eZ7Pn2gXnFOez59ovEd/uvB8/vQxntO3W9GkioiIiIjM+L//iIiIiBzASRURERGRAzipIiIiInIAJ1VEREREDuCkioiIiMgBnFQREREROYCTKiIiIiIHcFJFRERE5ABOqoiIiIgc8H8ACnt0cQ95y6AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x576 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_recon = pca.inverse_transform(x_train_proj)\n",
    "x_train_recon = x_train_recon.reshape(x_train_ori.shape)\n",
    "\n",
    "x_test_recon = pca.inverse_transform(x_test_proj)\n",
    "x_test_recon = x_test_recon.reshape(x_test_ori.shape)\n",
    "\n",
    "#show original images\n",
    "#Setup a figure 8 inches by 8 inches\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "# plot the components, each image is 26 by 26 pixels\n",
    "print('Original images:')\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 5, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.reshape(x_train_ori[i,:,:,0]/np.max(x_train_ori[i,:,:,0]), (16,16)), cmap=plt.cm.bone, interpolation='nearest')\n",
    "  \n",
    "#recon images\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "# plot the components, each image is 26 by 26 pixels\n",
    "print('reconstructed images:')\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(5, 5, i+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.reshape(x_train_recon[i,:,:,0]/np.max(x_train_recon[i,:,:,0]), (16,16)), cmap=plt.cm.bone, interpolation='nearest')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Origianal and reconstructured images look similar. Now we can calculate $BW_1$ and $BW_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$BW_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original images avg.BW/image:  1552.0  Bytes\n"
     ]
    }
   ],
   "source": [
    "BWr = HuffmanBW(np.floor(x_train_ori).astype('int32'))\n",
    "print('original images avg.BW/image: ',str(BWr),' Bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$BW_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projections avg.BW/image:  317.0  Bytes\n"
     ]
    }
   ],
   "source": [
    "BWp = HuffmanBW(x_train_proj.astype('int32'))\n",
    "print('projections avg.BW/image: ',str(BWp),' Bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train the network to obtain $a_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape to resnet:  (16, 16, 12)\n"
     ]
    }
   ],
   "source": [
    "### max normalization\n",
    "x_train=x_train_recon/np.max(np.abs(x_train_recon))\n",
    "x_test=x_test_recon/np.max(np.abs(x_test_recon))\n",
    "\n",
    "input_shape = x_test.shape[1:]\n",
    "print('input shape to resnet: ',input_shape)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complile the ResNet-8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 16, 16, 12)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 64)   6976        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 64)   256         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 64)   36928       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 64)   256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 64)   36928       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 64)   256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 64)   0           activation_22[0][0]              \n",
      "                                                                 batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 64)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 96)     55392       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 96)     384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 96)     0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 96)     83040       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 96)     6240        activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 96)     384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 96)     0           conv2d_33[0][0]                  \n",
      "                                                                 batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 96)     0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 4, 4, 144)    124560      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 144)    576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 144)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 4, 4, 144)    186768      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 4, 4, 144)    13968       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 4, 4, 144)    576         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 144)    0           conv2d_36[0][0]                  \n",
      "                                                                 batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 144)    0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 1, 1, 144)    0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 144)          0           average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           1450        flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 554,938\n",
      "Trainable params: 553,594\n",
      "Non-trainable params: 1,344\n",
      "__________________________________________________________________________________________________\n",
      "ResNet8\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32  \n",
    "epochs = 1\n",
    "\n",
    "model = resnet_v1(input_shape=input_shape, depth=depth,num_classes=num_classes)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=lr_schedule(0)),metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set call backs and fit. (We train only for 1 epoch to show the code works. We used a Titan_V GPU with 200 epochs to train both original DWT and PCA models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lahiru d. chamain\\anaconda3\\envs\\tfgpumy\\lib\\site-packages\\keras_preprocessing\\image\\numpy_array_iterator.py:127: UserWarning: NumpyArrayIterator is set to use the data format convention \"channels_last\" (channels on axis 3), i.e. expected either 1, 3, or 4 channels on axis 3. However, it was passed an array with shape (50000, 16, 16, 12) (12 channels).\n",
      "  str(self.x.shape[channels_axis]) + ' channels).')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lahiru d. chamain\\anaconda3\\envs\\tfgpumy\\lib\\site-packages\\keras\\callbacks.py:1109: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1609abcbf28>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [lr_reducer, lr_scheduler]\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model.fit_generator(creategen(x_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=int(np.ceil(x_train.shape[0]/32.0)),\n",
    "                        epochs=epochs, verbose=0, workers=1,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the testSet after 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time per image : 0.2656890869140625  ms\n",
      "Test loss: 1.3354834184646607\n",
      "Test accuracy: 0.5856\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('time per image :',(time.time()-start)*1000/10000,' ms')\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
